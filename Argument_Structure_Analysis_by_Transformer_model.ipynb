{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Structure Analysis by Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot  as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "from scipy import sparse\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(9837)\n",
    "torch.manual_seed(9837)\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切断ポアソン分布を生成する関数\n",
    "def rtpois(mu, a, b, n):\n",
    "    FA = scipy.stats.poisson.cdf(a, mu)\n",
    "    FB = scipy.stats.poisson.cdf(b, mu)\n",
    "    return np.array(scipy.stats.poisson.ppf(np.random.uniform(0, 1, n)*(FB-FA)+FA, mu), dtype=\"int\")\n",
    "\n",
    "# 多項分布の乱数を生成する関数\n",
    "def rmnom(pr, n, k, pattern):\n",
    "    if pattern==1:\n",
    "        z_id = np.array(np.argmax(np.cumsum(pr, axis=1) >= np.random.uniform(0, 1, n)[:, np.newaxis], axis=1), dtype=\"int\")\n",
    "        Z = np.diag(np.repeat(1, k))[z_id, ]\n",
    "        return z_id, Z\n",
    "    z_id = np.array(np.argmax((np.cumsum(pr, axis=1) >= np.random.uniform(0, 1, n)[:, np.newaxis]), axis=1), dtype=\"int\")\n",
    "    return z_id\n",
    "\n",
    "# rel関数を定義\n",
    "def rel(x):\n",
    "    x[x < 0] = 0.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込みと分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "path = \"D:/Statistics/data/NLP/\"\n",
    "kyoto_corpus = pd.read_csv(path + \"new_kyoto_corpus.csv\")\n",
    "kyoto_dependency = pd.read_csv(path + \"new_kyoto_dependency_feature.csv\")\n",
    "kwdlc_corpus = pd.read_csv(path + \"new_kwdlc_corpus.csv\")\n",
    "kwdlc_dependency = pd.read_csv(path + \"new_kwdlc_dependency_feature.csv\")\n",
    "rel_type_dependency = pd.read_csv(path + \"rel_type_dependency.csv\")\n",
    "D1 = np.unique(kyoto_corpus[\"d_id\"]).shape[0]\n",
    "D2 = np.unique(kwdlc_corpus[\"d_id\"]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分割\n",
    "split = 0.9\n",
    "split1 = np.split(np.arange(D1), [int(D1 * split)])\n",
    "split2 = np.split(np.arange(D2), [int(D2 * split)])\n",
    "kyoto_corpus1 = kyoto_corpus.iloc[np.where(np.in1d(np.array(kyoto_corpus[\"d_id\"]), split1[0]))[0]]\n",
    "kyoto_corpus2 = kyoto_corpus.iloc[np.where(np.in1d(np.array(kyoto_corpus[\"d_id\"]), split1[1]))[0]]\n",
    "kyoto_dependency1 = kyoto_dependency.iloc[np.where(np.in1d(np.array(kyoto_dependency[\"d_id\"]), split1[0]))[0]]\n",
    "kyoto_dependency2 = kyoto_dependency.iloc[np.where(np.in1d(np.array(kyoto_dependency[\"d_id\"]), split1[1]))[0]]\n",
    "kwdlc_corpus1 = kwdlc_corpus.iloc[np.where(np.in1d(np.array(kwdlc_corpus[\"d_id\"]), split2[0]))[0]]\n",
    "kwdlc_corpus2 = kwdlc_corpus.iloc[np.where(np.in1d(np.array(kwdlc_corpus[\"d_id\"]), split2[1]))[0]]\n",
    "kwdlc_dependency1 = kwdlc_dependency.iloc[np.where(np.in1d(np.array(kwdlc_dependency[\"d_id\"]), split2[0]))[0]]\n",
    "kwdlc_dependency2 = kwdlc_dependency.iloc[np.where(np.in1d(np.array(kwdlc_dependency[\"d_id\"]), split2[1]))[0]]\n",
    "kyoto_corpus2.index = np.arange(kyoto_corpus2.shape[0])\n",
    "kyoto_dependency2.index = np.arange(kyoto_dependency2.shape[0])\n",
    "kwdlc_corpus2.index = np.arange(kwdlc_corpus2.shape[0])\n",
    "kwdlc_dependency2.index = np.arange(kwdlc_dependency2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## データの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idを定義\n",
    "# 文章idを定義\n",
    "d_id1 = np.array(kyoto_corpus1[\"d_id\"].iloc[np.where(kyoto_corpus1[[\"d_id\", \"phrase_no\"]].duplicated()==False)[0]], dtype=\"int\")\n",
    "d_id2 = np.array(kwdlc_corpus1[\"d_id\"].iloc[np.where(kwdlc_corpus1[[\"d_id\", \"phrase_no\"]].duplicated()==False)[0]], dtype=\"int\")\n",
    "d_id = np.append(d_id1, d_id2 + np.max(d_id1) + 1)\n",
    "d_long1 = np.array(kyoto_corpus1[\"d_id\"], dtype=\"int\")\n",
    "d_long2 = np.array(kwdlc_corpus1[\"d_id\"], dtype=\"int\")\n",
    "d_long = np.append(d_long1, d_long2 + np.max(d_long1) + 1)\n",
    "\n",
    "# phrase idを定義\n",
    "phrase_id1 = np.array(kyoto_corpus1[\"phrase_no\"], dtype=\"int\")\n",
    "phrase_id2 = np.array(kwdlc_corpus1[\"phrase_no\"], dtype=\"int\")\n",
    "phrase_id = np.append(phrase_id1, phrase_id2 + np.max(phrase_id1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの定義\n",
    "# 統計量を定義\n",
    "D = len(np.unique(d_id))\n",
    "d = np.unique(d_id, return_counts=True)[1].astype(\"int\")\n",
    "phrase = len(np.unique(phrase_id))\n",
    "n = np.unique(phrase_id, return_counts=True)[1].astype(\"int\")\n",
    "N = np.sum(n)\n",
    "max_m = np.max(np.append(np.max(kyoto_corpus[\"d_id\"].value_counts()), np.max(kwdlc_corpus[\"d_id\"].value_counts())))\n",
    "max_n = np.max(np.append(np.max(kyoto_corpus[\"phrase_no\"].value_counts()), np.max(kwdlc_corpus[\"phrase_no\"].value_counts())))\n",
    "\n",
    "# インデックスを定義\n",
    "d_list1 = [i for i in range(D)]\n",
    "d_list2 = [i for i in range(D)]\n",
    "phrase_list = [i for i in range(phrase)]\n",
    "pt = np.repeat(0, D)\n",
    "for i in range(D):\n",
    "    d_list1[i] = np.where(d_id==i)[0].astype(\"int\")\n",
    "    d_list2[i] = np.where(d_long==i)[0].astype(\"int\")\n",
    "    pt[i] = d_list2[i].shape[0]\n",
    "    \n",
    "for i in range(phrase):\n",
    "    if i==0:\n",
    "        max_no = 0\n",
    "        phrase_list[i] = np.arange(n[i])\n",
    "        max_no = np.max(phrase_list[i]) + 1\n",
    "    else:\n",
    "        phrase_list[i] = max_no + np.arange(n[i])\n",
    "        max_no = np.max(phrase_list[i]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書のフレーズを定義\n",
    "# フレーズ間の組み合わせを定義\n",
    "kyoto_max = np.max(np.array(kyoto_dependency1[[\"phrase_no1\", \"phrase_no2\"]]))\n",
    "kyoto_phrase_id = np.array(kyoto_dependency1[\"d_id\"], dtype=\"int\")\n",
    "kwdlc_phrase_id = np.array(kwdlc_dependency1[\"d_id\"], dtype=\"int\")\n",
    "kyoto_phrase_no1 = np.array(kyoto_dependency1[\"phrase_no1\"], dtype=\"int\")\n",
    "kyoto_phrase_no2 = np.array(kyoto_dependency1[\"phrase_no2\"], dtype=\"int\")\n",
    "kwdlc_phrase_no1 = np.array(kwdlc_dependency1[\"phrase_no1\"], dtype=\"int\")\n",
    "kwdlc_phrase_no2 = np.array(kwdlc_dependency1[\"phrase_no2\"], dtype=\"int\")\n",
    "feature_phrase1 = np.append(kyoto_phrase_no1, kwdlc_phrase_no1 + kyoto_max + 1)\n",
    "feature_phrase2 = np.append(kyoto_phrase_no2, kwdlc_phrase_no2 + kyoto_max + 1)\n",
    "feature_id = np.append(kyoto_phrase_id, kwdlc_phrase_id + np.max(kyoto_phrase_id) + 1)\n",
    "feature_phrase = np.hstack((feature_phrase1[:, np.newaxis], feature_phrase2[:, np.newaxis]))\n",
    "feature_list = [np.where(feature_id==i)[0].astype(\"int\") for i in range(D)]\n",
    "F1 = feature_phrase.shape[0]\n",
    "F2 = feature_phrase.shape[1]\n",
    "\n",
    "# フレーズ間距離を定義\n",
    "C = 2\n",
    "distance = np.array(feature_phrase[:, 1] - feature_phrase[:, 0] <= C, dtype=\"int\")\n",
    "distance_index = [np.where(distance==0)[0].astype(\"int\"), np.where(distance==1)[0].astype(\"int\")]\n",
    "\n",
    "# フレーズがあるレコードを抽出\n",
    "phrase_flag = []\n",
    "for i in range(D):\n",
    "    flag = np.repeat(0, max_m)\n",
    "    flag[np.arange(d[i])] = 1\n",
    "    phrase_flag.append(flag)\n",
    "phrase_flag = np.hstack((phrase_flag))\n",
    "phrase_index = np.where(phrase_flag==1)[0].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encodingを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語単位のpositional encodingをマッピング\n",
    "# 単語の位置を定義\n",
    "splits = 30\n",
    "start = 0.0; end = 0.999\n",
    "mapping_target1 = np.hstack(([np.arange(n[i]) for i in range(phrase)])) \n",
    "allocation1 = np.unique(np.quantile(mapping_target1, q=np.linspace(start, end, splits)).astype(\"int\"))\n",
    "max_pt1 = len(allocation1)\n",
    "\n",
    "# 位置idをマッピング\n",
    "pt_list1 = [j for j in range(max_pt1)]\n",
    "pt_id1 = np.repeat(0, N)\n",
    "for j in range(max_pt1):\n",
    "    if (max_pt1-1) > j:\n",
    "        pt_list1[j] = np.where((mapping_target1 >= allocation1[j]) & (mapping_target1 < allocation1[j+1]))[0].astype(\"int\")\n",
    "        pt_id1[pt_list1[j]] = np.repeat(j, pt_list1[j].shape[0])\n",
    "    if (max_pt1-1)==j:\n",
    "        pt_list1[j] = np.where(mapping_target1 >= allocation1[j])[0].astype(\"int\")\n",
    "        pt_id1[pt_list1[j]] = np.repeat(j, pt_list1[j].shape[0])\n",
    "        \n",
    "# フレーズの末尾の位置を定義\n",
    "function_flag = np.repeat(0, N)\n",
    "for i in range(phrase):\n",
    "    function_flag[np.max(phrase_list[i])] = 1\n",
    "pt_id1[function_flag==1] = max_pt1\n",
    "max_pt1 = np.unique(pt_id1).shape[0]\n",
    "\n",
    "# idとインデックスを定義\n",
    "pt_list1 = [j for j in range(max_pt1)]\n",
    "pt_n1 = np.repeat(0, max_pt1)\n",
    "for j in range(max_pt1):\n",
    "    pt_list1[j] = np.where(pt_id1==j)[0].astype(\"int\")\n",
    "    pt_n1[j] = pt_list1[j].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレーズ単位のpositional encodingをマッピング\n",
    "# フレーズの位置を定義\n",
    "splits = 30\n",
    "start = 0.0; end = 0.999\n",
    "mapping_target2 = np.hstack(([np.arange(d[i]) for i in range(D)])) \n",
    "allocation2 = np.unique(np.quantile(mapping_target2, q=np.linspace(start, end, splits)).astype(\"int\"))\n",
    "max_pt2 = len(allocation2)\n",
    "\n",
    "# 位置idをマッピング\n",
    "pt_list2 = [j for j in range(max_pt2)]\n",
    "pt_id2 = np.repeat(0, phrase)\n",
    "pt_n2 = np.repeat(0, max_pt2)\n",
    "for j in range(max_pt2):\n",
    "    if (max_pt2-1) > j:\n",
    "        pt_list2[j] = np.where((mapping_target2 >= allocation2[j]) & (mapping_target2 < allocation2[j+1]))[0].astype(\"int\")\n",
    "        pt_id2[pt_list2[j]] = np.repeat(j, pt_list2[j].shape[0])\n",
    "        pt_n2[j] = pt_list2[j].shape[0]\n",
    "    if (max_pt2-1)==j:\n",
    "        pt_list2[j] = np.where(mapping_target2 >= allocation2[j])[0].astype(\"int\")\n",
    "        pt_id2[pt_list2[j]] = np.repeat(j, pt_list2[j].shape[0])\n",
    "        pt_n2[j] = pt_list2[j].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力単語を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 低頻度の単語を品詞で入れ替える\n",
    "# テキストの正規化\n",
    "word_class = np.append(kyoto_corpus[\"word_class\"], kwdlc_corpus[\"word_class\"])\n",
    "class_detail1 = np.append(kyoto_corpus[\"class_detail1\"], kwdlc_corpus[\"class_detail1\"])\n",
    "class_detail2 = np.append(kyoto_corpus[\"class_detail2\"], kwdlc_corpus[\"class_detail2\"])\n",
    "class_detail3 = np.append(kyoto_corpus[\"class_detail3\"], kwdlc_corpus[\"class_detail3\"])\n",
    "new_genkei = np.append(kyoto_corpus[\"genkei\"], kwdlc_corpus[\"genkei\"])\n",
    "new_genkei[class_detail1==\"数\"] = \"0\"\n",
    "new_genkei = np.array(pd.Series(new_genkei).str.lower().str.normalize(\"NFKC\"))\n",
    "\n",
    "# 単語頻度を定義\n",
    "threshold_freq = 25\n",
    "word_freq = pd.Series(new_genkei).value_counts()\n",
    "factorized_word = np.array(word_freq.index)[np.where(word_freq < threshold_freq)[0]]\n",
    "flag = np.repeat(1, len(factorized_word))\n",
    "\n",
    "# 名寄対象の単語データフレームを定義\n",
    "info1 = pd.DataFrame({\"serial_no\": np.arange(len(new_genkei)), \"genkei\": new_genkei, \"class\": word_class, \"class_detail1\": class_detail1,\n",
    "                      \"class_detail2\": class_detail2, \"class_detail3\": class_detail3})\n",
    "info2 = pd.DataFrame({\"genkei\": factorized_word, \"flag\": flag})\n",
    "factorized_df = pd.merge(info1, info2, on=\"genkei\", how=\"left\")\n",
    "factorized_df = factorized_df.iloc[np.where(pd.isna(factorized_df[\"flag\"])==False)[0]]\n",
    "factorized_df.index = np.arange(factorized_df.shape[0])\n",
    "index_factorized = np.array(factorized_df[\"serial_no\"], dtype=\"int\")\n",
    "\n",
    "# 単語を品詞に置き換える\n",
    "index_detail1 = np.where(factorized_df[\"class_detail1\"]!=\"*\")[0].astype(\"int\")\n",
    "index_detail2 = np.where(factorized_df[\"class_detail2\"]!=\"*\")[0].astype(\"int\")\n",
    "new_genkei[index_factorized] = np.array(factorized_df[\"class\"])\n",
    "new_genkei[index_factorized[index_detail1]] = np.array(factorized_df[\"class_detail1\"].iloc[index_detail1])\n",
    "new_genkei[index_factorized[index_detail2]] = np.array(factorized_df[\"class_detail1\"].iloc[index_detail2])\n",
    "del info1, info2, factorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語idおよび活用形id定義\n",
    "# 学習データのレコード\n",
    "index1 = np.where(np.in1d(np.array(kyoto_corpus[\"d_id\"]), split1[0]))[0].astype(\"int\")\n",
    "index2 = np.where(np.in1d(np.append(np.repeat(-1, kyoto_corpus.shape[0]), kwdlc_corpus[\"d_id\"]), split2[0]))[0].astype(\"int\")\n",
    "index = np.append(index1, index2)\n",
    "\n",
    "# 単語idをマッピング\n",
    "unique_word = np.unique(new_genkei)\n",
    "v1 = unique_word.shape[0]\n",
    "word_df = pd.DataFrame({\"word\": unique_word, \"id\": np.arange(v1)})\n",
    "word_id = np.array(pd.merge(pd.DataFrame({\"word\": new_genkei[index]}), word_df, on=\"word\", how=\"left\")[\"id\"])\n",
    "\n",
    "# 活用形idをマッピング\n",
    "inflection = np.append(kyoto_corpus[\"inflectional2\"], kwdlc_corpus[\"inflectional2\"])\n",
    "unique_inflection = np.unique(inflection)\n",
    "v2 = unique_inflection.shape[0]\n",
    "inflection_df = pd.DataFrame({\"inflection\": unique_inflection, \"id\": np.arange(v2)})\n",
    "inflection_id = np.array(pd.merge(pd.DataFrame({\"inflection\": inflection[index]}), inflection_df, on=\"inflection\", how=\"left\")[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idのベクトルを行列に変換\n",
    "# 単語レベルのベクトルを行列に変換\n",
    "word_box = np.full((phrase, max_n+1), v1+1)\n",
    "inflection_box = np.full((phrase, max_n+1), v2+1)\n",
    "pt_box = np.full((phrase, max_n+1), max_pt1+1)\n",
    "for i in range(phrase):\n",
    "    word_box[i, np.arange(n[i]+1)] = np.append(0, word_id[phrase_list[i]]+1)\n",
    "    inflection_box[i, np.arange(n[i]+1)] = np.append(0, inflection_id[phrase_list[i]]+1)\n",
    "    pt_box[i, np.arange(n[i]+1)] = np.append(0, pt_id1[phrase_list[i]]+1)\n",
    "    \n",
    "# フレーズレベルのベクトルを行列に変換\n",
    "phrase_box = np.full((D, max_m), phrase)\n",
    "for i in range(D):\n",
    "    phrase_box[i, np.arange(d[i])] = d_list1[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 応答変数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係り受けの応答変数を定義\n",
    "# rel typeのmappingを読み込む\n",
    "rel_mapping = pd.read_csv(path + \"/rel_mapping/rel_mapping.csv\")\n",
    "rel_mapping = rel_mapping.iloc[np.where(pd.isna(rel_mapping[\"mapping\"])==False)[0]]\n",
    "rel_mapping.index = np.arange(rel_mapping.shape[0])\n",
    "rel_class = np.append(np.unique(rel_mapping[\"mapping\"]), np.array([\"係り受け\"]))\n",
    "classes = len(rel_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パターンごとに係り受け関係を取得\n",
    "columns = [\"dependency\", \"rel\", \"rel_type\"]\n",
    "dependency = pd.concat((kyoto_dependency1[columns], kwdlc_dependency1[columns]), axis=0)\n",
    "dependency.index = np.arange(F1)\n",
    "\n",
    "Y = np.full((F1, classes), 0)\n",
    "for j in range(classes-1):\n",
    "    search_word = \"^%s$|;%s;|^%s;|;%s$\" % (rel_class[j], rel_class[j], rel_class[j], rel_class[j])\n",
    "    index = np.where(dependency[\"rel_type\"].str.contains(search_word)==True)[0].astype(\"int\")\n",
    "    Y[index, j] = 1\n",
    "Y[np.where(dependency[\"dependency\"]==1)[0], classes-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idを定義\n",
    "# 文章idを定義\n",
    "d_id1 = np.array(kyoto_corpus2[\"d_id\"].iloc[np.where(kyoto_corpus2[[\"d_id\", \"phrase_no\"]].duplicated()==False)[0]], dtype=\"int\")\n",
    "d_id2 = np.array(kwdlc_corpus2[\"d_id\"].iloc[np.where(kwdlc_corpus2[[\"d_id\", \"phrase_no\"]].duplicated()==False)[0]], dtype=\"int\")\n",
    "d_id0 = np.append(d_id1 - np.min(d_id1), d_id2 - np.min(d_id2) + np.max(d_id1 - np.min(d_id1)) + 1)\n",
    "d_long1 = np.array(kyoto_corpus2[\"d_id\"], dtype=\"int\")\n",
    "d_long2 = np.array(kwdlc_corpus2[\"d_id\"], dtype=\"int\")\n",
    "d_long0 = np.append(d_long1 - np.min(d_long1), d_long2 - np.min(d_long2) + np.max(d_long1 - np.min(d_long1)) + 1)\n",
    "\n",
    "# phrase idを定義\n",
    "phrase_id1 = np.array(kyoto_corpus2[\"phrase_no\"], dtype=\"int\")\n",
    "phrase_id2 = np.array(kwdlc_corpus2[\"phrase_no\"], dtype=\"int\")\n",
    "phrase_id0 = np.append(phrase_id1 - np.min(phrase_id1), phrase_id2 - np.min(phrase_id2) + np.max(phrase_id1 - np.min(phrase_id1)) + 1)\n",
    "phrase_master = pd.DataFrame({\"id1\": np.append(phrase_id1, phrase_id2 + np.max(phrase_id1)), \"id2\": phrase_id0})\n",
    "phrase_master = phrase_master.iloc[np.where(phrase_master.duplicated()==False)[0]]\n",
    "phrase_master.index = np.arange(phrase_master.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの定義\n",
    "# 統計量を定義\n",
    "D0 = len(np.unique(d_id0))\n",
    "d0 = np.unique(d_id0, return_counts=True)[1].astype(\"int\")\n",
    "phrase0 = len(np.unique(phrase_id0))\n",
    "n0 = np.unique(phrase_id0, return_counts=True)[1].astype(\"int\")\n",
    "N0 = np.sum(n0)\n",
    "\n",
    "# インデックスを定義\n",
    "d_list01 = [i for i in range(D0)]\n",
    "d_list02 = [i for i in range(D0)]\n",
    "phrase_list0 = [i for i in range(phrase0)]\n",
    "pt0 = np.repeat(0, D0)\n",
    "for i in range(D0):\n",
    "    d_list01[i] = np.where(d_id0==i)[0].astype(\"int\")\n",
    "    d_list02[i] = np.where(d_long0==i)[0].astype(\"int\")\n",
    "    pt0[i] = d_list02[i].shape[0]\n",
    "    \n",
    "for i in range(phrase0):\n",
    "    if i==0:\n",
    "        max_no = 0\n",
    "        phrase_list0[i] = np.arange(n0[i])\n",
    "        max_no = np.max(phrase_list0[i]) + 1\n",
    "    else:\n",
    "        phrase_list0[i] = max_no + np.arange(n0[i])\n",
    "        max_no = np.max(phrase_list0[i]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書のフレーズを定義\n",
    "# フレーズ間の組み合わせを定義\n",
    "kyoto_max = np.max(np.array(kyoto_dependency2[[\"phrase_no1\", \"phrase_no2\"]]))\n",
    "kyoto_id = np.array(kyoto_dependency2[\"d_id\"], dtype=\"int\")\n",
    "kwdlc_id = np.array(kwdlc_dependency2[\"d_id\"], dtype=\"int\")\n",
    "kyoto_phrase_no1 = np.array(kyoto_dependency2[\"phrase_no1\"], dtype=\"int\")\n",
    "kyoto_phrase_no2 = np.array(kyoto_dependency2[\"phrase_no2\"], dtype=\"int\")\n",
    "kwdlc_phrase_no1 = np.array(kwdlc_dependency2[\"phrase_no1\"], dtype=\"int\")\n",
    "kwdlc_phrase_no2 = np.array(kwdlc_dependency2[\"phrase_no2\"], dtype=\"int\")\n",
    "joint_no1 = pd.DataFrame({\"id1\": np.append(kyoto_phrase_no1, kwdlc_phrase_no1 + np.max(phrase_id1))})\n",
    "joint_no2 = pd.DataFrame({\"id1\": np.append(kyoto_phrase_no2, kwdlc_phrase_no2 + np.max(phrase_id1))})\n",
    "feature_phrase1 = np.array(pd.merge(joint_no1, phrase_master, on=\"id1\", how=\"left\")[\"id2\"])\n",
    "feature_phrase2 = np.array(pd.merge(joint_no2, phrase_master, on=\"id1\", how=\"left\")[\"id2\"])\n",
    "\n",
    "feature_id0 = np.append(kyoto_id - np.min(kyoto_id), kwdlc_id - np.min(kwdlc_id) + np.max(kyoto_id - np.min(kyoto_id)) + 1)\n",
    "feature_phrase0 = np.hstack((feature_phrase1[:, np.newaxis], feature_phrase2[:, np.newaxis]))\n",
    "feature_list0 = [np.where(feature_id0==i)[0].astype(\"int\") for i in range(D0)]\n",
    "F01 = feature_phrase0.shape[0]\n",
    "\n",
    "# フレーズ間距離を定義\n",
    "distance0 = np.array(feature_phrase0[:, 1] - feature_phrase0[:, 0] <= C, dtype=\"int\")\n",
    "distance_index0 = [np.where(distance0==0)[0].astype(\"int\"), np.where(distance0==1)[0].astype(\"int\")]\n",
    "\n",
    "# フレーズがあるレコードを抽出\n",
    "phrase_flag0 = []\n",
    "for i in range(D0):\n",
    "    flag = np.repeat(0, max_m)\n",
    "    flag[np.arange(d0[i])] = 1\n",
    "    phrase_flag0.append(flag)\n",
    "phrase_flag0 = np.hstack((phrase_flag0))\n",
    "phrase_index0 = np.where(phrase_flag0==1)[0].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encodingを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語単位のpositional encodingをマッピング\n",
    "# 単語の位置を定義\n",
    "mapping_target01 = np.hstack(([np.arange(n0[i]) for i in range(phrase0)])) \n",
    "\n",
    "# 位置idをマッピング\n",
    "pt_list01 = [j for j in range(max_pt1)]\n",
    "pt_id01 = np.repeat(0, N0)\n",
    "for j in range(max_pt1-1):\n",
    "    if (max_pt1-2) > j:\n",
    "        pt_list01[j] = np.where((mapping_target01 >= allocation1[j]) & (mapping_target01 < allocation1[j+1]))[0].astype(\"int\")\n",
    "        pt_id01[pt_list01[j]] = np.repeat(j, pt_list01[j].shape[0])\n",
    "    if (max_pt1-2)==j:\n",
    "        pt_list01[j] = np.where(mapping_target01 >= allocation1[j])[0].astype(\"int\")\n",
    "        pt_id01[pt_list01[j]] = np.repeat(j, pt_list01[j].shape[0])\n",
    "        \n",
    "# フレーズの末尾の位置を定義\n",
    "function_flag0 = np.repeat(0, N0)\n",
    "for i in range(phrase0):\n",
    "    function_flag0[np.max(phrase_list0[i])] = 1\n",
    "pt_id01[function_flag0==1] = max_pt1-1\n",
    "\n",
    "# idとインデックスを定義\n",
    "pt_list01 = [j for j in range(max_pt1)]\n",
    "pt_n01 = np.repeat(0, max_pt1)\n",
    "for j in range(max_pt1):\n",
    "    pt_list01[j] = np.where(pt_id01==j)[0].astype(\"int\")\n",
    "    pt_n01[j] = pt_list01[j].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレーズ単位のpositional encodingをマッピング\n",
    "# フレーズの位置を定義\n",
    "mapping_target02 = np.hstack(([np.arange(d0[i]) for i in range(D0)])) \n",
    "\n",
    "# 位置idをマッピング\n",
    "pt_list02 = [j for j in range(max_pt2)]\n",
    "pt_id02 = np.repeat(0, phrase0)\n",
    "pt_n02 = np.repeat(0, max_pt2)\n",
    "for j in range(max_pt2):\n",
    "    if (max_pt2-1) > j:\n",
    "        pt_list02[j] = np.where((mapping_target02 >= allocation2[j]) & (mapping_target02 < allocation2[j+1]))[0].astype(\"int\")\n",
    "        pt_id02[pt_list02[j]] = np.repeat(j, pt_list02[j].shape[0])\n",
    "        pt_n02[j] = pt_list02[j].shape[0]\n",
    "    if (max_pt2-1)==j:\n",
    "        pt_list02[j] = np.where(mapping_target02 >= allocation2[j])[0].astype(\"int\")\n",
    "        pt_id02[pt_list02[j]] = np.repeat(j, pt_list02[j].shape[0])\n",
    "        pt_n02[j] = pt_list02[j].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力単語を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語idおよび活用形idを定義\n",
    "# 学習データのレコード\n",
    "index1 = np.where(np.in1d(np.array(kyoto_corpus[\"d_id\"]), split1[1]))[0].astype(\"int\")\n",
    "index2 = np.where(np.in1d(np.append(np.repeat(-1, kyoto_corpus.shape[0]), kwdlc_corpus[\"d_id\"]), split2[1]))[0].astype(\"int\")\n",
    "index = np.append(index1, index2)\n",
    "\n",
    "# idをマッピング\n",
    "word_id0 = np.array(pd.merge(pd.DataFrame({\"word\": new_genkei[index]}), word_df, on=\"word\", how=\"left\")[\"id\"])\n",
    "inflection_id0 = np.array(pd.merge(pd.DataFrame({\"inflection\": inflection[index]}), inflection_df, on=\"inflection\", how=\"left\")[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idのベクトルを行列に変換\n",
    "# 単語レベルのベクトルを行列に変換\n",
    "word_box0 = np.full((phrase0, max_n+1), v1+1)\n",
    "inflection_box0 = np.full((phrase0, max_n+1), v2+1)\n",
    "pt_box0 = np.full((phrase0, max_n+1), max_pt1+1)\n",
    "for i in range(phrase0):\n",
    "    word_box0[i, np.arange(n0[i]+1)] = np.append(0, word_id0[phrase_list0[i]]+1)\n",
    "    inflection_box0[i, np.arange(n0[i]+1)] = np.append(0, inflection_id0[phrase_list0[i]]+1)\n",
    "    pt_box0[i, np.arange(n0[i]+1)] = np.append(0, pt_id01[phrase_list0[i]]+1)\n",
    "    \n",
    "# フレーズレベルのベクトルを行列に変換\n",
    "phrase_box0 = np.full((D0, max_m), phrase0)\n",
    "for i in range(D0):\n",
    "    phrase_box0[i, np.arange(d0[i])] = d_list01[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 応答変数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パターンごとに係り受け関係を取得\n",
    "columns = [\"dependency\", \"rel\", \"rel_type\"]\n",
    "dependency0 = pd.concat((kyoto_dependency2[columns], kwdlc_dependency2[columns]), axis=0)\n",
    "dependency0.index = np.arange(F01)\n",
    "\n",
    "Y0 = np.full((F01, classes), 0)\n",
    "for j in range(classes-1):\n",
    "    search_word = \"^%s$|;%s;|^%s;|;%s$\" % (rel_class[j], rel_class[j], rel_class[j], rel_class[j])\n",
    "    index = np.where(dependency0[\"rel_type\"].str.contains(search_word)==True)[0].astype(\"int\")\n",
    "    Y0[index, j] = 1\n",
    "Y0[np.where(dependency0[\"dependency\"]==1)[0], classes-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Structure Analysis by Transformer modelを推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor配列を定義\n",
    "# 応答変数をTensor配列に変換\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Y_ = torch.Tensor(Y).to(device)\n",
    "Y0_ = torch.Tensor(Y0).to(device)\n",
    "\n",
    "# 入力変数をTensor配列に変換\n",
    "word_box_ = torch.LongTensor(word_box)\n",
    "inflection_box_ = torch.LongTensor(inflection_box)\n",
    "pt_box_ = torch.LongTensor(pt_box)\n",
    "pt_id1_ = torch.LongTensor(pt_id1)\n",
    "pt_id2_ = torch.LongTensor(pt_id2)\n",
    "phrase_box_ = torch.LongTensor(phrase_box)\n",
    "phrase_index_ = torch.LongTensor(phrase_index)\n",
    "feature_phrase_ = torch.LongTensor(feature_phrase)\n",
    "distance_ = torch.Tensor(distance[:, np.newaxis]).to(device)\n",
    "unique_phrase = torch.LongTensor(torch.arange(phrase).long())\n",
    "\n",
    "word_box0_ = torch.LongTensor(word_box0)\n",
    "inflection_box0_ = torch.LongTensor(inflection_box0)\n",
    "pt_box0_ = torch.LongTensor(pt_box0)\n",
    "pt_id01_ = torch.LongTensor(pt_id01)\n",
    "pt_id02_ = torch.LongTensor(pt_id02)\n",
    "phrase_box0_ = torch.LongTensor(phrase_box0)\n",
    "phrase_index0_ = torch.LongTensor(phrase_index0)\n",
    "feature_phrase0_ = torch.LongTensor(feature_phrase0)\n",
    "distance0_ = torch.Tensor(distance0[:, np.newaxis]).to(device)\n",
    "unique_phrase0 = torch.LongTensor(torch.arange(phrase0).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "# 埋め込み層を定義\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, in_fearture, v1, v2, max_pt1, max_pt2):\n",
    "        super().__init__()\n",
    "        self.theta_v = nn.Embedding(num_embeddings=v1+1, embedding_dim=in_features)\n",
    "        self.theta_f = nn.Embedding(num_embeddings=v2+1, embedding_dim=in_features)\n",
    "        self.theta_h1 = nn.Embedding(num_embeddings=max_pt1+1, embedding_dim=in_features)\n",
    "        self.theta_h2 = nn.Embedding(num_embeddings=max_pt2, embedding_dim=in_features)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.theta_v.weight)\n",
    "        nn.init.xavier_normal_(self.theta_f.weight)\n",
    "        nn.init.xavier_normal_(self.theta_h1.weight)\n",
    "        nn.init.xavier_normal_(self.theta_h2.weight)\n",
    "        \n",
    "    def forward(self, word_box, inflection_box, pt_box, pt_id):\n",
    "        zeros = torch.full((1, in_features), 0.0).to(device)\n",
    "        theta_v = torch.cat((self.theta_v(torch.arange(v1+1).to(device)), zeros), dim=0)\n",
    "        theta_f = torch.cat((self.theta_f(torch.arange(v2+1).to(device)), zeros), dim=0)\n",
    "        theta_h1 = torch.cat((self.theta_h1(torch.arange(max_pt1+1).to(device)), zeros), dim=0)\n",
    "        theta_h2 = self.theta_h2(torch.arange(max_pt2).to(device))\n",
    "        v_features = theta_v[word_box, ]\n",
    "        f_features = theta_f[inflection_box, ]\n",
    "        h_features1 = theta_h1[pt_box, ]\n",
    "        h_features2 = theta_h2[pt_id, ]\n",
    "        features = v_features + f_features + h_features1\n",
    "        return features, v_features, f_features, h_features1, h_features2\n",
    "    \n",
    "# Self Attention層を定義\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.gamma_k = nn.Linear(in_features, in_features, bias=False)\n",
    "        self.gamma_q = nn.Linear(in_features, in_features, bias=False)\n",
    "        self.gamma_g = nn.Linear(in_features, in_features, bias=False)\n",
    "        self.gamma_o = nn.Linear(in_features, in_features, bias=False)\n",
    "\n",
    "        nn.init.xavier_normal_(self.gamma_k.weight)\n",
    "        nn.init.xavier_normal_(self.gamma_q.weight)\n",
    "        nn.init.xavier_normal_(self.gamma_g.weight)\n",
    "        nn.init.xavier_normal_(self.gamma_o.weight)\n",
    "        \n",
    "    def forward(self, features, id_box, k):\n",
    "        # 全結合層で特徴量を変換\n",
    "        hidden_k = self.gamma_k(features)\n",
    "        hidden_q = self.gamma_q(features)\n",
    "        hidden_g = self.gamma_g(features)\n",
    "\n",
    "        # Attention Mapを定義\n",
    "        input_mask = torch.BoolTensor(id_box==k).to(device)\n",
    "        weights = torch.matmul(hidden_q, hidden_k.transpose(2, 1)) / np.sqrt(in_features)\n",
    "        mask = input_mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask==1, -1e9)\n",
    "        normalized_weights = F.softmax(weights, dim=2)\n",
    "\n",
    "        # Attention Mapの特徴量を変換\n",
    "        output = torch.matmul(normalized_weights, hidden_g)\n",
    "        output = self.gamma_o(output)\n",
    "        return output, normalized_weights\n",
    "    \n",
    "# Transformer Block層を定義\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.attention_model = Attention(in_features, out_features)\n",
    "        self.gamma_f1 = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.gamma_f2 = nn.Linear(out_features, in_features, bias=False)\n",
    "        self.layernorm1 = nn.LayerNorm(in_features)\n",
    "        self.layernorm2 = nn.LayerNorm(in_features)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        nn.init.xavier_normal_(self.gamma_f1.weight)\n",
    "        nn.init.xavier_normal_(self.gamma_f2.weight)\n",
    "        \n",
    "    def forward(self, features, id_box, k):\n",
    "        # Self Attentionで特徴量を変換\n",
    "        normalized_features = self.layernorm1(features)\n",
    "        attention_features, normalized_weights = self.attention_model(features, id_box, k)\n",
    "\n",
    "        # 正規化とfeed forward層\n",
    "        dropout_attention = features + self.dropout1(attention_features)\n",
    "        normalized_attention = self.layernorm2(dropout_attention)\n",
    "        features_ff1 = self.dropout2(F.relu(self.gamma_f1(normalized_attention)))\n",
    "        features_ff2 = dropout_attention + self.gamma_f2(features_ff1)\n",
    "        return features_ff2\n",
    "    \n",
    "# 行列分解層を定義\n",
    "class DMF(nn.Module):\n",
    "    def __init__(self, in_features, out_features, classes, C):\n",
    "        super().__init__()\n",
    "        self.gamma11 = nn.ModuleList([nn.Linear(in_features, out_features) for j in range(C)])\n",
    "        self.gamma12 = nn.ModuleList([nn.Linear(in_features, out_features) for j in range(C)])\n",
    "        self.gamma21 = nn.Linear(2*out_features, classes, bias=True)\n",
    "        self.gamma22 = nn.Linear(out_features, classes, bias=False)\n",
    "        \n",
    "        # 重み初期化処理\n",
    "        for j in range(C):\n",
    "            nn.init.xavier_normal_(self.gamma11[j].weight)\n",
    "            nn.init.xavier_normal_(self.gamma12[j].weight)\n",
    "        nn.init.xavier_normal_(self.gamma21.weight)\n",
    "        nn.init.xavier_normal_(self.gamma22.weight)\n",
    "        \n",
    "    def forward(self, x1, x2, distance):\n",
    "        ff1 = F.relu(distance*self.gamma11[0](x1) + (1-distance)*self.gamma11[1](x1))\n",
    "        ff2 = F.relu(distance*self.gamma12[0](x2) + (1-distance)*self.gamma12[1](x2))\n",
    "        logit = self.gamma21(torch.cat((ff1, ff2), dim=1)) + self.gamma22(ff1 * ff2)\n",
    "        return logit\n",
    "    \n",
    "# 結合層を定義\n",
    "class Joint(nn.Module):\n",
    "    def __init__(self, in_features, out_features, out_dim, v1, v2, max_pt1, max_pt2, C, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.embedding_model = Embedding(in_features, v1, v2, max_pt1, max_pt2)\n",
    "        self.transformer_model11 = Transformer(in_features, out_features, dropout_prob)\n",
    "        self.transformer_model12 = Transformer(in_features, out_features, dropout_prob)\n",
    "        \n",
    "        self.transformer_model21_1 = Transformer(in_features, out_features, dropout_prob)\n",
    "        self.transformer_model21_2 = Transformer(in_features, out_features, dropout_prob)\n",
    "        self.transformer_model22_1 = Transformer(in_features, out_features, dropout_prob)\n",
    "        self.transformer_model22_2 = Transformer(in_features, out_features, dropout_prob)\n",
    "        self.dmf_model = DMF(in_features, out_dim, classes, C)\n",
    "        \n",
    "    def forward(self, feature_phrase, distance, word_box, inflection_box, pt_box, phrase_box, pt_id, \n",
    "                phrase_index, D, phrase, v1, v2, max_m, zeros):\n",
    "        features_tensor1, v_features, f_features, h_features1, h_features2 = self.embedding_model(word_box, inflection_box, pt_box, pt_id)\n",
    "        \n",
    "        features_transformer11 = self.transformer_model11(features_tensor1, word_box, v1+1)\n",
    "        features_transformer12 = self.transformer_model12(features_transformer11, word_box, v1+1)\n",
    "        features_tensor2 = torch.cat((features_transformer12[:, 0, :] + h_features2, zeros), dim=0)[phrase_box, ]\n",
    "        \n",
    "        features_transformer21_1 = self.transformer_model21_1(features_tensor2, phrase_box, phrase)\n",
    "        features_transformer21_2 = self.transformer_model21_2(features_transformer21_1, phrase_box, phrase)\n",
    "        features_transformer22_1 = self.transformer_model22_1(features_tensor2, phrase_box, phrase)\n",
    "        features_transformer22_2 = self.transformer_model22_2(features_transformer22_1, phrase_box, phrase)\n",
    "        \n",
    "        x1 = features_transformer21_2.reshape(D*max_m, in_features)[phrase_index, ][feature_phrase[:, 0], ]\n",
    "        x2 = features_transformer22_2.reshape(D*max_m, in_features)[phrase_index, ][feature_phrase[:, 1], ]\n",
    "        logit = self.dmf_model(x1, x2, distance)\n",
    "        return logit\n",
    "    \n",
    "# 早期終了アルゴリズム\n",
    "class EarlyStopping:\n",
    "    '''\n",
    "    早期終了 (early stopping)\n",
    "    '''\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "# ハイパーパラメータを定義\n",
    "Lambda = torch.tensor(0.0)\n",
    "in_features = 32\n",
    "out_features = 128\n",
    "out_dim = 256\n",
    "dropout_prob = 0.1\n",
    "input_mask = torch.BoolTensor(phrase_box==phrase)\n",
    "input_mask0 = torch.BoolTensor(phrase_box0==phrase0)\n",
    "zeros1 = torch.Tensor([0.0]).repeat(in_features).reshape(1, in_features)\n",
    "zeros2 = torch.Tensor([0.0]).repeat(out_features).reshape(1, out_features)\n",
    "phrase_flag = torch.LongTensor(phrase_box!=phrase)\n",
    "phrase_flag0 = torch.LongTensor(phrase_box0!=phrase0)\n",
    "\n",
    "# 対数尤度を定義\n",
    "def loglike(Y, logit):\n",
    "    Prob = np.exp(logit) / (1 + np.exp(logit))\n",
    "    Prob[Prob==1.0] = 0.9999999\n",
    "    Prob[Prob==0.0] = 0.0000001\n",
    "    LL = np.sum(Y * np.log(Prob) + (1-Y) * np.log(1-Prob))\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アルゴリズムの定義\n",
    "model = Joint(in_features, out_features, out_dim, v1, v2, max_pt1, max_pt2, C, dropout_prob).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "optimizer = optimizers.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.99), amsgrad=True, weight_decay=0.25)\n",
    "\n",
    "def compute_loss(t, y, Lambda):\n",
    "    Lho = criterion(t, y)\n",
    "    if Lambda > 0.0:\n",
    "        l2_reg = torch.tensor(0.)\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "        Lho += Lambda*l2_reg\n",
    "    return Lho\n",
    "\n",
    "def train_step(y, feature_phrase, distance, word_box, inflection_box, pt_box, phrase_box, pt_id, phrase_index, \n",
    "               D, phrase, v1, v2, max_m, zeros, model, optimizer, Lambda):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    mu = model(feature_phrase, distance, word_box, inflection_box, pt_box, phrase_box, pt_id, \n",
    "               phrase_index, D, phrase, v1, v2, max_m, zeros)\n",
    "    Lho = compute_loss(mu, y, Lambda)\n",
    "    Lho.backward()\n",
    "    optimizer.step()\n",
    "    return Lho, mu\n",
    "\n",
    "def val_step(y, feature_phrase, distance, word_box, inflection_box, pt_box, phrase_box, pt_id,\n",
    "             phrase_index, D, phrase, v1, v2, max_m, zeros, model, Lambda):\n",
    "    model.eval()\n",
    "    mu = model(feature_phrase, distance, word_box, inflection_box, pt_box, phrase_box, pt_id, \n",
    "               phrase_index, D, phrase, v1, v2, max_m, zeros)\n",
    "    Lho = compute_loss(mu, y, Lambda)\n",
    "    return Lho, mu\n",
    "\n",
    "\n",
    "# モデルの設定\n",
    "epochs = 200\n",
    "n_batches_train = 100\n",
    "n_batches_val = 50\n",
    "batch_size = D // n_batches_train\n",
    "batch_size0 = D0 // n_batches_val\n",
    "batch_index = np.array_split(np.arange(D), n_batches_train)\n",
    "batch_index0 = np.array_split(np.arange(D0), n_batches_val)\n",
    "mini_batch_size = np.array([len(batch_index[i]) for i in range(n_batches_train)])\n",
    "mini_batch_size0 = np.array([len(batch_index0[i]) for i in range(n_batches_val)])\n",
    "es = EarlyStopping(patience=5, verbose=1)\n",
    "hist = {\"train_loglike\": [], \"val_loglike\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-687873.9  -43243. ]\n",
      "[-687873.9  -43243. ]\n",
      "1\n",
      "[-372921.5  -35527.9]\n",
      "[-372921.5  -35527.9]\n",
      "2\n",
      "[-322848.   -32746.1]\n",
      "[-322848.1  -32746.1]\n",
      "3\n",
      "[-299988.   -31079.9]\n",
      "[-299988.   -31079.9]\n",
      "4\n",
      "[-285747.9  -30066.9]\n",
      "[-285747.9  -30066.9]\n",
      "5\n",
      "[-272515.7  -28717.3]\n",
      "[-272515.8  -28717.3]\n",
      "6\n",
      "[-261881.4  -27995.3]\n",
      "[-261881.4  -27995.3]\n",
      "7\n",
      "[-253580.7  -27368.4]\n",
      "[-253580.7  -27368.4]\n",
      "8\n",
      "[-246758.3  -26924.4]\n",
      "[-246758.4  -26924.4]\n",
      "9\n",
      "[-241425.2  -26562.9]\n",
      "[-241425.2  -26562.9]\n",
      "10\n",
      "[-237275.   -26273.1]\n",
      "[-237275.1  -26273.1]\n",
      "11\n",
      "[-233278.   -25954.7]\n",
      "[-233278.   -25954.7]\n",
      "12\n",
      "[-230098.9  -25736.5]\n",
      "[-230099.   -25736.5]\n",
      "13\n",
      "[-227030.7  -25738.7]\n",
      "[-227030.7  -25738.7]\n",
      "14\n",
      "[-224653.4  -25582.4]\n",
      "[-224653.4  -25582.4]\n",
      "15\n",
      "[-222020.8  -25472. ]\n",
      "[-222020.8  -25472. ]\n",
      "16\n",
      "[-220045.2  -25269.5]\n",
      "[-220045.2  -25269.5]\n",
      "17\n",
      "[-217979.7  -25275.2]\n",
      "[-217979.8  -25275.2]\n",
      "18\n",
      "[-215424.5  -25141.8]\n",
      "[-215424.5  -25141.8]\n",
      "19\n",
      "[-213711.4  -24858.4]\n",
      "[-213711.4  -24858.4]\n",
      "20\n",
      "[-211914.4  -24868.4]\n",
      "[-211914.4  -24868.4]\n",
      "21\n",
      "[-210827.6  -24665. ]\n",
      "[-210827.6  -24665. ]\n",
      "22\n",
      "[-209101.3  -24663. ]\n",
      "[-209101.4  -24663. ]\n",
      "23\n",
      "[-207613.9  -24669.7]\n",
      "[-207613.9  -24669.7]\n",
      "24\n",
      "[-206492.1  -24518.5]\n",
      "[-206492.1  -24518.6]\n",
      "25\n",
      "[-205470.   -24571.3]\n",
      "[-205470.   -24571.3]\n",
      "26\n",
      "[-204599.2  -24368.2]\n",
      "[-204599.2  -24368.2]\n",
      "27\n",
      "[-203036.7  -24356.2]\n",
      "[-203036.7  -24356.2]\n",
      "28\n",
      "[-202245.1  -24272. ]\n",
      "[-202245.1  -24272. ]\n",
      "29\n",
      "[-200474.   -24411.2]\n",
      "[-200474.   -24411.2]\n",
      "30\n",
      "[-199822.8  -24284.1]\n",
      "[-199822.8  -24284.1]\n",
      "31\n",
      "[-199029.2  -24206.4]\n",
      "[-199029.2  -24206.4]\n",
      "32\n",
      "[-197683.4  -23965.1]\n",
      "[-197683.4  -23965.1]\n",
      "33\n",
      "[-197196.2  -24241.7]\n",
      "[-197196.2  -24241.7]\n",
      "34\n",
      "[-196329.2  -24104. ]\n",
      "[-196329.2  -24104. ]\n",
      "35\n",
      "[-195065.   -24243.9]\n",
      "[-195065.   -24243.9]\n",
      "36\n",
      "[-194608.3  -24066.6]\n",
      "[-194608.3  -24066.6]\n",
      "37\n",
      "[-193573.5  -24224.7]\n",
      "[-193573.5  -24224.7]\n",
      "38\n",
      "[-193080.6  -24112.7]\n",
      "[-193080.6  -24112.7]\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "# 確率的勾配法でモデルパラメータを推定\n",
    "for rp in range(epochs):\n",
    "    \n",
    "    # 学習データでモデルを学習\n",
    "    random_index = np.argsort(np.random.uniform(0, 1, D)).astype(\"int\")\n",
    "    preds_train = []\n",
    "    y_train = []\n",
    "    train_loglike = np.repeat(0.0, n_batches_train)\n",
    "\n",
    "    # ミニバッチごとに学習\n",
    "    for batch in range(n_batches_train):\n",
    "\n",
    "        # インデックスを定義\n",
    "        size = mini_batch_size[batch]\n",
    "        index = np.sort(random_index[batch_index[batch]])\n",
    "        index1 = np.hstack(([d_list1[index[i]] for i in range(size)]))\n",
    "        index2 = np.hstack(([feature_list[index[i]] for i in range(size)]))\n",
    "        phrase_df = pd.DataFrame({\"phrase_id\": index1, \"id\": np.arange(len(index1))})\n",
    "        phrase_ = len(index1)\n",
    "        \n",
    "        # ミニバッチを定義\n",
    "        Y_ = torch.Tensor(Y[index2, ]).to(device)\n",
    "        word_box_ = word_box[index1, ]\n",
    "        inflection_box_ = inflection_box[index1, ]\n",
    "        pt_box_ = pt_box[index1, ]\n",
    "        pt_id_ = pt_id2[index1]\n",
    "        phrase_index_ = torch.where(phrase_flag[index, ].reshape(-1)==True)[0]\n",
    "        feature_phrase_ = torch.LongTensor(feature_phrase[index2, ])\n",
    "        distance_ = torch.Tensor(distance[index2][:, np.newaxis]).to(device)\n",
    "\n",
    "        # idを連番に置き換える\n",
    "        phrase_box_ = torch.LongTensor([phrase_]).repeat(size*max_m)\n",
    "        phrase_box_[phrase_index_] = torch.arange(phrase_, dtype=torch.long)\n",
    "        phrase_box_ = phrase_box_.reshape(size, max_m)\n",
    "        feature_no1_ = pd.merge(pd.DataFrame({\"phrase_id\": feature_phrase_[:, 0]}), phrase_df, on=\"phrase_id\", how=\"left\")\n",
    "        feature_no2_ = pd.merge(pd.DataFrame({\"phrase_id\": feature_phrase_[:, 1]}), phrase_df, on=\"phrase_id\", how=\"left\")\n",
    "        feature_no_ = torch.LongTensor(pd.concat((feature_no1_[\"id\"], feature_no2_[\"id\"]), axis=1).to_numpy())\n",
    "\n",
    "        # ADAMでモデルを学習\n",
    "        # パラメータを更新\n",
    "        Lho, mu = train_step(Y_, feature_no_, distance_, word_box_, inflection_box_, pt_box_, phrase_box_, pt_id_, \n",
    "                             phrase_index_, size, phrase_, v1, v2, max_m, zeros1.to(device), model, optimizer, Lambda)\n",
    "\n",
    "        # 学習結果を格納\n",
    "        preds_train.append(mu.cpu().detach().numpy().astype(\"float\"))\n",
    "        y_train.append(Y_.cpu().detach().numpy().astype(\"int\") )\n",
    "        train_loglike[batch] = np.array(-Lho.cpu().detach(), dtype=\"float\")\n",
    "        \n",
    "    # 学習データの対数尤度を更新\n",
    "    preds_train = np.vstack((preds_train))\n",
    "    y_train = np.vstack((y_train))\n",
    "    LL = loglike(y_train, preds_train)\n",
    "    hist[\"train_loglike\"].append(LL)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    # 検証データでモデルを評価\n",
    "    preds_val = []\n",
    "    y_val = []\n",
    "    val_loglike = np.repeat(0.0, n_batches_val)\n",
    "    \n",
    "    # ミニバッチごとに学習\n",
    "    for batch in range(n_batches_val):\n",
    "        \n",
    "        # インデックスを定義\n",
    "        size = mini_batch_size0[batch]\n",
    "        index = batch_index0[batch]\n",
    "        index1 = np.hstack(([d_list01[index[i]] for i in range(size)]))\n",
    "        index2 = np.hstack(([feature_list0[index[i]] for i in range(size)]))\n",
    "        phrase_df0 = pd.DataFrame({\"phrase_id\": index1, \"id\": np.arange(len(index1))})\n",
    "        phrase0_ = len(index1)\n",
    "        \n",
    "        # ミニバッチを定義\n",
    "        Y0_ = torch.Tensor(Y0[index2, ]).to(device)\n",
    "        word_box0_ = word_box0[index1, ]\n",
    "        inflection_box0_ = inflection_box0[index1, ]\n",
    "        pt_box0_ = pt_box0[index1, ]\n",
    "        pt_id02_ = pt_id02[index1]\n",
    "        phrase_index0_ = torch.where(phrase_flag0[index, ].reshape(-1)==True)[0]\n",
    "        feature_phrase0_ = torch.LongTensor(feature_phrase0[index2, ])\n",
    "        distance0_ = torch.Tensor(distance0[index2][:, np.newaxis]).to(device)\n",
    "\n",
    "        # idを連番に置き換える\n",
    "        phrase_box0_ = torch.LongTensor([phrase0_]).repeat(size*max_m)\n",
    "        phrase_box0_[phrase_index0_] = torch.arange(phrase0_, dtype=torch.long)\n",
    "        phrase_box0_ = phrase_box0_.reshape(size, max_m)\n",
    "        feature_no01_ = pd.merge(pd.DataFrame({\"phrase_id\": feature_phrase0_[:, 0]}), phrase_df0, on=\"phrase_id\", how=\"left\")\n",
    "        feature_no02_ = pd.merge(pd.DataFrame({\"phrase_id\": feature_phrase0_[:, 1]}), phrase_df0, on=\"phrase_id\", how=\"left\")\n",
    "        feature_no0_ = torch.LongTensor(pd.concat((feature_no01_[\"id\"], feature_no02_[\"id\"]), axis=1).to_numpy())\n",
    "        \n",
    "        # 推定されたモデルを評価\n",
    "        Lho, mu = val_step(Y0_, feature_no0_, distance0_, word_box0_, inflection_box0_, pt_box0_, phrase_box0_, pt_id02_,\n",
    "                           phrase_index0_, size, phrase0_, v1, v2, max_m, zeros1.to(device), model, Lambda)\n",
    "\n",
    "        # 評価結果を格納\n",
    "        preds_val.append(mu.cpu().detach().numpy().astype(\"float\"))\n",
    "        y_val.append(Y0_.cpu().detach().numpy().astype(\"int\") )\n",
    "        val_loglike[batch] = np.array(-Lho.cpu().detach(), dtype=\"float\")\n",
    "        \n",
    "    # 学習データの対数尤度を更新\n",
    "    preds_val = np.vstack((preds_val))\n",
    "    y_val = np.vstack((y_val))\n",
    "    LL0 = loglike(Y0, preds_val)\n",
    "    hist[\"val_loglike\"].append(LL0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    # 学習結果を表示\n",
    "    print(rp)\n",
    "    print(np.round([np.sum(train_loglike), np.sum(val_loglike)], 1))\n",
    "    print(np.round([LL, LL0], 1))\n",
    "    \n",
    "    # 早期終了\n",
    "    if es(-np.sum(val_loglike))==True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 14\n",
    "Prob = np.exp(preds_val[:, j]) / (1 + np.exp(preds_val[:, j]))\n",
    "res = Y0[:, j]*Prob + (1-Y0[:, j])*(1-Prob)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.sum(Y0[:, j]*Prob) / (np.sum(Y0[:, j]*Prob) + np.sum((1-Y0[:, j])*(Prob)))\n",
    "B = np.sum((1-Y0)[:, j]*(1-Prob)) / (np.sum((1-Y0)[:, j]*(1-Prob)) + np.sum(Y0[:, j]*(1-Prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2*B*A) / (A + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0\n",
    "[-597315.5  -40302.7]\n",
    "[-597315.6  -40302.7]\n",
    "1\n",
    "[-345526.6  -34632.2]\n",
    "[-345526.7  -34632.3]\n",
    "2\n",
    "[-308278.2  -32397.6]\n",
    "[-308278.3  -32397.7]\n",
    "3\n",
    "[-288602.9  -30349.6]\n",
    "[-288603.   -30349.7]\n",
    "4\n",
    "[-273989.4  -29342.1]\n",
    "[-273989.6  -29342.1]\n",
    "5\n",
    "[-262963.1  -28135.6]\n",
    "[-262963.3  -28135.7]\n",
    "6\n",
    "[-255411.8  -27616.4]\n",
    "[-255411.9  -27616.5]\n",
    "7\n",
    "[-248952.6  -27001.9]\n",
    "[-248952.8  -27002. ]\n",
    "8\n",
    "[-244104.1  -26420.1]\n",
    "[-244104.3  -26420.2]\n",
    "9\n",
    "[-239704.2  -26479.5]\n",
    "[-239704.4  -26479.6]\n",
    "10\n",
    "[-236701.3  -25888.1]\n",
    "[-236701.5  -25888.2]\n",
    "11\n",
    "[-232443.2  -25491.1]\n",
    "[-232443.4  -25491.2]\n",
    "12\n",
    "[-230476.9  -25636.2]\n",
    "[-230477.1  -25636.2]\n",
    "13\n",
    "[-228320.2  -25254.9]\n",
    "[-228320.4  -25255. ]\n",
    "14\n",
    "[-227173.4  -25385.7]\n",
    "[-227173.6  -25385.8]\n",
    "15\n",
    "[-225713.5  -24909.5]\n",
    "[-225713.7  -24909.6]\n",
    "16\n",
    "[-222821.8  -24784.1]\n",
    "[-222822.   -24784.2]\n",
    "17\n",
    "[-221800.7  -24810.6]\n",
    "[-221800.9  -24810.7]\n",
    "18\n",
    "[-220962.7  -24656.9]\n",
    "[-220962.9  -24657. ]\n",
    "19\n",
    "[-219160.8  -24583. ]\n",
    "[-219161.   -24583.1]\n",
    "20\n",
    "[-218092.7  -24405.2]\n",
    "[-218092.9  -24405.3]\n",
    "21\n",
    "[-216878.7  -24347.3]\n",
    "[-216878.9  -24347.4]\n",
    "22\n",
    "[-215723.8  -24213.2]\n",
    "[-215724.   -24213.3]\n",
    "23\n",
    "[-214967.   -24222.1]\n",
    "[-214967.2  -24222.1]\n",
    "24\n",
    "[-214054.5  -24021.9]\n",
    "[-214054.7  -24022. ]\n",
    "25\n",
    "[-212480.3  -23861.9]\n",
    "[-212480.5  -23862. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob = np.exp(preds_val) / np.sum(np.exp(preds_val), axis=1)[:, np.newaxis]\n",
    "\n",
    "Y0 * np.log(Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike(Y, logit):\n",
    "    Prob = np.exp(logit) / (1 + np.exp(logit))\n",
    "    Prob[Prob==1.0] = 0.9999999\n",
    "    Prob[Prob==0.0] = 0.0000001\n",
    "    LL = np.sum(Y * np.log(Prob) + (1-Y)*np.log(1-Prob))\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結合層を定義\n",
    "class Joint(nn.Module):\n",
    "    def __init__(self, in_features, out_features, out_dim, v1, v2, max_pt1, max_pt2, C, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.embedding_model = Embedding(in_features, v1, v2, max_pt1, max_pt2)\n",
    "        self.transformer_model1_11 = Transformer(in_features, out_features, dropout_prob)\n",
    "        self.transformer_model1_12 = Transformer(in_features, out_features, dropout_prob)\n",
    "        self.transformer_model1_21 = Transformer(in_features, out_features, dropout_prob)\n",
    "        self.transformer_model1_22 = Transformer(in_features, out_features, dropout_prob)\n",
    "        \n",
    "        self.transformer_model21_1 = Transformer(2*in_features, out_features, dropout_prob)\n",
    "        self.transformer_model21_2 = Transformer(2*in_features, out_features, dropout_prob)\n",
    "        self.transformer_model22_1 = Transformer(2*in_features, out_features, dropout_prob)\n",
    "        self.transformer_model22_2 = Transformer(2*in_features, out_features, dropout_prob)\n",
    "        self.dmf_model = DMF(2*in_features, out_dim, classes, C)\n",
    "        \n",
    "    def forward(self, feature_phrase, distance, word_box, inflection_box, pt_box, phrase_box, pt_id, \n",
    "                phrase_index, D, phrase, v1, v2, max_m, zeros):\n",
    "        features_tensor1, v_features, f_features, h_features1, h_features2 = self.embedding_model(word_box, inflection_box, pt_box, pt_id)\n",
    "        \n",
    "        features_transformer1_11 = self.transformer_model1_11(features_tensor1, word_box, v1+1)\n",
    "        features_transformer1_12 = self.transformer_model1_12(features_transformer1_11, word_box, v1+1)\n",
    "        features_transformer1_21 = self.transformer_model1_21(features_tensor1, word_box, v1+1)\n",
    "        features_transformer1_22 = self.transformer_model1_22(features_transformer1_21, word_box, v1+1)\n",
    "        features_transformer1 = torch.cat((features_transformer1_12[:, 0, :], features_transformer1_22[:, 0, :]), dim=1)\n",
    "        features_tensor2 = torch.cat((features_transformer1 + h_features2, zeros), dim=0)[phrase_box, ]\n",
    "        \n",
    "        features_transformer21_1 = self.transformer_model21_1(features_tensor2, phrase_box, phrase)\n",
    "        features_transformer21_2 = self.transformer_model21_2(features_transformer21_1, phrase_box, phrase)\n",
    "        features_transformer22_1 = self.transformer_model22_1(features_tensor2, phrase_box, phrase)\n",
    "        features_transformer22_2 = self.transformer_model22_2(features_transformer22_1, phrase_box, phrase)\n",
    "        \n",
    "        x1 = features_transformer21_2.reshape(D*max_m, 2*in_features)[phrase_index, ][feature_phrase[:, 0], ]\n",
    "        x2 = features_transformer22_2.reshape(D*max_m, 2*in_features)[phrase_index, ][feature_phrase[:, 1], ]\n",
    "        logit = self.dmf_model(x1, x2, distance)\n",
    "        return logit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
