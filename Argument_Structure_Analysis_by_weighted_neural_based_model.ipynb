{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Structure Analysis by weighted neural based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot  as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "from scipy import sparse\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(9837)\n",
    "torch.manual_seed(9837)\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切断ポアソン分布を生成する関数\n",
    "def rtpois(mu, a, b, n):\n",
    "    FA = scipy.stats.poisson.cdf(a, mu)\n",
    "    FB = scipy.stats.poisson.cdf(b, mu)\n",
    "    return np.array(scipy.stats.poisson.ppf(np.random.uniform(0, 1, n)*(FB-FA)+FA, mu), dtype=\"int\")\n",
    "\n",
    "# 多項分布の乱数を生成する関数\n",
    "def rmnom(pr, n, k, pattern):\n",
    "    if pattern==1:\n",
    "        z_id = np.array(np.argmax(np.cumsum(pr, axis=1) >= np.random.uniform(0, 1, n)[:, np.newaxis], axis=1), dtype=\"int\")\n",
    "        Z = np.diag(np.repeat(1, k))[z_id, ]\n",
    "        return z_id, Z\n",
    "    z_id = np.array(np.argmax((np.cumsum(pr, axis=1) >= np.random.uniform(0, 1, n)[:, np.newaxis]), axis=1), dtype=\"int\")\n",
    "    return z_id\n",
    "\n",
    "# rel関数を定義\n",
    "def rel(x):\n",
    "    x[x < 0] = 0.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込みと分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "path = \"D:/Statistics/data/NLP/\"\n",
    "kyoto_corpus = pd.read_csv(path + \"new_kyoto_corpus.csv\")\n",
    "kyoto_dependency = pd.read_csv(path + \"new_kyoto_dependency_feature.csv\")\n",
    "kwdlc_corpus = pd.read_csv(path + \"new_kwdlc_corpus.csv\")\n",
    "kwdlc_dependency = pd.read_csv(path + \"new_kwdlc_dependency_feature.csv\")\n",
    "rel_type_dependency = pd.read_csv(path + \"rel_type_dependency.csv\")\n",
    "D1 = np.unique(kyoto_corpus[\"d_id\"]).shape[0]\n",
    "D2 = np.unique(kwdlc_corpus[\"d_id\"]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分割\n",
    "split = 0.9\n",
    "split1 = np.split(np.arange(D1), [int(D1 * split)])\n",
    "split2 = np.split(np.arange(D2), [int(D2 * split)])\n",
    "kyoto_corpus1 = kyoto_corpus.iloc[np.where(np.in1d(np.array(kyoto_corpus[\"d_id\"]), split1[0]))[0]]\n",
    "kyoto_corpus2 = kyoto_corpus.iloc[np.where(np.in1d(np.array(kyoto_corpus[\"d_id\"]), split1[1]))[0]]\n",
    "kyoto_dependency1 = kyoto_dependency.iloc[np.where(np.in1d(np.array(kyoto_dependency[\"d_id\"]), split1[0]))[0]]\n",
    "kyoto_dependency2 = kyoto_dependency.iloc[np.where(np.in1d(np.array(kyoto_dependency[\"d_id\"]), split1[1]))[0]]\n",
    "kwdlc_corpus1 = kwdlc_corpus.iloc[np.where(np.in1d(np.array(kwdlc_corpus[\"d_id\"]), split2[0]))[0]]\n",
    "kwdlc_corpus2 = kwdlc_corpus.iloc[np.where(np.in1d(np.array(kwdlc_corpus[\"d_id\"]), split2[1]))[0]]\n",
    "kwdlc_dependency1 = kwdlc_dependency.iloc[np.where(np.in1d(np.array(kwdlc_dependency[\"d_id\"]), split2[0]))[0]]\n",
    "kwdlc_dependency2 = kwdlc_dependency.iloc[np.where(np.in1d(np.array(kwdlc_dependency[\"d_id\"]), split2[1]))[0]]\n",
    "kyoto_corpus2.index = np.arange(kyoto_corpus2.shape[0])\n",
    "kyoto_dependency2.index = np.arange(kyoto_dependency2.shape[0])\n",
    "kwdlc_corpus2.index = np.arange(kwdlc_corpus2.shape[0])\n",
    "kwdlc_dependency2.index = np.arange(kwdlc_dependency2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## データの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idを定義\n",
    "# 文章idを定義\n",
    "d_id1 = np.array(kyoto_corpus1[\"d_id\"].iloc[np.where(kyoto_corpus1[[\"d_id\", \"phrase_no\"]].duplicated()==False)[0]], dtype=\"int\")\n",
    "d_id2 = np.array(kwdlc_corpus1[\"d_id\"].iloc[np.where(kwdlc_corpus1[[\"d_id\", \"phrase_no\"]].duplicated()==False)[0]], dtype=\"int\")\n",
    "d_id = np.append(d_id1, d_id2 + np.max(d_id1) + 1)\n",
    "d_long1 = np.array(kyoto_corpus1[\"d_id\"], dtype=\"int\")\n",
    "d_long2 = np.array(kwdlc_corpus1[\"d_id\"], dtype=\"int\")\n",
    "d_long = np.append(d_long1, d_long2 + np.max(d_long1) + 1)\n",
    "\n",
    "# phrase idを定義\n",
    "phrase_id1 = np.array(kyoto_corpus1[\"phrase_no\"], dtype=\"int\")\n",
    "phrase_id2 = np.array(kwdlc_corpus1[\"phrase_no\"], dtype=\"int\")\n",
    "phrase_id = np.append(phrase_id1, phrase_id2 + np.max(phrase_id1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの定義\n",
    "# 統計量を定義\n",
    "D = len(np.unique(d_id))\n",
    "d = np.unique(d_id, return_counts=True)[1].astype(\"int\")\n",
    "phrase = len(np.unique(phrase_id))\n",
    "n = np.unique(phrase_id, return_counts=True)[1].astype(\"int\")\n",
    "N = np.sum(n)\n",
    "max_m = np.max(np.append(np.max(kyoto_corpus[\"d_id\"].value_counts()), np.max(kwdlc_corpus[\"d_id\"].value_counts())))\n",
    "max_n = np.max(np.append(np.max(kyoto_corpus[\"phrase_no\"].value_counts()), np.max(kwdlc_corpus[\"phrase_no\"].value_counts())))\n",
    "\n",
    "# インデックスを定義\n",
    "d_list1 = [i for i in range(D)]\n",
    "d_list2 = [i for i in range(D)]\n",
    "phrase_list = [i for i in range(phrase)]\n",
    "pt = np.repeat(0, D)\n",
    "for i in range(D):\n",
    "    d_list1[i] = np.where(d_id==i)[0].astype(\"int\")\n",
    "    d_list2[i] = np.where(d_long==i)[0].astype(\"int\")\n",
    "    pt[i] = d_list2[i].shape[0]\n",
    "    \n",
    "for i in range(phrase):\n",
    "    if i==0:\n",
    "        max_no = 0\n",
    "        phrase_list[i] = np.arange(n[i])\n",
    "        max_no = np.max(phrase_list[i]) + 1\n",
    "    else:\n",
    "        phrase_list[i] = max_no + np.arange(n[i])\n",
    "        max_no = np.max(phrase_list[i]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書のフレーズを定義\n",
    "# フレーズ間の組み合わせを定義\n",
    "kyoto_max = np.max(np.array(kyoto_dependency1[[\"phrase_no1\", \"phrase_no2\"]]))\n",
    "kyoto_phrase_id = np.array(kyoto_dependency1[\"d_id\"], dtype=\"int\")\n",
    "kwdlc_phrase_id = np.array(kwdlc_dependency1[\"d_id\"], dtype=\"int\")\n",
    "kyoto_phrase_no1 = np.array(kyoto_dependency1[\"phrase_no1\"], dtype=\"int\")\n",
    "kyoto_phrase_no2 = np.array(kyoto_dependency1[\"phrase_no2\"], dtype=\"int\")\n",
    "kwdlc_phrase_no1 = np.array(kwdlc_dependency1[\"phrase_no1\"], dtype=\"int\")\n",
    "kwdlc_phrase_no2 = np.array(kwdlc_dependency1[\"phrase_no2\"], dtype=\"int\")\n",
    "feature_phrase1 = np.append(kyoto_phrase_no1, kwdlc_phrase_no1 + kyoto_max + 1)\n",
    "feature_phrase2 = np.append(kyoto_phrase_no2, kwdlc_phrase_no2 + kyoto_max + 1)\n",
    "feature_id = np.append(kyoto_phrase_id, kwdlc_phrase_id + np.max(kyoto_phrase_id) + 1)\n",
    "feature_phrase = np.hstack((feature_phrase1[:, np.newaxis], feature_phrase2[:, np.newaxis]))\n",
    "feature_list = [np.where(feature_id==i)[0].astype(\"int\") for i in range(D)]\n",
    "F1 = feature_phrase.shape[0]\n",
    "F2 = feature_phrase.shape[1]\n",
    "\n",
    "# フレーズ間距離を定義\n",
    "C = 2\n",
    "distance = np.array(feature_phrase[:, 1] - feature_phrase[:, 0] <= C, dtype=\"int\")\n",
    "distance_index = [np.where(distance==0)[0].astype(\"int\"), np.where(distance==1)[0].astype(\"int\")]\n",
    "\n",
    "# フレーズがあるレコードを抽出\n",
    "phrase_flag = []\n",
    "for i in range(D):\n",
    "    flag = np.repeat(0, max_m)\n",
    "    flag[np.arange(d[i])] = 1\n",
    "    phrase_flag.append(flag)\n",
    "phrase_flag = np.hstack((phrase_flag))\n",
    "phrase_index = np.where(phrase_flag==1)[0].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encodingを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語単位のpositional encodingをマッピング\n",
    "# 単語の位置を定義\n",
    "splits = 30\n",
    "start = 0.0; end = 0.999\n",
    "mapping_target1 = np.hstack(([np.arange(n[i]) for i in range(phrase)])) \n",
    "allocation1 = np.unique(np.quantile(mapping_target1, q=np.linspace(start, end, splits)).astype(\"int\"))\n",
    "max_pt1 = len(allocation1)\n",
    "\n",
    "# 位置idをマッピング\n",
    "pt_list1 = [j for j in range(max_pt1)]\n",
    "pt_id1 = np.repeat(0, N)\n",
    "for j in range(max_pt1):\n",
    "    if (max_pt1-1) > j:\n",
    "        pt_list1[j] = np.where((mapping_target1 >= allocation1[j]) & (mapping_target1 < allocation1[j+1]))[0].astype(\"int\")\n",
    "        pt_id1[pt_list1[j]] = np.repeat(j, pt_list1[j].shape[0])\n",
    "    if (max_pt1-1)==j:\n",
    "        pt_list1[j] = np.where(mapping_target1 >= allocation1[j])[0].astype(\"int\")\n",
    "        pt_id1[pt_list1[j]] = np.repeat(j, pt_list1[j].shape[0])\n",
    "        \n",
    "# フレーズの末尾の位置を定義\n",
    "function_flag = np.repeat(0, N)\n",
    "for i in range(phrase):\n",
    "    function_flag[np.max(phrase_list[i])] = 1\n",
    "pt_id1[function_flag==1] = max_pt1\n",
    "max_pt1 = np.unique(pt_id1).shape[0]\n",
    "\n",
    "# idとインデックスを定義\n",
    "pt_list1 = [j for j in range(max_pt1)]\n",
    "pt_n1 = np.repeat(0, max_pt1)\n",
    "for j in range(max_pt1):\n",
    "    pt_list1[j] = np.where(pt_id1==j)[0].astype(\"int\")\n",
    "    pt_n1[j] = pt_list1[j].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレーズ単位のpositional encodingをマッピング\n",
    "# フレーズの位置を定義\n",
    "splits = 30\n",
    "start = 0.0; end = 0.999\n",
    "mapping_target2 = np.hstack(([np.arange(d[i]) for i in range(D)])) \n",
    "allocation2 = np.unique(np.quantile(mapping_target2, q=np.linspace(start, end, splits)).astype(\"int\"))\n",
    "max_pt2 = len(allocation2)\n",
    "\n",
    "# 位置idをマッピング\n",
    "pt_list2 = [j for j in range(max_pt2)]\n",
    "pt_id2 = np.repeat(0, phrase)\n",
    "pt_n2 = np.repeat(0, max_pt2)\n",
    "for j in range(max_pt2):\n",
    "    if (max_pt2-1) > j:\n",
    "        pt_list2[j] = np.where((mapping_target2 >= allocation2[j]) & (mapping_target2 < allocation2[j+1]))[0].astype(\"int\")\n",
    "        pt_id2[pt_list2[j]] = np.repeat(j, pt_list2[j].shape[0])\n",
    "        pt_n2[j] = pt_list2[j].shape[0]\n",
    "    if (max_pt2-1)==j:\n",
    "        pt_list2[j] = np.where(mapping_target2 >= allocation2[j])[0].astype(\"int\")\n",
    "        pt_id2[pt_list2[j]] = np.repeat(j, pt_list2[j].shape[0])\n",
    "        pt_n2[j] = pt_list2[j].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力単語を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 低頻度の単語を品詞で入れ替える\n",
    "# テキストの正規化\n",
    "word_class = np.append(kyoto_corpus[\"word_class\"], kwdlc_corpus[\"word_class\"])\n",
    "class_detail1 = np.append(kyoto_corpus[\"class_detail1\"], kwdlc_corpus[\"class_detail1\"])\n",
    "class_detail2 = np.append(kyoto_corpus[\"class_detail2\"], kwdlc_corpus[\"class_detail2\"])\n",
    "class_detail3 = np.append(kyoto_corpus[\"class_detail3\"], kwdlc_corpus[\"class_detail3\"])\n",
    "new_genkei = np.append(kyoto_corpus[\"genkei\"], kwdlc_corpus[\"genkei\"])\n",
    "new_genkei[class_detail1==\"数\"] = \"0\"\n",
    "new_genkei = np.array(pd.Series(new_genkei).str.lower().str.normalize(\"NFKC\"))\n",
    "\n",
    "# 単語頻度を定義\n",
    "threshold_freq = 25\n",
    "word_freq = pd.Series(new_genkei).value_counts()\n",
    "factorized_word = np.array(word_freq.index)[np.where(word_freq < threshold_freq)[0]]\n",
    "flag = np.repeat(1, len(factorized_word))\n",
    "\n",
    "# 名寄対象の単語データフレームを定義\n",
    "info1 = pd.DataFrame({\"serial_no\": np.arange(len(new_genkei)), \"genkei\": new_genkei, \"class\": word_class, \"class_detail1\": class_detail1,\n",
    "                      \"class_detail2\": class_detail2, \"class_detail3\": class_detail3})\n",
    "info2 = pd.DataFrame({\"genkei\": factorized_word, \"flag\": flag})\n",
    "factorized_df = pd.merge(info1, info2, on=\"genkei\", how=\"left\")\n",
    "factorized_df = factorized_df.iloc[np.where(pd.isna(factorized_df[\"flag\"])==False)[0]]\n",
    "factorized_df.index = np.arange(factorized_df.shape[0])\n",
    "index_factorized = np.array(factorized_df[\"serial_no\"], dtype=\"int\")\n",
    "\n",
    "# 単語を品詞に置き換える\n",
    "index_detail1 = np.where(factorized_df[\"class_detail1\"]!=\"*\")[0].astype(\"int\")\n",
    "index_detail2 = np.where(factorized_df[\"class_detail2\"]!=\"*\")[0].astype(\"int\")\n",
    "new_genkei[index_factorized] = np.array(factorized_df[\"class\"])\n",
    "new_genkei[index_factorized[index_detail1]] = np.array(factorized_df[\"class_detail1\"].iloc[index_detail1])\n",
    "new_genkei[index_factorized[index_detail2]] = np.array(factorized_df[\"class_detail1\"].iloc[index_detail2])\n",
    "del info1, info2, factorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語idおよび活用形id定義\n",
    "# 学習データのレコード\n",
    "index1 = np.where(np.in1d(np.array(kyoto_corpus[\"d_id\"]), split1[0]))[0].astype(\"int\")\n",
    "index2 = np.where(np.in1d(np.append(np.repeat(-1, kyoto_corpus.shape[0]), kwdlc_corpus[\"d_id\"]), split2[0]))[0].astype(\"int\")\n",
    "index = np.append(index1, index2)\n",
    "\n",
    "# 単語idをマッピング\n",
    "unique_word = np.unique(new_genkei)\n",
    "v1 = unique_word.shape[0]\n",
    "word_df = pd.DataFrame({\"word\": unique_word, \"id\": np.arange(v1)})\n",
    "word_id = np.array(pd.merge(pd.DataFrame({\"word\": new_genkei[index]}), word_df, on=\"word\", how=\"left\")[\"id\"])\n",
    "\n",
    "# 活用形idをマッピング\n",
    "inflection = np.append(kyoto_corpus[\"inflectional2\"], kwdlc_corpus[\"inflectional2\"])\n",
    "unique_inflection = np.unique(inflection)\n",
    "v2 = unique_inflection.shape[0]\n",
    "inflection_df = pd.DataFrame({\"inflection\": unique_inflection, \"id\": np.arange(v2)})\n",
    "inflection_id = np.array(pd.merge(pd.DataFrame({\"inflection\": inflection[index]}), inflection_df, on=\"inflection\", how=\"left\")[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idのベクトルを行列に変換\n",
    "# 単語レベルのベクトルを行列に変換\n",
    "word_box = np.full((phrase, max_n+1), v1+1)\n",
    "inflection_box = np.full((phrase, max_n+1), v2+1)\n",
    "pt_box = np.full((phrase, max_n+1), max_pt1+1)\n",
    "for i in range(phrase):\n",
    "    word_box[i, np.arange(n[i]+1)] = np.append(0, word_id[phrase_list[i]]+1)\n",
    "    inflection_box[i, np.arange(n[i]+1)] = np.append(0, inflection_id[phrase_list[i]]+1)\n",
    "    pt_box[i, np.arange(n[i]+1)] = np.append(0, pt_id1[phrase_list[i]]+1)\n",
    "    \n",
    "# フレーズレベルのベクトルを行列に変換\n",
    "phrase_box = np.full((D, max_m), phrase)\n",
    "for i in range(D):\n",
    "    phrase_box[i, np.arange(d[i])] = d_list1[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 応答変数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係り受けの応答変数を定義\n",
    "# rel typeのmappingを読み込む\n",
    "rel_mapping = pd.read_csv(path + \"/rel_mapping/rel_mapping.csv\")\n",
    "rel_mapping = rel_mapping.iloc[np.where(pd.isna(rel_mapping[\"mapping\"])==False)[0]]\n",
    "rel_mapping.index = np.arange(rel_mapping.shape[0])\n",
    "rel_class = np.append(np.unique(rel_mapping[\"mapping\"]), np.array([\"係り受け\", \"係り受けなし\"]))\n",
    "classes = len(rel_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パターンごとに係り受け関係を取得\n",
    "columns = [\"dependency\", \"rel\", \"rel_type\"]\n",
    "dependency = pd.concat((kyoto_dependency1[columns], kwdlc_dependency1[columns]), axis=0)\n",
    "dependency.index = np.arange(F1)\n",
    "\n",
    "Y = np.full((F1, classes), 0)\n",
    "for j in range(classes-1):\n",
    "    search_word = \"^%s$|;%s;|^%s;|;%s$\" % (rel_class[j], rel_class[j], rel_class[j], rel_class[j])\n",
    "    index = np.where(dependency[\"rel_type\"].str.contains(search_word)==True)[0].astype(\"int\")\n",
    "    Y[index, j] = 1\n",
    "Y[np.where((np.sum(Y, axis=1)==0) & (dependency[\"dependency\"]==1))[0], classes-2] = 1\n",
    "Y[np.where(np.sum(Y, axis=1)==0)[0], classes-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idを定義\n",
    "# 文章idを定義\n",
    "d_id1 = np.array(kyoto_corpus2[\"d_id\"].iloc[np.where(kyoto_corpus2[[\"d_id\", \"phrase_no\"]].duplicated()==False)[0]], dtype=\"int\")\n",
    "d_id2 = np.array(kwdlc_corpus2[\"d_id\"].iloc[np.where(kwdlc_corpus2[[\"d_id\", \"phrase_no\"]].duplicated()==False)[0]], dtype=\"int\")\n",
    "d_id0 = np.append(d_id1 - np.min(d_id1), d_id2 - np.min(d_id2) + np.max(d_id1 - np.min(d_id1)) + 1)\n",
    "d_long1 = np.array(kyoto_corpus2[\"d_id\"], dtype=\"int\")\n",
    "d_long2 = np.array(kwdlc_corpus2[\"d_id\"], dtype=\"int\")\n",
    "d_long0 = np.append(d_long1 - np.min(d_long1), d_long2 - np.min(d_long2) + np.max(d_long1 - np.min(d_long1)) + 1)\n",
    "\n",
    "# phrase idを定義\n",
    "phrase_id1 = np.array(kyoto_corpus2[\"phrase_no\"], dtype=\"int\")\n",
    "phrase_id2 = np.array(kwdlc_corpus2[\"phrase_no\"], dtype=\"int\")\n",
    "phrase_id0 = np.append(phrase_id1 - np.min(phrase_id1), phrase_id2 - np.min(phrase_id2) + np.max(phrase_id1 - np.min(phrase_id1)) + 1)\n",
    "phrase_master = pd.DataFrame({\"id1\": np.append(phrase_id1, phrase_id2 + np.max(phrase_id1)), \"id2\": phrase_id0})\n",
    "phrase_master = phrase_master.iloc[np.where(phrase_master.duplicated()==False)[0]]\n",
    "phrase_master.index = np.arange(phrase_master.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの定義\n",
    "# 統計量を定義\n",
    "D0 = len(np.unique(d_id0))\n",
    "d0 = np.unique(d_id0, return_counts=True)[1].astype(\"int\")\n",
    "phrase0 = len(np.unique(phrase_id0))\n",
    "n0 = np.unique(phrase_id0, return_counts=True)[1].astype(\"int\")\n",
    "N0 = np.sum(n0)\n",
    "\n",
    "# インデックスを定義\n",
    "d_list01 = [i for i in range(D0)]\n",
    "d_list02 = [i for i in range(D0)]\n",
    "phrase_list0 = [i for i in range(phrase0)]\n",
    "pt0 = np.repeat(0, D0)\n",
    "for i in range(D0):\n",
    "    d_list01[i] = np.where(d_id0==i)[0].astype(\"int\")\n",
    "    d_list02[i] = np.where(d_long0==i)[0].astype(\"int\")\n",
    "    pt0[i] = d_list02[i].shape[0]\n",
    "    \n",
    "for i in range(phrase0):\n",
    "    if i==0:\n",
    "        max_no = 0\n",
    "        phrase_list0[i] = np.arange(n0[i])\n",
    "        max_no = np.max(phrase_list0[i]) + 1\n",
    "    else:\n",
    "        phrase_list0[i] = max_no + np.arange(n0[i])\n",
    "        max_no = np.max(phrase_list0[i]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書のフレーズを定義\n",
    "# フレーズ間の組み合わせを定義\n",
    "kyoto_max = np.max(np.array(kyoto_dependency2[[\"phrase_no1\", \"phrase_no2\"]]))\n",
    "kyoto_phrase_id = np.array(kyoto_dependency2[\"d_id\"], dtype=\"int\")\n",
    "kwdlc_phrase_id = np.array(kwdlc_dependency2[\"d_id\"], dtype=\"int\")\n",
    "kyoto_phrase_no1 = np.array(kyoto_dependency2[\"phrase_no1\"], dtype=\"int\")\n",
    "kyoto_phrase_no2 = np.array(kyoto_dependency2[\"phrase_no2\"], dtype=\"int\")\n",
    "kwdlc_phrase_no1 = np.array(kwdlc_dependency2[\"phrase_no1\"], dtype=\"int\")\n",
    "kwdlc_phrase_no2 = np.array(kwdlc_dependency2[\"phrase_no2\"], dtype=\"int\")\n",
    "joint_no1 = pd.DataFrame({\"id1\": np.append(kyoto_phrase_no1, kwdlc_phrase_no1 + np.max(phrase_id1))})\n",
    "joint_no2 = pd.DataFrame({\"id1\": np.append(kyoto_phrase_no2, kwdlc_phrase_no2 + np.max(phrase_id1))})\n",
    "feature_phrase1 = np.array(pd.merge(joint_no1, phrase_master, on=\"id1\", how=\"left\")[\"id2\"])\n",
    "feature_phrase2 = np.array(pd.merge(joint_no2, phrase_master, on=\"id1\", how=\"left\")[\"id2\"])\n",
    "\n",
    "feature_id0 = np.append(kyoto_phrase_id, kwdlc_phrase_id + np.max(kyoto_phrase_id) + 1)\n",
    "feature_phrase0 = np.hstack((feature_phrase1[:, np.newaxis], feature_phrase2[:, np.newaxis]))\n",
    "feature_list0 = [np.where(feature_id0==i)[0].astype(\"int\") for i in range(D0)]\n",
    "F01 = feature_phrase0.shape[0]\n",
    "\n",
    "# フレーズ間距離を定義\n",
    "distance0 = np.array(feature_phrase0[:, 1] - feature_phrase0[:, 0] <= C, dtype=\"int\")\n",
    "distance_index0 = [np.where(distance0==0)[0].astype(\"int\"), np.where(distance0==1)[0].astype(\"int\")]\n",
    "\n",
    "# フレーズがあるレコードを抽出\n",
    "phrase_flag0 = []\n",
    "for i in range(D0):\n",
    "    flag = np.repeat(0, max_m)\n",
    "    flag[np.arange(d0[i])] = 1\n",
    "    phrase_flag0.append(flag)\n",
    "phrase_flag0 = np.hstack((phrase_flag0))\n",
    "phrase_index0 = np.where(phrase_flag0==1)[0].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encodingを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語単位のpositional encodingをマッピング\n",
    "# 単語の位置を定義\n",
    "mapping_target01 = np.hstack(([np.arange(n0[i]) for i in range(phrase0)])) \n",
    "\n",
    "# 位置idをマッピング\n",
    "pt_list01 = [j for j in range(max_pt1)]\n",
    "pt_id01 = np.repeat(0, N0)\n",
    "for j in range(max_pt1-1):\n",
    "    if (max_pt1-2) > j:\n",
    "        pt_list01[j] = np.where((mapping_target01 >= allocation1[j]) & (mapping_target01 < allocation1[j+1]))[0].astype(\"int\")\n",
    "        pt_id01[pt_list01[j]] = np.repeat(j, pt_list01[j].shape[0])\n",
    "    if (max_pt1-2)==j:\n",
    "        pt_list01[j] = np.where(mapping_target01 >= allocation1[j])[0].astype(\"int\")\n",
    "        pt_id01[pt_list01[j]] = np.repeat(j, pt_list01[j].shape[0])\n",
    "        \n",
    "# フレーズの末尾の位置を定義\n",
    "function_flag0 = np.repeat(0, N0)\n",
    "for i in range(phrase0):\n",
    "    function_flag0[np.max(phrase_list0[i])] = 1\n",
    "pt_id01[function_flag0==1] = max_pt1-1\n",
    "\n",
    "# idとインデックスを定義\n",
    "pt_list01 = [j for j in range(max_pt1)]\n",
    "pt_n01 = np.repeat(0, max_pt1)\n",
    "for j in range(max_pt1):\n",
    "    pt_list01[j] = np.where(pt_id01==j)[0].astype(\"int\")\n",
    "    pt_n01[j] = pt_list01[j].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレーズ単位のpositional encodingをマッピング\n",
    "# フレーズの位置を定義\n",
    "mapping_target02 = np.hstack(([np.arange(d0[i]) for i in range(D0)])) \n",
    "\n",
    "# 位置idをマッピング\n",
    "pt_list02 = [j for j in range(max_pt2)]\n",
    "pt_id02 = np.repeat(0, phrase0)\n",
    "pt_n02 = np.repeat(0, max_pt2)\n",
    "for j in range(max_pt2):\n",
    "    if (max_pt2-1) > j:\n",
    "        pt_list02[j] = np.where((mapping_target02 >= allocation2[j]) & (mapping_target02 < allocation2[j+1]))[0].astype(\"int\")\n",
    "        pt_id02[pt_list02[j]] = np.repeat(j, pt_list02[j].shape[0])\n",
    "        pt_n02[j] = pt_list02[j].shape[0]\n",
    "    if (max_pt2-1)==j:\n",
    "        pt_list02[j] = np.where(mapping_target02 >= allocation2[j])[0].astype(\"int\")\n",
    "        pt_id02[pt_list02[j]] = np.repeat(j, pt_list02[j].shape[0])\n",
    "        pt_n02[j] = pt_list02[j].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力単語を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語idを定義\n",
    "# 学習データのレコード\n",
    "index1 = np.where(np.in1d(np.array(kyoto_corpus[\"d_id\"]), split1[1]))[0].astype(\"int\")\n",
    "index2 = np.where(np.in1d(np.append(np.repeat(-1, kyoto_corpus.shape[0]), kwdlc_corpus[\"d_id\"]), split2[1]))[0].astype(\"int\")\n",
    "index = np.append(index1, index2)\n",
    "\n",
    "# 単語idをマッピング\n",
    "word_id0 = np.array(pd.merge(pd.DataFrame({\"word\": new_genkei[index]}), word_df, on=\"word\", how=\"left\")[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idのベクトルを行列に変換\n",
    "# 単語レベルのベクトルを行列に変換\n",
    "word_box0 = np.full((phrase0, max_n+1), v+1)\n",
    "pt_box0 = np.full((phrase0, max_n+1), max_pt1+1)\n",
    "for i in range(phrase0):\n",
    "    word_box0[i, np.arange(n0[i]+1)] = np.append(0, word_id0[phrase_list0[i]]+1)\n",
    "    pt_box0[i, np.arange(n0[i]+1)] = np.append(0, pt_id01[phrase_list0[i]]+1)\n",
    "    \n",
    "# フレーズレベルのベクトルを行列に変換\n",
    "phrase_box0 = np.full((D0, max_m), phrase0)\n",
    "for i in range(D0):\n",
    "    phrase_box0[i, np.arange(d0[i])] = d_list01[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 応答変数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パターンごとに係り受け関係を取得\n",
    "columns = [\"dependency\", \"rel\", \"rel_type\"]\n",
    "dependency0 = pd.concat((kyoto_dependency2[columns], kwdlc_dependency2[columns]), axis=0)\n",
    "dependency0.index = np.arange(F01)\n",
    "\n",
    "Y0 = np.full((F01, classes), 0)\n",
    "for j in range(classes-1):\n",
    "    search_word = \"^%s$|;%s;|^%s;|;%s$\" % (rel_class[j], rel_class[j], rel_class[j], rel_class[j])\n",
    "    index = np.where(dependency0[\"rel_type\"].str.contains(search_word)==True)[0].astype(\"int\")\n",
    "    Y0[index, j] = 1\n",
    "Y0[np.where((np.sum(Y0, axis=1)==0) & (dependency0[\"dependency\"]==1))[0], classes-2] = 1\n",
    "Y0[np.where(np.sum(Y0, axis=1)==0)[0], classes-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Structure Analysis by Transformer modelを推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor配列を定義\n",
    "# 応答変数をTensor配列に変換\n",
    "Y_ = torch.Tensor(Y)\n",
    "Y0_ = torch.Tensor(Y0)\n",
    "\n",
    "# 入力変数をTensor配列に変換\n",
    "word_id_ = torch.LongTensor(word_id)\n",
    "word_box_ = torch.LongTensor(word_box)\n",
    "pt_box_ = torch.LongTensor(pt_box)\n",
    "pt_id1_ = torch.LongTensor(pt_id1)\n",
    "pt_id2_ = torch.LongTensor(pt_id2)\n",
    "phrase_id_ = torch.LongTensor(phrase_id)\n",
    "phrase_box_ = torch.LongTensor(phrase_box)\n",
    "phrase_index_ = torch.LongTensor(phrase_index)\n",
    "feature_phrase_ = torch.LongTensor(feature_phrase)\n",
    "distance_ = torch.Tensor(distance[:, np.newaxis])\n",
    "unique_phrase = torch.LongTensor(torch.arange(phrase).long())\n",
    "\n",
    "word_id0_ = torch.LongTensor(word_id0)\n",
    "word_box0_ = torch.LongTensor(word_box0)\n",
    "pt_box0_ = torch.LongTensor(pt_box0)\n",
    "pt_id01_ = torch.LongTensor(pt_id01)\n",
    "pt_id02_ = torch.LongTensor(pt_id02)\n",
    "phrase_id0_ = torch.LongTensor(phrase_id0)\n",
    "phrase_box0_ = torch.LongTensor(phrase_box0)\n",
    "phrase_index0_ = torch.LongTensor(phrase_index0)\n",
    "feature_phrase0_ = torch.LongTensor(feature_phrase0)\n",
    "distance0_ = torch.Tensor(distance0[:, np.newaxis])\n",
    "unique_phrase0 = torch.LongTensor(torch.arange(phrase0).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "# 埋め込み層を定義\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, in_fearture, out_features, v, max_pt1, max_pt2):\n",
    "        super().__init__()\n",
    "        self.theta_v = nn.Embedding(num_embeddings=v, embedding_dim=in_features)\n",
    "        self.theta_h1 = nn.Embedding(num_embeddings=max_pt1, embedding_dim=in_features)\n",
    "        self.theta_h21 = nn.Embedding(num_embeddings=max_pt2, embedding_dim=out_features)\n",
    "        self.theta_h22 = nn.Embedding(num_embeddings=max_pt2, embedding_dim=out_features)\n",
    "        self.gamma_v = nn.Linear(in_features=in_features, out_features=out_features, bias=False)\n",
    "        self.gamma_h = nn.Linear(in_features=in_features, out_features=out_features, bias=False)\n",
    "        \n",
    "    def forward(self, word_id, pt_id, v):\n",
    "        theta_v = self.theta_v(torch.arange(v))\n",
    "        theta_h1 = self.theta_h1(torch.arange(max_pt1))\n",
    "        theta_h21 = self.theta_h21(torch.arange(max_pt2))\n",
    "        theta_h22 = self.theta_h22(torch.arange(max_pt2))\n",
    "        v_features = F.relu(self.gamma_v(theta_v))[word_id, ]\n",
    "        h_features = F.relu(self.gamma_h(theta_h1))[pt_id, ]\n",
    "        features = v_features + h_features\n",
    "        return features, theta_h21, theta_h22\n",
    "    \n",
    "# 重み層を定義\n",
    "class Weights(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Linear(in_features=out_features, out_features=1, bias=False)\n",
    "        \n",
    "    def forward(self, features, phrase_id, unique_phrase):\n",
    "        logit = torch.exp(self.gamma(features).reshape(-1))\n",
    "        logit_partition = torch.zeros_like(unique_phrase, dtype=torch.float).scatter_add_(0, phrase_id, logit)[phrase_id]\n",
    "        weights = logit / logit_partition\n",
    "        return weights[:, np.newaxis]\n",
    "    \n",
    "# Self Attention層を定義\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.gamma_k = nn.Linear(in_features, in_features)\n",
    "        self.gamma_q = nn.Linear(in_features, in_features)\n",
    "        self.gamma_g = nn.Linear(in_features, in_features)\n",
    "        self.gamma_o = nn.Linear(in_features, in_features)\n",
    "\n",
    "        nn.init.normal_(self.gamma_k.weight, std=0.02)\n",
    "        nn.init.normal_(self.gamma_q.weight, std=0.02)\n",
    "        nn.init.normal_(self.gamma_g.weight, std=0.02)\n",
    "        nn.init.normal_(self.gamma_o.weight, std=0.02)\n",
    "        \n",
    "    def forward(self, features, id_box, k):\n",
    "        # 全結合層で特徴量を変換\n",
    "        hidden_k = self.gamma_k(features)\n",
    "        hidden_q = self.gamma_q(features)\n",
    "        hidden_g = self.gamma_g(features)\n",
    "\n",
    "        # Attention Mapを定義\n",
    "        input_mask = torch.BoolTensor(id_box==k)\n",
    "        weights = torch.matmul(hidden_q, hidden_k.transpose(2, 1)) / np.sqrt(in_features)\n",
    "        mask = input_mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask==1, -1e9)\n",
    "        normalized_weights = F.softmax(weights, dim=2)\n",
    "\n",
    "        # Attention Mapの特徴量を変換\n",
    "        output = torch.matmul(normalized_weights, hidden_g)\n",
    "        output = self.gamma_o(output)\n",
    "        return output, normalized_weights\n",
    "    \n",
    "# Transformer Block層を定義\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.attention_model = Attention(in_features, out_features)\n",
    "        self.gamma_f1 = nn.Linear(in_features, out_features)\n",
    "        self.gamma_f2 = nn.Linear(out_features, in_features)\n",
    "        self.layernorm1 = nn.LayerNorm(in_features)\n",
    "        self.layernorm2 = nn.LayerNorm(in_features)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        nn.init.normal_(self.gamma_f1.weight, std=0.02)\n",
    "        nn.init.normal_(self.gamma_f2.weight, std=0.02)\n",
    "        \n",
    "    def forward(self, features, id_box, k):\n",
    "        # Self Attentionで特徴量を変換\n",
    "        normalized_features = self.layernorm1(features)\n",
    "        attention_features, normalized_weights = self.attention_model(features, id_box, k)\n",
    "\n",
    "        # 正規化とfeed forward層\n",
    "        dropout_attention = features + self.dropout1(attention_features)\n",
    "        normalized_attention = self.layernorm2(dropout_attention)\n",
    "        features_ff1 = self.dropout2(F.relu(self.gamma_f1(normalized_attention)))\n",
    "        features_ff2 = dropout_attention + self.gamma_f2(features_ff1)\n",
    "        return features_ff2\n",
    "    \n",
    "# 行列分解層を定義\n",
    "class DMF(nn.Module):\n",
    "    def __init__(self, in_features, out_features, classes, C):\n",
    "        super().__init__()\n",
    "        self.gamma11 = nn.ModuleList([nn.Linear(in_features, out_features) for j in range(C)])\n",
    "        self.gamma12 = nn.ModuleList([nn.Linear(in_features, out_features) for j in range(C)])\n",
    "        self.gamma21 = nn.Linear(2*out_features, classes, bias=False)\n",
    "        self.gamma22 = nn.Linear(out_features, classes, bias=False)\n",
    "        \n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.gamma11[0].weight, std=0.02)\n",
    "        nn.init.normal_(self.gamma11[1].weight, std=0.02)\n",
    "        nn.init.normal_(self.gamma12[0].weight, std=0.02)\n",
    "        nn.init.normal_(self.gamma12[1].weight, std=0.02)\n",
    "        nn.init.normal_(self.gamma21.weight, std=0.02)\n",
    "        # nn.init.normal_(self.gamma21.bias, 0)\n",
    "        nn.init.normal_(self.gamma22.weight, std=0.02)\n",
    "        \n",
    "    def forward(self, x1, x2, distance):\n",
    "        ff1 = F.relu(distance*self.gamma11[0](x1) + (1-distance)*self.gamma11[1](x1))\n",
    "        ff2 = F.relu(distance*self.gamma12[0](x2) + (1-distance)*self.gamma12[1](x2))\n",
    "        logit = self.gamma21(torch.cat((ff1, ff2), dim=1)) + self.gamma22(ff1 * ff2)\n",
    "        return logit\n",
    "    \n",
    "# 結合層を定義\n",
    "class Joint(nn.Module):\n",
    "    def __init__(self, in_features, out_features, out_dim, v, max_pt1, max_pt2, C, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(in_features, out_features, v, max_pt1, max_pt2)\n",
    "        self.weights = Weights(out_features)\n",
    "        self.transformer_model21_1 = Transformer(out_features, out_features, dropout_prob)\n",
    "        self.transformer_model21_2 = Transformer(out_features, out_features, dropout_prob)\n",
    "        self.transformer_model22_1 = Transformer(out_features, out_features, dropout_prob)\n",
    "        self.transformer_model22_2 = Transformer(out_features, out_features, dropout_prob)\n",
    "        self.dmf_model = DMF(out_features, out_dim, classes, C)\n",
    "        \n",
    "    def forward(self, word_id, pt_id1, pt_id2, phrase_id, unique_phrase, phrase_box, phrase_index, feature_phrase, distance, \n",
    "                N, D, phrase, F1, F2, max_m):\n",
    "        features, theta_h21, theta_h22 = self.embedding(word_id, pt_id1, v)\n",
    "        word_weights = self.weights(features, phrase_id, unique_phrase)\n",
    "        phrase_tensor = phrase_id.view(N, 1).expand(-1, out_features) \n",
    "        hidden_weights = torch.zeros(phrase, out_features).scatter_add_(0, phrase_tensor, word_weights * features)\n",
    "        hidden_features1 = torch.cat((hidden_weights + theta_h21[pt_id2, ], zeros), 0)[phrase_box, ]\n",
    "        hidden_features2 = torch.cat((hidden_weights + theta_h21[pt_id2, ], zeros), 0)[phrase_box, ]\n",
    "        \n",
    "        features_transformer21_1 = self.transformer_model21_1(hidden_features1, phrase_box, phrase)\n",
    "        features_transformer21_2 = self.transformer_model21_2(features_transformer21_1, phrase_box, phrase)\n",
    "        features_transformer22_1 = self.transformer_model22_1(hidden_features2, phrase_box, phrase)\n",
    "        features_transformer22_2 = self.transformer_model22_2(features_transformer22_1, phrase_box, phrase)\n",
    "        x1 = features_transformer21_2.reshape(D*max_m, out_features)[phrase_index, ][feature_phrase[:, 0], ]\n",
    "        x2 = features_transformer22_2.reshape(D*max_m, out_features)[phrase_index, ][feature_phrase[:, 1], ]\n",
    "        logit = self.dmf_model(x1, x2, distance)\n",
    "        return logit\n",
    "\n",
    "\n",
    "# 早期終了アルゴリズム\n",
    "class EarlyStopping:\n",
    "    '''\n",
    "    早期終了 (early stopping)\n",
    "    '''\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "# ハイパーパラメータを定義\n",
    "in_features = 24\n",
    "out_features = 48\n",
    "out_dim = 128\n",
    "dropout_prob = 0.1\n",
    "input_mask = torch.BoolTensor(phrase_box==phrase)\n",
    "input_mask0 = torch.BoolTensor(phrase_box0==phrase0)\n",
    "zeros = torch.Tensor([0.0]).repeat(out_features).reshape(1, out_features)\n",
    "phrase_flag = torch.LongTensor(phrase_box!=phrase)\n",
    "\n",
    "# 対数尤度を定義\n",
    "def loglike(Y, logit):\n",
    "    Prob = np.exp(logit) / (1 + np.exp(logit))\n",
    "    Prob[Prob==1.0] = 0.9999999\n",
    "    Prob[Prob==0.0] = 0.0000001\n",
    "    LL = np.sum(Y * np.log(Prob))\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アルゴリズムの定義\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Joint(in_features, out_features, out_dim, v, max_pt1, max_pt2, C, dropout_prob).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "optimizer = optimizers.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def compute_loss(t, y):\n",
    "    return criterion(t, y)\n",
    "\n",
    "\n",
    "def train_step(y, word_id, pt_id1, pt_id2, phrase_id, unique_phrase, phrase_box, phrase_index, feature_phrase, distance, \n",
    "               N, D, phrase, F1, F2, max_m, model, optimizer):\n",
    "    model.train()\n",
    "    mu = model(word_id, pt_id1, pt_id2, phrase_id, unique_phrase, phrase_box, phrase_index, feature_phrase, distance, \n",
    "               N, D, phrase, F1, F2, max_m)\n",
    "    Lho = compute_loss(mu, y)\n",
    "    optimizer.zero_grad()\n",
    "    Lho.backward()\n",
    "    optimizer.step()\n",
    "    return Lho, mu\n",
    "\n",
    "def val_step(y, word_id, pt_id1, pt_id2, phrase_id, unique_phrase, phrase_box, phrase_index, feature_phrase, distance, \n",
    "             N, D, phrase, F1, F2, max_m, model):\n",
    "    model.eval()\n",
    "    mu = model(word_id, pt_id1, pt_id2, phrase_id, unique_phrase, phrase_box, phrase_index, feature_phrase, distance, \n",
    "               N, D, phrase, F1, F2, max_m)\n",
    "    Lho = compute_loss(mu, y)\n",
    "    return Lho, mu\n",
    "\n",
    "\n",
    "# モデルの設定\n",
    "epochs = 200\n",
    "n_batches_train = 100\n",
    "n_batches_val = 100\n",
    "batch_size = D // n_batches_train\n",
    "batch_size0 = D0 // n_batches_val\n",
    "batch_index = np.array_split(np.arange(D), n_batches_train)\n",
    "batch_index0 = np.array_split(np.arange(D0), n_batches_val)\n",
    "mini_batch_size = np.array([len(batch_index[i]) for i in range(n_batches_train)])\n",
    "mini_batch_size0 = np.array([len(batch_index0[i]) for i in range(n_batches_val)])\n",
    "es = EarlyStopping(patience=3, verbose=1)\n",
    "hist = {\"train_loglike\": [], \"val_loglike\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 確率的勾配法でモデルパラメータを推定\n",
    "for rp in range(epochs):\n",
    "    \n",
    "    # モデルを学習\n",
    "    random_index = np.argsort(np.random.uniform(0, 1, D)).astype(\"int\")\n",
    "    preds_train = []\n",
    "    y_train = []\n",
    "    train_loglike = np.repeat(0.0, n_batches_train)\n",
    "\n",
    "    # ミニバッチごとに学習\n",
    "    for batch in range(n_batches_train):\n",
    "\n",
    "        # データを定義\n",
    "        # インデックスを定義\n",
    "        size = mini_batch_size[batch]\n",
    "        index = np.sort(random_index[batch_index[batch]])\n",
    "        index1 = np.hstack(([d_list1[index[i]] for i in range(size)]))\n",
    "        index2 = np.hstack(([d_list2[index[i]] for i in range(size)]))\n",
    "        index3 = np.hstack(([feature_list[index[i]] for i in range(size)]))\n",
    "        N_ = len(index2)\n",
    "        F1_ = len(index3)\n",
    "\n",
    "        # ミニバッチを定義\n",
    "        Y_ = torch.Tensor(Y[index3])\n",
    "        input_mask_ = input_mask[index, ]\n",
    "        word_id_ = torch.LongTensor(word_id[index2])\n",
    "        pt_id1_ = torch.LongTensor(pt_id1[index2])\n",
    "        pt_id2_ = torch.LongTensor(pt_id2[index1])\n",
    "        phrase_id_ = phrase_id[index2]\n",
    "        phrase_no_ = np.repeat(torch.arange(len(index1)), np.unique(phrase_id_, return_counts=True)[1])\n",
    "        phrase_ = torch.max(phrase_no_) + 1\n",
    "        phrase_index_ = torch.where(input_mask_.reshape(-1)==False)[0]\n",
    "        unique_phrase = torch.arange(phrase_)\n",
    "        feature_phrase_ = feature_phrase[index3, ]\n",
    "        distance_ = torch.Tensor(distance[index3])[:, np.newaxis]\n",
    "\n",
    "        # idを連番に置き換える\n",
    "        phrase_df = pd.DataFrame({\"phrase_id\": np.unique(phrase_id_), \"id\": np.arange(phrase_)})\n",
    "        phrase_box_ = torch.LongTensor([phrase_]).repeat(size*max_m)\n",
    "        phrase_box_[phrase_index_] = torch.arange(phrase_, dtype=torch.long)\n",
    "        phrase_box_ = phrase_box_.reshape(size, max_m)\n",
    "        feature_no1_ = pd.merge(pd.DataFrame({\"phrase_id\": feature_phrase_[:, 0]}), phrase_df, on=\"phrase_id\", how=\"left\")\n",
    "        feature_no2_ = pd.merge(pd.DataFrame({\"phrase_id\": feature_phrase_[:, 1]}), phrase_df, on=\"phrase_id\", how=\"left\")\n",
    "        feature_no_ = torch.LongTensor(np.array(pd.concat((feature_no1_[\"id\"], feature_no2_[\"id\"] ), axis=1)))\n",
    "\n",
    "        # ADAMでミニバッチを学習\n",
    "        # パラメータを更新\n",
    "        Lho, mu = train_step(Y_, word_id_, pt_id1_, pt_id2_, phrase_no_, unique_phrase, phrase_box_, phrase_index_, feature_no_, distance_, \n",
    "                             N_, size, phrase_, F1_, F2, max_m, model, optimizer)\n",
    "\n",
    "        # 学習結果を格納\n",
    "        preds_train.append(mu.detach().numpy().astype(\"float\"))\n",
    "        y_train.append(Y_.detach().numpy().astype(\"int\") )\n",
    "        train_loglike[batch] = np.array(-Lho.detach(), dtype=\"float\")\n",
    "\n",
    "    # 学習データの対数尤度を更新\n",
    "    preds_train = np.vstack((preds_train))\n",
    "    y_train = np.vstack((y_train))\n",
    "    LL = loglike(y_train, preds_train)\n",
    "    hist[\"train_loglike\"].append(LL)\n",
    "\n",
    "\n",
    "    # モデルの評価\n",
    "    # 推定されたモデルを評価\n",
    "    Lho, mu = val_step(Y0_, word_id0_, pt_id01_, pt_id02_, phrase_id0_, unique_phrase0, phrase_box0_, phrase_index0_, feature_phrase0_,\n",
    "                       distance0_, N0, D0, phrase0, F01, F2, max_m, model)\n",
    "\n",
    "    # テストデータの対数尤度を更新\n",
    "    preds_val = mu.detach().numpy().astype(\"float\")\n",
    "    y_val = Y0_.detach().numpy().astype(\"int\")\n",
    "    val_loglike = -Lho.detach().numpy().astype(\"float\")\n",
    "    LL0 = loglike(Y0, preds_val)\n",
    "    hist[\"val_loglike\"].append(LL0)\n",
    "\n",
    "\n",
    "    # 学習結果を表示\n",
    "    print(rp)\n",
    "    print(np.round([np.sum(train_loglike), np.sum(val_loglike)], 1))\n",
    "    print(np.round([LL, LL0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat((pd.DataFrame(Y0), pd.DataFrame(np.round(preds_val, 1))), axis=1).iloc[np.where(Y0[:, j]==1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((pd.DataFrame(np.vstack((y_train))), pd.DataFrame(np.round(np.vstack((preds_train)), 1))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 早期終了\n",
    "if es(-np.sum(val_loglike))==True:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob = np.exp(preds_val) / np.sum(np.exp(preds_val), axis=1)[:, np.newaxis]\n",
    "\n",
    "Y0 * np.log(Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike(Y, logit):\n",
    "    Prob = np.exp(logit) / (1 + np.exp(logit))\n",
    "    Prob[Prob==1.0] = 0.9999999\n",
    "    Prob[Prob==0.0] = 0.0000001\n",
    "    LL = np.sum(Y * np.log(Prob) + (1-Y)*np.log(1-Prob))\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
