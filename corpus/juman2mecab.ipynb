{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import itertools\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import MeCab\n",
    "from numpy.random import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JumanをMeCabにマッピング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "path = \"D:/Statistics/data/NLP/\"\n",
    "corpus_path = path + \"corpus/\"\n",
    "kyoto_corpus = pd.read_csv(corpus_path + \"kyoto_info.csv\").iloc[:, 1:]\n",
    "kwdlc_corpus = pd.read_csv(corpus_path + \"kwdlc_info.csv\")\n",
    "kyoto_corpus = kyoto_corpus.sort_values([\"sentence_id\", \"serial_no\"])\n",
    "kwdlc_corpus = kwdlc_corpus.sort_values([\"sentence_id\", \"serial_no\"])\n",
    "kyoto_corpus.index = np.arange(kyoto_corpus.shape[0])\n",
    "kwdlc_corpus.index = np.arange(kwdlc_corpus.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書idとインデックスを定義\n",
    "# idを定義\n",
    "unique_sentence1 = np.unique(kyoto_corpus[\"sentence_id\"])\n",
    "unique_sentence2 = np.unique(kwdlc_corpus[\"sentence_id\"])\n",
    "sentence_df1 = pd.DataFrame({\"id\": np.arange(len(unique_sentence1)), \"sentence_id\": unique_sentence1})\n",
    "sentence_df2 = pd.DataFrame({\"id\": np.arange(len(unique_sentence2)), \"sentence_id\": unique_sentence2})\n",
    "kyoto_corpus[\"d_id\"] = pd.merge(kyoto_corpus[[\"sentence_id\"]], sentence_df1, on=\"sentence_id\", how=\"left\")[\"id\"]\n",
    "kwdlc_corpus[\"d_id\"] = pd.merge(kwdlc_corpus[[\"sentence_id\"]], sentence_df2, on=\"sentence_id\", how=\"left\")[\"id\"]\n",
    "d_id1 = np.array(kyoto_corpus[\"d_id\"])\n",
    "d_id2 = np.array(kwdlc_corpus[\"d_id\"])\n",
    "D1 = len(unique_sentence1)\n",
    "D2 = len(unique_sentence2)\n",
    "N1 = len(d_id1)\n",
    "N2 = len(d_id2)\n",
    "\n",
    "# インデックスを定義\n",
    "d_list1 = []\n",
    "d_list2 = []\n",
    "for i in range(D1):\n",
    "    d_list1.append(np.where(d_id1==i)[0].astype(\"int\"))\n",
    "for i in range(D2):\n",
    "    d_list2.append(np.where(d_id2==i)[0].astype(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jumanの結果に対してidを付与する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kyoto corpusの単語の文字列のidを定義\n",
    "# 文書を抽出\n",
    "kyoto_word = np.array(kyoto_corpus[\"word\"])\n",
    "kyoto_word_no1 = [i for i in range(D1)]\n",
    "max_id = 0\n",
    "\n",
    "# 文書ごとに処理を行う\n",
    "for i in range(D1):\n",
    "    \n",
    "    # 単語ごとにidを付与\n",
    "    index = d_list1[i]\n",
    "    n = len(index)\n",
    "    word_no = [j for j in range(n)]\n",
    "    for j in range(n):\n",
    "        m = len(kyoto_word[index[j]])\n",
    "        if j==0:\n",
    "            serial_no = np.arange(m)[:, np.newaxis] \n",
    "            word_serial = np.repeat(max_id, m)[:, np.newaxis]\n",
    "            word_id = np.repeat(j, m)[:, np.newaxis]\n",
    "            word_no[j] = np.hstack((word_serial, serial_no, word_id))\n",
    "            max_no = np.max(word_no[j][:, 1], axis=0)\n",
    "            max_id += 1\n",
    "        elif j > 0:\n",
    "            serial_no = np.arange(m)[:, np.newaxis] + max_no + 1\n",
    "            word_serial = np.repeat(max_id, m)[:, np.newaxis]\n",
    "            word_id = np.repeat(j, m)[:, np.newaxis]\n",
    "            word_no[j] = np.hstack((word_serial, serial_no, word_id))\n",
    "            max_no = np.max(word_no[j][:, 1], axis=0)\n",
    "            max_id += 1\n",
    "            \n",
    "    # リストに格納\n",
    "    kyoto_word_no1[i] = np.vstack((word_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwdlc corpusの単語の文字列のidを定義\n",
    "# 文書を抽出\n",
    "kwdlc_word = np.array(kwdlc_corpus[\"word\"])\n",
    "kwdlc_word_no1 = [i for i in range(D2)]\n",
    "max_id = 0\n",
    "\n",
    "# 文書ごとに処理を行う\n",
    "for i in range(D2):\n",
    "    \n",
    "    # 単語ごとにidを付与\n",
    "    index = d_list2[i]\n",
    "    n = len(index)\n",
    "    word_no = [j for j in range(n)]\n",
    "    for j in range(n):\n",
    "        m = len(kwdlc_word[index[j]])\n",
    "        if j==0:\n",
    "            serial_no = np.arange(m)[:, np.newaxis] \n",
    "            word_serial = np.repeat(max_id, m)[:, np.newaxis]\n",
    "            word_id = np.repeat(j, m)[:, np.newaxis]\n",
    "            word_no[j] = np.hstack((word_serial, serial_no, word_id))\n",
    "            max_no = np.max(word_no[j][:, 1], axis=0)\n",
    "            max_id += 1\n",
    "        elif j > 0:\n",
    "            serial_no = np.arange(m)[:, np.newaxis] + max_no + 1\n",
    "            word_serial = np.repeat(max_id, m)[:, np.newaxis]\n",
    "            word_id = np.repeat(j, m)[:, np.newaxis]\n",
    "            word_no[j] = np.hstack((word_serial, serial_no, word_id))\n",
    "            max_no = np.max(word_no[j][:, 1], axis=0)\n",
    "            max_id += 1\n",
    "            \n",
    "    # リストに格納\n",
    "    kwdlc_word_no1[i] = np.vstack((word_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeCabの結果に対してidを付与する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kyoto corpusの単語の文字列のidを定義\n",
    "# 結果の格納用配列\n",
    "mecab_columns = [\"word\", \"word_class\", \"class_detail1\", \"class_detail2\", \"class_detail3\",\n",
    "                 \"inflectional1\", \"inflectional2\", \"genkei\", \"readings1\", \"readings2\"]\n",
    "parsed_list = [i for i in range(D1)]\n",
    "kyoto_word_no2 = [i for i in range(D1)]\n",
    "max_id = 0\n",
    "\n",
    "# 文章ごとにMeCabを実行\n",
    "for i in range(D1):\n",
    "    text = kyoto_corpus[\"word\"].iloc[d_list1[i]].str.cat()\n",
    "    mecab = MeCab.Tagger()\n",
    "    res = mecab.parse(text)\n",
    "    parsed_split = pd.Series(res.split(\"\\n\")).str.split('\\t|,').tolist()\n",
    "    parsed_list[i] = pd.DataFrame.from_records(parsed_split[0:len(parsed_split)-2])\n",
    "    parsed_list[i].columns = mecab_columns\n",
    "    n = parsed_list[i].shape[0]\n",
    "    \n",
    "    # 単語ごとにidを付与\n",
    "    parsed_result = parsed_list[i]\n",
    "    parsed_word = np.array(parsed_result[\"word\"])\n",
    "    max_no = 0\n",
    "    word_no = [j for j in range(n)]\n",
    "    for j in range(n):\n",
    "        if j==0:\n",
    "            m = len(parsed_word[j])\n",
    "            serial_no = np.arange(m)[:, np.newaxis]\n",
    "            word_serial = np.repeat(max_id, m)[:, np.newaxis]\n",
    "            word_id = np.repeat(j, m)[:, np.newaxis]\n",
    "            word_no[j] = np.hstack((word_serial, serial_no, word_id))\n",
    "            max_no = np.max(serial_no)\n",
    "            max_id += 1\n",
    "        elif j > 0:\n",
    "            m = len(parsed_word[j])\n",
    "            serial_no = np.arange(m)[:, np.newaxis] + max_no + 1\n",
    "            word_serial = np.repeat(max_id, m)[:, np.newaxis]\n",
    "            word_id = np.repeat(j, m)[:, np.newaxis]\n",
    "            word_no[j] = np.hstack((word_serial, serial_no, word_id))\n",
    "            max_no = np.max(serial_no)\n",
    "            max_id += 1\n",
    "    word_no = np.vstack((word_no))\n",
    "\n",
    "    # リストに格納\n",
    "    kyoto_word_no2[i] = np.vstack((word_no))\n",
    "    \n",
    "# リストを結合\n",
    "kyoto_parsed = pd.concat((parsed_list), axis=0)\n",
    "kyoto_parsed.index = np.arange(kyoto_parsed.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwdlc corpusの単語の文字列のidを定義\n",
    "# 結果の格納用配列\n",
    "mecab_columns = [\"word\", \"word_class\", \"class_detail1\", \"class_detail2\", \"class_detail3\",\n",
    "                 \"inflectional1\", \"inflectional2\", \"genkei\", \"readings1\", \"readings2\"]\n",
    "parsed_list = [i for i in range(D2)]\n",
    "kwdlc_word_no2 = [i for i in range(D2)]\n",
    "max_id = 0\n",
    "\n",
    "# 文章ごとにMeCabを実行\n",
    "for i in range(D2):\n",
    "    text = kwdlc_corpus[\"word\"].iloc[d_list2[i]].str.cat()\n",
    "    mecab = MeCab.Tagger()\n",
    "    res = mecab.parse(text)\n",
    "    parsed_split = pd.Series(res.split(\"\\n\")).str.split('\\t|,').tolist()\n",
    "    parsed_list[i] = pd.DataFrame.from_records(parsed_split[0:len(parsed_split)-2])\n",
    "    row = parsed_list[i].shape[0]\n",
    "    col = parsed_list[i].shape[1]\n",
    "    if col < len(mecab_columns):\n",
    "        parsed_list[i] = pd.concat((parsed_list[i], pd.DataFrame(np.full((row, len(mecab_columns)-col), \"*\"))), axis=1)\n",
    "    parsed_list[i].columns = mecab_columns\n",
    "    n = parsed_list[i].shape[0]\n",
    "    \n",
    "    # 単語ごとにidを付与\n",
    "    parsed_result = parsed_list[i]\n",
    "    parsed_word = np.array(parsed_result[\"word\"])\n",
    "    max_no = 0\n",
    "    word_no = [j for j in range(n)]\n",
    "    for j in range(n):\n",
    "        if j==0:\n",
    "            m = len(parsed_word[j])\n",
    "            serial_no = np.arange(m)[:, np.newaxis]\n",
    "            word_serial = np.repeat(max_id, m)[:, np.newaxis]\n",
    "            word_id = np.repeat(j, m)[:, np.newaxis]\n",
    "            word_no[j] = np.hstack((word_serial, serial_no, word_id))\n",
    "            max_no = np.max(serial_no)\n",
    "            max_id += 1\n",
    "        elif j > 0:\n",
    "            m = len(parsed_word[j])\n",
    "            serial_no = np.arange(m)[:, np.newaxis] + max_no + 1\n",
    "            word_serial = np.repeat(max_id, m)[:, np.newaxis]\n",
    "            word_id = np.repeat(j, m)[:, np.newaxis]\n",
    "            word_no[j] = np.hstack((word_serial, serial_no, word_id))\n",
    "            max_no = np.max(serial_no)\n",
    "            max_id += 1\n",
    "    word_no = np.vstack((word_no))\n",
    "\n",
    "    # リストに格納\n",
    "    kwdlc_word_no2[i] = np.vstack((word_no))\n",
    "    \n",
    "# リストを結合\n",
    "kwdlc_parsed = pd.concat((parsed_list), axis=0)\n",
    "kwdlc_parsed.index = np.arange(kwdlc_parsed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeCabの結果をマッピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kyoto corpusの結果をマッピング\n",
    "# マッピングidを定義\n",
    "mapping_list = [i for i in range(D1)]\n",
    "for i in range(D1):\n",
    "    columns_name = [\"word_serial1\", \"word_serial2\"]\n",
    "    kyoto_id1 = pd.DataFrame(kyoto_word_no1[i], columns=[\"word_serial1\", \"serial_no\", \"word_id1\"])\n",
    "    kyoto_id2 = pd.DataFrame(kyoto_word_no2[i], columns=[\"word_serial2\", \"serial_no\", \"word_id2\"])\n",
    "    mapping = pd.merge(kyoto_id2, kyoto_id1, on=\"serial_no\", how=\"left\")[columns_name]\n",
    "    mapping_list[i] = np.array(mapping.iloc[np.where(mapping.duplicated()==False)[0]])\n",
    "kyoto_mapping = np.vstack((mapping_list))\n",
    "mapping_df = pd.DataFrame(kyoto_mapping, columns=[\"word_id1\", \"word_id2\"])\n",
    "\n",
    "# データフレームを結合\n",
    "columns1 = [\"doc_id\", \"d_id\", \"sentence_id\", \"word\", \"phrase_id\", \"phrase_dependency\", \"dependency_type1\", \"tag_id\", \n",
    "            \"tag_dependency\", \"dependency_type2\", \"rel\", \"target\", \"sid\", \"tag\"]\n",
    "columns2 = [\"word\", \"genkei\", \"word_class\", \"class_detail1\", \"class_detail2\", \"class_detail3\", \n",
    "            \"inflectional1\", \"inflectional2\"]\n",
    "columns = [\"doc_id\", \"d_id\", \"sentence_id\", \"word_id2\", \"word2\", \"genkei\", \"word_class\",\n",
    "           \"class_detail1\", \"class_detail2\", \"class_detail3\", \"inflectional1\", \"inflectional2\",\n",
    "           \"phrase_id\", \"phrase_dependency\", \"dependency_type1\", \"tag_id\", \"tag_dependency\", \n",
    "           \"dependency_type2\", \"rel\", \"target\", \"sid\", \"tag\"]\n",
    "temp1 = kyoto_corpus[columns1].iloc[kyoto_mapping[:, 0]]\n",
    "temp2 = kyoto_parsed[columns2].iloc[kyoto_mapping[:, 1]]\n",
    "temp1 = temp1.rename(columns={\"word\": \"word1\"})\n",
    "temp2 = temp2.rename(columns={\"word\": \"word2\"})\n",
    "temp1.index = np.arange(temp1.shape[0])\n",
    "temp2.index = np.arange(temp2.shape[0])\n",
    "temp = pd.concat((mapping_df, temp1, temp2), axis=1)\n",
    "new_kyoto_corpus = temp[columns].iloc[np.where(temp[\"word_id2\"].duplicated()==False)[0]]\n",
    "new_kyoto_corpus = new_kyoto_corpus.rename(columns={\"word_id2\": \"word_id\", \"word2\": \"word\"})\n",
    "del temp1, temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kwdlc corpusの結果をマッピング\n",
    "# マッピングidを定義\n",
    "mapping_list = [i for i in range(D2)]\n",
    "for i in range(D2):\n",
    "    columns_name = [\"word_serial1\", \"word_serial2\"]\n",
    "    kwdlc_id1 = pd.DataFrame(kwdlc_word_no1[i], columns=[\"word_serial1\", \"serial_no\", \"word_id1\"])\n",
    "    kwdlc_id2 = pd.DataFrame(kwdlc_word_no2[i], columns=[\"word_serial2\", \"serial_no\", \"word_id2\"])\n",
    "    mapping = pd.merge(kwdlc_id2, kwdlc_id1, on=\"serial_no\", how=\"left\")[columns_name]\n",
    "    mapping_list[i] = np.array(mapping.iloc[np.where(mapping.duplicated()==False)[0]])\n",
    "kwdlc_mapping = np.vstack((mapping_list))\n",
    "mapping_df = pd.DataFrame(kwdlc_mapping, columns=[\"word_id1\", \"word_id2\"])\n",
    "\n",
    "# データフレームを結合\n",
    "columns1 = [\"doc_id\", \"d_id\", \"sentence_id\", \"word\", \"phrase_id\", \"phrase_dependency\", \"dependency_type1\", \"tag_id\", \n",
    "            \"tag_dependency\", \"dependency_type2\", \"rel\", \"target\", \"sid\", \"tag\"]\n",
    "columns2 = [\"word\", \"genkei\", \"word_class\", \"class_detail1\", \"class_detail2\", \"class_detail3\", \n",
    "            \"inflectional1\", \"inflectional2\"]\n",
    "columns = [\"doc_id\", \"d_id\", \"sentence_id\", \"word_id2\", \"word2\", \"genkei\", \"word_class\",\n",
    "           \"class_detail1\", \"class_detail2\", \"class_detail3\", \"inflectional1\", \"inflectional2\",\n",
    "           \"phrase_id\", \"phrase_dependency\", \"dependency_type1\", \"tag_id\", \"tag_dependency\", \n",
    "           \"dependency_type2\", \"rel\", \"target\", \"sid\", \"tag\"]\n",
    "temp1 = kwdlc_corpus[columns1].iloc[kwdlc_mapping[:, 0]]\n",
    "temp2 = kwdlc_parsed[columns2].iloc[kwdlc_mapping[:, 1]]\n",
    "temp1 = temp1.rename(columns={\"word\": \"word1\"})\n",
    "temp2 = temp2.rename(columns={\"word\": \"word2\"})\n",
    "temp1.index = np.arange(temp1.shape[0])\n",
    "temp2.index = np.arange(temp2.shape[0])\n",
    "temp = pd.concat((mapping_df, temp1, temp2), axis=1)\n",
    "new_kwdlc_corpus = temp[columns].iloc[np.where(temp[\"word_id2\"].duplicated()==False)[0]]\n",
    "new_kwdlc_corpus = new_kwdlc_corpus.rename(columns={\"word_id2\": \"word_id\", \"word2\": \"word\"})\n",
    "new_kwdlc_corpus.index = np.arange(new_kwdlc_corpus.shape[0])\n",
    "del temp, temp1, temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_kyoto_corpus.to_excel(path + \"new_kyoto_corpus.xlsx\")\n",
    "new_kyoto_corpus.to_csv(path + \"new_kyoto_corpus.csv\", index=None)\n",
    "new_kwdlc_corpus.to_excel(path + \"new_kwdlc_corpus.xlsx\")\n",
    "new_kwdlc_corpus.to_csv(path + \"new_kwdlc_corpus.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フレーズ間の係り受け関係を定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kyoto Corpusの係り受け関係を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idの定義\n",
    "# 文書idを定義\n",
    "d_id = np.array(new_kyoto_corpus[\"d_id\"], dtype=\"int\")\n",
    "D = np.unique(d_id).shape[0]\n",
    "d_list = [i for i in range(D)]\n",
    "for i in range(D):\n",
    "    d_list[i] = np.where(d_id==i)[0].astype(\"int\")\n",
    "    \n",
    "# フレーズidの定義\n",
    "phrase_id = np.array(new_kyoto_corpus[\"phrase_id\"], dtype=\"int\")\n",
    "d = np.repeat(0, D)\n",
    "unique_phrase = [i for i in range(D)]\n",
    "for i in range(D):\n",
    "    unique_phrase[i] = np.unique(phrase_id[d_list[i]])\n",
    "    d[i] = unique_phrase[i].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係り受け関係と述語項関係を取得\n",
    "# データを抽出\n",
    "sentence_id = np.array(new_kyoto_corpus[\"sentence_id\"])\n",
    "phrase_id = np.array(new_kyoto_corpus[\"phrase_id\"], dtype=\"int\")\n",
    "tag_id = np.array(new_kyoto_corpus[\"tag_id\"], dtype=\"int\")\n",
    "phrase_dependency = np.array(new_kyoto_corpus[\"phrase_dependency\"], dtype=\"int\")\n",
    "dependency_type = np.array(new_kyoto_corpus[\"dependency_type1\"])\n",
    "rel = np.array(new_kyoto_corpus[\"rel\"])\n",
    "sid = np.array(new_kyoto_corpus[\"sid\"])\n",
    "tag = np.array(new_kyoto_corpus[\"tag\"])\n",
    "\n",
    "# フレーズ間のすべての組み合わせを取得\n",
    "feature_phrase_list = []\n",
    "feature_id_list = []\n",
    "for i in range(D):\n",
    "    flag = np.triu(np.full((d[i], d[i]), 1), k=1)\n",
    "    block1 = np.repeat(unique_phrase[i], d[i]).reshape(d[i], d[i])\n",
    "    block2 = np.tile(unique_phrase[i], d[i]).reshape(d[i], d[i])\n",
    "    feature_phrase_list.append(np.hstack((block1[flag==1][:, np.newaxis], block2[flag==1][:, np.newaxis])))\n",
    "    feature_id_list.append(np.repeat(i, len(feature_phrase_list[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 文書ごとに係り受け関係と述語項関係を取得\n",
    "# 結果の格納用配列\n",
    "dependency_list = [i for i in range(D)]\n",
    "rel_flag_list = [i for i in range(D)]\n",
    "rel_type_list = [i for i in range(D)]\n",
    "\n",
    "for i in range(D):\n",
    "    # 係り受け関係を取得\n",
    "    index = d_list[i]\n",
    "    phrase_id_ = phrase_id[index]\n",
    "    tag_id_ = tag_id[index]\n",
    "    feature_phrase_ = feature_phrase_list[i]\n",
    "    feature_str = feature_phrase_list[i].astype(\"U\").astype(\"object\")\n",
    "    dependency1 = feature_str[:, 0] + \"-\" + feature_str[:, 1]\n",
    "    dependency2 = phrase_id[index].astype(\"U\").astype(\"object\") + \"-\" + phrase_dependency[index].astype(\"U\").astype(\"object\")\n",
    "    dependency_list[i] = np.array(np.in1d(dependency1, dependency2), dtype=\"int\")\n",
    "\n",
    "    # 述語項関係のデータを定義\n",
    "    index_tag = np.where((pd.isna(tag[index])==False) & (pd.isna(sid[index])==False) & (pd.isna(rel[index])==False))[0].astype(\"int\")\n",
    "    m1 = len(index_tag)\n",
    "    target_sentence = sentence_id[index][0]\n",
    "    target_sid = sid[index][index_tag]\n",
    "    target_tag = tag[index][index_tag]\n",
    "    target_rel = rel[index][index_tag]\n",
    "\n",
    "    # 述語項関係を取得\n",
    "    rel_flag_ = np.repeat(0, len(feature_phrase_))\n",
    "    rel_type_ = np.repeat(\"\", len(feature_phrase_)).astype(\"object\")\n",
    "    for j1 in range(m1):\n",
    "        split_sid = str.split(target_sid[j1], \"; \")\n",
    "        split_tag = str.split(target_tag[j1], \"; \")\n",
    "        split_rel = str.split(target_rel[j1], \"; \")\n",
    "        m2 = len(split_sid)\n",
    "\n",
    "        for j2 in range(m2):\n",
    "            if (split_sid[j2]!=target_sentence) | (split_sid[j2]==\"\"):\n",
    "                continue\n",
    "            index_send = np.where(tag_id_==int(split_tag[j2]))[0].astype(\"int\")\n",
    "            if len(index_send)==0:\n",
    "                continue\n",
    "            send_phrase = phrase_id_[index_send][0]\n",
    "            receive_phrase = phrase_id_[index_tag[j1]]\n",
    "            \n",
    "            if send_phrase!=receive_phrase:\n",
    "                index_dependency = np.where((feature_phrase_[:, 0]==send_phrase) & (feature_phrase_[:, 1]==receive_phrase))[0].astype(\"int\")\n",
    "                rel_flag_[index_dependency] = 1\n",
    "                rel_type_[index_dependency] += split_rel[j2] + \"; \"\n",
    "\n",
    "    for j in range(rel_type_.shape[0]):\n",
    "        if rel_type_[j]!=\"\":\n",
    "            rel_type_[j] = pd.Series(np.unique(re.split(\" \", rel_type_[j]))).str.cat()\n",
    "            rel_type_[j] = re.sub(\";$\", \"\", rel_type_[j])\n",
    "            \n",
    "    # データを格納\n",
    "    rel_flag_list[i] = rel_flag_\n",
    "    rel_type_list[i] = rel_type_\n",
    "    \n",
    "# リストを配列に変換\n",
    "feature_id = np.hstack((feature_id_list))\n",
    "feature_phrase = np.vstack((feature_phrase_list))\n",
    "dependency = np.hstack((dependency_list))\n",
    "rel_flag = np.hstack((rel_flag_list))\n",
    "rel_type = np.hstack((rel_type_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 新しいphrase idを定義\n",
    "# データの定義\n",
    "phrase = np.unique(np.array(new_kyoto_corpus[\"d_id\"].astype(\"U\") + \"-\" + new_kyoto_corpus[\"phrase_id\"].astype(\"U\"))).shape[0]\n",
    "phrase_box1 = new_kyoto_corpus[[\"d_id\", \"phrase_id\", \"phrase_dependency\"]]\n",
    "phrase_box21 = pd.DataFrame(feature_phrase[:, 0], columns=[\"phrase_id\"])\n",
    "phrase_box22 = pd.DataFrame(feature_phrase[:, 1], columns=[\"phrase_id\"])\n",
    "\n",
    "# 新しいidの格納用配列\n",
    "phrase_no_list11 = [i for i in range(D1)]\n",
    "phrase_no_list12 = [i for i in range(D1)]\n",
    "phrase_no_list21 = [i for i in range(D1)]\n",
    "phrase_no_list22 = [i for i in range(D1)]\n",
    "max_id = 0\n",
    "\n",
    "# 文書ごとにphrase idを定義\n",
    "for i in range(D):\n",
    "    index1 = d_list[i]\n",
    "    index2 = np.where(feature_id==i)[0].astype(\"int\")\n",
    "    phrase_temp1 = phrase_box1.iloc[index1]\n",
    "    phrase_temp21 = phrase_box21.iloc[index2]\n",
    "    phrase_temp22 = phrase_box22.iloc[index2]\n",
    "    phrase_id = np.unique(np.array(phrase_temp1[\"phrase_id\"], dtype=\"int\"))\n",
    "    m = len(phrase_id)\n",
    "    no = np.arange(m) + max_id\n",
    "\n",
    "    target_phrase = pd.DataFrame({\"phrase_id\": phrase_id, \"phrase_no\": no})\n",
    "    phrase_no_list11[i] = np.array(pd.merge(phrase_temp1, target_phrase, on=\"phrase_id\", how=\"left\")[\"phrase_no\"])\n",
    "    phrase_no_list12[i] = np.array(pd.merge(phrase_temp1, target_phrase, \n",
    "                                            left_on=\"phrase_dependency\", right_on=\"phrase_id\", how=\"left\")[\"phrase_no\"])\n",
    "    phrase_no_list21[i] = np.array(pd.merge(phrase_temp21, target_phrase, on=\"phrase_id\", how=\"left\")[\"phrase_no\"])\n",
    "    phrase_no_list22[i] = np.array(pd.merge(phrase_temp22, target_phrase, on=\"phrase_id\", how=\"left\")[\"phrase_no\"])\n",
    "    max_id = np.max(no) + 1\n",
    "    \n",
    "# リストを配列に変換\n",
    "phrase_no = np.hstack((phrase_no_list11))\n",
    "dependency_no = np.hstack((phrase_no_list12))\n",
    "dependency_no[np.isnan(dependency_no)] = -1\n",
    "dependency_no = np.array(dependency_no, dtype=\"int\")\n",
    "feature_no = np.hstack((np.hstack((phrase_no_list21))[:, np.newaxis], np.hstack((phrase_no_list22))[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームの作成と出力\n",
    "# カラムを定義\n",
    "kyoto_columns = ['serial_no', 'doc_id', 'd_id', 'sentence_id', 'phrase_id', 'phrase_dependency', 'phrase_no', 'dependency_no',\n",
    "                 'word', 'genkei', 'word_class', 'class_detail1', 'class_detail2', 'class_detail3', 'inflectional1', 'inflectional2',\n",
    "                 'dependency_type1', 'tag_id', 'tag_dependency', 'dependency_type2', 'rel', 'target', 'sid', 'tag']\n",
    "\n",
    "# データフレームを作成\n",
    "kyoto_dependency_feature = pd.DataFrame({\"serial_no\": np.arange(feature_id.shape[0]), \"d_id\": feature_id, \n",
    "                                         \"phrase_id1\": feature_phrase[:, 0], \"phrase_id2\": feature_phrase[:, 1],\n",
    "                                         \"phrase_no1\": feature_no[:, 0], \"phrase_no2\": feature_no[:, 1], \"dependency\": dependency,\n",
    "                                         \"rel\": rel_flag, \"rel_type\": rel_type})\n",
    "new_kyoto_corpus[\"serial_no\"] = np.arange(new_kyoto_corpus.shape[0])\n",
    "new_kyoto_corpus[\"phrase_no\"] = phrase_no\n",
    "new_kyoto_corpus[\"dependency_no\"] = dependency_no\n",
    "new_kyoto_corpus = new_kyoto_corpus[kyoto_columns]\n",
    "\n",
    "# データフレームを出力\n",
    "new_kyoto_corpus.to_excel(path + \"new_kyoto_corpus.xlsx\")\n",
    "new_kyoto_corpus.to_csv(path + \"new_kyoto_corpus.csv\", index=None)\n",
    "kyoto_dependency_feature.to_excel(path + \"new_kyoto_dependency_feature.xlsx\")\n",
    "kyoto_dependency_feature.to_csv(path + \"new_kyoto_dependency_feature.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kwdlc corpusの係り受け関係を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idの定義\n",
    "# 文書idを定義\n",
    "d_id = np.array(new_kwdlc_corpus[\"d_id\"], dtype=\"int\")\n",
    "D = np.unique(d_id).shape[0]\n",
    "d_list = [i for i in range(D)]\n",
    "for i in range(D):\n",
    "    d_list[i] = np.where(d_id==i)[0].astype(\"int\")\n",
    "    \n",
    "# フレーズidの定義\n",
    "phrase_id = np.array(new_kwdlc_corpus[\"phrase_id\"], dtype=\"int\")\n",
    "d = np.repeat(0, D)\n",
    "unique_phrase = [i for i in range(D)]\n",
    "for i in range(D):\n",
    "    unique_phrase[i] = np.unique(phrase_id[d_list[i]])\n",
    "    d[i] = unique_phrase[i].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係り受け関係と述語項関係を取得\n",
    "# データを抽出\n",
    "sentence_id = np.array(new_kwdlc_corpus[\"sentence_id\"])\n",
    "phrase_id = np.array(new_kwdlc_corpus[\"phrase_id\"], dtype=\"int\")\n",
    "tag_id = np.array(new_kwdlc_corpus[\"tag_id\"], dtype=\"int\")\n",
    "phrase_dependency = np.array(new_kwdlc_corpus[\"phrase_dependency\"], dtype=\"int\")\n",
    "dependency_type = np.array(new_kwdlc_corpus[\"dependency_type1\"])\n",
    "rel = np.array(new_kwdlc_corpus[\"rel\"])\n",
    "sid = np.array(new_kwdlc_corpus[\"sid\"])\n",
    "tag = np.array(new_kwdlc_corpus[\"tag\"])\n",
    "\n",
    "# フレーズ間のすべての組み合わせを取得\n",
    "feature_phrase_list = []\n",
    "feature_id_list = []\n",
    "for i in range(D):\n",
    "    flag = np.triu(np.full((d[i], d[i]), 1), k=1)\n",
    "    block1 = np.repeat(unique_phrase[i], d[i]).reshape(d[i], d[i])\n",
    "    block2 = np.tile(unique_phrase[i], d[i]).reshape(d[i], d[i])\n",
    "    feature_phrase_list.append(np.hstack((block1[flag==1][:, np.newaxis], block2[flag==1][:, np.newaxis])))\n",
    "    feature_id_list.append(np.repeat(i, len(feature_phrase_list[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書ごとに係り受け関係と述語項関係を取得\n",
    "# 結果の格納用配列\n",
    "dependency_list = [i for i in range(D)]\n",
    "rel_flag_list = [i for i in range(D)]\n",
    "rel_type_list = [i for i in range(D)]\n",
    "\n",
    "for i in range(D):\n",
    "    # 係り受け関係を取得\n",
    "    index = d_list[i]\n",
    "    phrase_id_ = phrase_id[index]\n",
    "    tag_id_ = tag_id[index]\n",
    "    feature_phrase_ = feature_phrase_list[i]\n",
    "    feature_str = feature_phrase_list[i].astype(\"U\").astype(\"object\")\n",
    "    dependency1 = feature_str[:, 0] + \"-\" + feature_str[:, 1]\n",
    "    dependency2 = phrase_id[index].astype(\"U\").astype(\"object\") + \"-\" + phrase_dependency[index].astype(\"U\").astype(\"object\")\n",
    "    dependency_list[i] = np.array(np.in1d(dependency1, dependency2), dtype=\"int\")\n",
    "\n",
    "    # 述語項関係のデータを定義\n",
    "    index_tag = np.where((pd.isna(tag[index])==False) & (pd.isna(sid[index])==False) & (pd.isna(rel[index])==False))[0].astype(\"int\")\n",
    "    m1 = len(index_tag)\n",
    "    target_sentence = sentence_id[index][0]\n",
    "    target_sid = sid[index][index_tag]\n",
    "    target_tag = tag[index][index_tag]\n",
    "    target_rel = rel[index][index_tag]\n",
    "\n",
    "    # 述語項関係を取得\n",
    "    rel_flag_ = np.repeat(0, len(feature_phrase_))\n",
    "    rel_type_ = np.repeat(\"\", len(feature_phrase_)).astype(\"object\")\n",
    "    for j1 in range(m1):\n",
    "        split_sid = str.split(target_sid[j1], \"; \")\n",
    "        split_tag = str.split(target_tag[j1], \"; \")\n",
    "        split_rel = str.split(target_rel[j1], \"; \")\n",
    "        m2 = len(split_sid)\n",
    "\n",
    "        for j2 in range(m2):\n",
    "            if (split_sid[j2]!=target_sentence) | (split_sid[j2]==\"\"):\n",
    "                continue\n",
    "            index_send = np.where(tag_id_==int(split_tag[j2]))[0].astype(\"int\")\n",
    "            if len(index_send)==0:\n",
    "                continue\n",
    "            send_phrase = phrase_id_[index_send][0]\n",
    "            receive_phrase = phrase_id_[index_tag[j1]]\n",
    "            \n",
    "            if send_phrase!=receive_phrase:\n",
    "                index_dependency = np.where((feature_phrase_[:, 0]==send_phrase) & (feature_phrase_[:, 1]==receive_phrase))[0].astype(\"int\")\n",
    "                rel_flag_[index_dependency] = 1\n",
    "                rel_type_[index_dependency] += split_rel[j2] + \"; \"\n",
    "\n",
    "    for j in range(rel_type_.shape[0]):\n",
    "        if rel_type_[j]!=\"\":\n",
    "            rel_type_[j] = pd.Series(np.unique(re.split(\" \", rel_type_[j]))).str.cat()\n",
    "            rel_type_[j] = re.sub(\";$\", \"\", rel_type_[j])\n",
    "            \n",
    "    # データを格納\n",
    "    rel_flag_list[i] = rel_flag_\n",
    "    rel_type_list[i] = rel_type_\n",
    "    \n",
    "# リストを配列に変換\n",
    "feature_id = np.hstack((feature_id_list))\n",
    "feature_phrase = np.vstack((feature_phrase_list))\n",
    "dependency = np.hstack((dependency_list))\n",
    "rel_flag = np.hstack((rel_flag_list))\n",
    "rel_type = np.hstack((rel_type_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しいphrase idを定義\n",
    "# データの定義\n",
    "phrase = np.unique(np.array(new_kwdlc_corpus[\"d_id\"].astype(\"U\") + \"-\" + new_kwdlc_corpus[\"phrase_id\"].astype(\"U\"))).shape[0]\n",
    "phrase_box1 = new_kwdlc_corpus[[\"d_id\", \"phrase_id\", \"phrase_dependency\"]]\n",
    "phrase_box21 = pd.DataFrame(feature_phrase[:, 0], columns=[\"phrase_id\"])\n",
    "phrase_box22 = pd.DataFrame(feature_phrase[:, 1], columns=[\"phrase_id\"])\n",
    "\n",
    "# 新しいidの格納用配列\n",
    "phrase_no_list11 = [i for i in range(D)]\n",
    "phrase_no_list12 = [i for i in range(D)]\n",
    "phrase_no_list21 = [i for i in range(D)]\n",
    "phrase_no_list22 = [i for i in range(D)]\n",
    "max_id = 0\n",
    "\n",
    "# 文書ごとにphrase idを定義\n",
    "for i in range(D):\n",
    "    index1 = d_list[i]\n",
    "    index2 = np.where(feature_id==i)[0].astype(\"int\")\n",
    "    phrase_temp1 = phrase_box1.iloc[index1]\n",
    "    phrase_temp21 = phrase_box21.iloc[index2]\n",
    "    phrase_temp22 = phrase_box22.iloc[index2]\n",
    "    phrase_id = np.unique(np.array(phrase_temp1[\"phrase_id\"], dtype=\"int\"))\n",
    "    m = len(phrase_id)\n",
    "    no = np.arange(m) + max_id\n",
    "\n",
    "    target_phrase = pd.DataFrame({\"phrase_id\": phrase_id, \"phrase_no\": no})\n",
    "    phrase_no_list11[i] = np.array(pd.merge(phrase_temp1, target_phrase, on=\"phrase_id\", how=\"left\")[\"phrase_no\"])\n",
    "    phrase_no_list12[i] = np.array(pd.merge(phrase_temp1, target_phrase, \n",
    "                                            left_on=\"phrase_dependency\", right_on=\"phrase_id\", how=\"left\")[\"phrase_no\"])\n",
    "    phrase_no_list21[i] = np.array(pd.merge(phrase_temp21, target_phrase, on=\"phrase_id\", how=\"left\")[\"phrase_no\"])\n",
    "    phrase_no_list22[i] = np.array(pd.merge(phrase_temp22, target_phrase, on=\"phrase_id\", how=\"left\")[\"phrase_no\"])\n",
    "    max_id = np.max(no) + 1\n",
    "    \n",
    "# リストを配列に変換\n",
    "phrase_no = np.hstack((phrase_no_list11))\n",
    "dependency_no = np.hstack((phrase_no_list12))\n",
    "dependency_no[np.isnan(dependency_no)] = -1\n",
    "dependency_no = np.array(dependency_no, dtype=\"int\")\n",
    "feature_no = np.hstack((np.hstack((phrase_no_list21))[:, np.newaxis], np.hstack((phrase_no_list22))[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームの作成と出力\n",
    "# カラムを定義\n",
    "kwdlc_columns = ['serial_no', 'doc_id', 'd_id', 'sentence_id', 'phrase_id', 'phrase_dependency', 'phrase_no', 'dependency_no',\n",
    "                 'word', 'genkei', 'word_class', 'class_detail1', 'class_detail2', 'class_detail3', 'inflectional1', 'inflectional2',\n",
    "                 'dependency_type1', 'tag_id', 'tag_dependency', 'dependency_type2', 'rel', 'target', 'sid', 'tag']\n",
    "\n",
    "# データフレームを作成\n",
    "kwdlc_dependency_feature = pd.DataFrame({\"serial_no\": np.arange(feature_id.shape[0]), \"d_id\": feature_id, \n",
    "                                         \"phrase_id1\": feature_phrase[:, 0], \"phrase_id2\": feature_phrase[:, 1],\n",
    "                                         \"phrase_no1\": feature_no[:, 0], \"phrase_no2\": feature_no[:, 1], \"dependency\": dependency,\n",
    "                                         \"rel\": rel_flag, \"rel_type\": rel_type})\n",
    "new_kwdlc_corpus[\"serial_no\"] = np.arange(new_kwdlc_corpus.shape[0])\n",
    "new_kwdlc_corpus[\"phrase_no\"] = phrase_no\n",
    "new_kwdlc_corpus[\"dependency_no\"] = dependency_no\n",
    "new_kwdlc_corpus = new_kwdlc_corpus[kwdlc_columns]\n",
    "\n",
    "# データフレームを出力\n",
    "new_kwdlc_corpus.to_excel(path + \"new_kwdlc_corpus.xlsx\")\n",
    "new_kwdlc_corpus.to_csv(path + \"new_kwdlc_corpus.csv\", index=None)\n",
    "kwdlc_dependency_feature.to_excel(path + \"new_kwdlc_dependency_feature.xlsx\")\n",
    "kwdlc_dependency_feature.to_csv(path + \"new_kwdlc_dependency_feature.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 係り受け関係を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 係り受け関係のフレーズ間の組み合わせを取得\n",
    "# rel_typeを定義\n",
    "dependency = pd.concat((kyoto_dependency_feature, kwdlc_dependency_feature), axis=0)\n",
    "dependency.index = np.arange(dependency.shape[0])\n",
    "temp = np.unique(dependency[\"rel_type\"])[1:]\n",
    "rel_type = np.unique(np.hstack(([str.split(temp[j], \";\") for j in range(len(temp))])))\n",
    "\n",
    "# データの定義\n",
    "columns = [\"phrase_no1\", \"phrase_no2\", \"rel_type\"]\n",
    "kyoto_phrase_no = np.array(new_kyoto_corpus[\"phrase_no\"])\n",
    "kwdlc_phrase_no = np.array(new_kwdlc_corpus[\"phrase_no\"])\n",
    "kyoto_word = new_kyoto_corpus[\"word\"]\n",
    "kwdlc_word = new_kwdlc_corpus[\"word\"]\n",
    "\n",
    "# データの格納用リスト\n",
    "kyoto_box_list = []\n",
    "kwdlc_box_list = []\n",
    "\n",
    "# rel typeごとに組み合わせを取得\n",
    "for i in range(len(rel_type)):\n",
    "    \n",
    "    # 該当するrel_typeのフレーズを取得\n",
    "    search_word = \"^%s$|;%s;|^%s;|;%s$\" % (rel_type[i], rel_type[i], rel_type[i], rel_type[i])\n",
    "    kyoto_target = kyoto_dependency_feature[columns].iloc[np.where(kyoto_dependency_feature[\"rel_type\"].str.contains(search_word)==True)[0]]\n",
    "    kwdlc_target = kwdlc_dependency_feature[columns].iloc[np.where(kwdlc_dependency_feature[\"rel_type\"].str.contains(search_word)==True)[0]]\n",
    "    n1 = kyoto_target.shape[0]\n",
    "    n2 = kwdlc_target.shape[0]\n",
    "\n",
    "    # kyoto corpusの係り受けのフレーズ間組み合わせを取得\n",
    "    if n1 > 0:\n",
    "        kyoto_box0 = np.full((n1, 4), \"\", dtype=\"object\")\n",
    "        for j in range(n1):\n",
    "            index1 = np.where(kyoto_phrase_no==kyoto_target.iloc[j, 0])[0].astype(\"int\")\n",
    "            index2 = np.where(kyoto_phrase_no==kyoto_target.iloc[j, 1])[0].astype(\"int\")\n",
    "            kyoto_box0[j, 0] = kyoto_word.iloc[index1].str.cat(sep=\" \")\n",
    "            kyoto_box0[j, 1] = kyoto_word.iloc[index2].str.cat(sep=\" \")\n",
    "\n",
    "        kyoto_box0[:, 2] = np.repeat(rel_type[i], n1).astype(\"object\")\n",
    "        kyoto_box0[:, 3] = np.array(kyoto_target[\"rel_type\"])\n",
    "        kyoto_box_list.append(kyoto_box0)\n",
    "\n",
    "    # kwdlc corpusの係り受けのフレーズ間組み合わせを取得\n",
    "    if n2 > 0:\n",
    "        kwdlc_box = np.full((n2, 4), \"\", dtype=\"object\")\n",
    "        for j in range(n2):\n",
    "            index1 = np.where(kwdlc_phrase_no==kwdlc_target.iloc[j, 0])[0].astype(\"int\")\n",
    "            index2 = np.where(kwdlc_phrase_no==kwdlc_target.iloc[j, 1])[0].astype(\"int\")\n",
    "            kwdlc_box[j, 0] = kwdlc_word.iloc[index1].str.cat(sep=\" \")\n",
    "            kwdlc_box[j, 1] = kwdlc_word.iloc[index2].str.cat(sep=\" \")\n",
    "\n",
    "        kwdlc_box[:, 2] = np.repeat(rel_type[i], n2).astype(\"object\")\n",
    "        kwdlc_box[:, 3] = np.array(kwdlc_target[\"rel_type\"])\n",
    "        kwdlc_box_list.append(kwdlc_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを出力\n",
    "kyoto_rel_dependency = pd.DataFrame(np.vstack((kyoto_box_list)), columns=[\"phrase1\", \"phrase2\", \"rel\", \"rel_type\"])\n",
    "kwdlc_rel_dependency = pd.DataFrame(np.vstack((kwdlc_box_list)), columns=[\"phrase1\", \"phrase2\", \"rel\", \"rel_type\"])\n",
    "\n",
    "F1 = kyoto_rel_dependency.shape[0]\n",
    "F2 = kwdlc_rel_dependency.shape[0]\n",
    "rel_dependency = pd.concat((kyoto_rel_dependency, kwdlc_rel_dependency), axis=0)\n",
    "rel_dependency[\"corpus\"] = np.append(np.repeat(\"kyoto\", F1), np.repeat(\"kwdlc\", F2))\n",
    "rel_dependency = rel_dependency[[\"corpus\", \"phrase1\", \"phrase2\", \"rel\", \"rel_type\"]]\n",
    "rel_dependency.index = np.arange(rel_dependency.shape[0])\n",
    "\n",
    "kyoto_rel_dependency.to_excel(path + \"kyoto_rel_type_dependency.xlsx\")\n",
    "kyoto_rel_dependency.to_csv(path + \"kyoto_rel_type_dependency.csv\", index=None)\n",
    "kwdlc_rel_dependency.to_excel(path + \"kwdlc_rel_type_dependency.xlsx\")\n",
    "kwdlc_rel_dependency.to_csv(path + \"kwdlc_rel_type_dependency.csv\", index=None)\n",
    "rel_dependency.to_excel(path + \"rel_type_dependency.xlsx\")\n",
    "rel_dependency.to_csv(path + \"rel_type_dependency.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
