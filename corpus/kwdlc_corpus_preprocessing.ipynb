{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import itertools\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from numpy.random import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "# ファイルの取得\n",
    "path = \"D:/Statistics/data/scenario_extract/\"\n",
    "corpus_path = path + \"tdb/corpus/KWDLC-1.0/dat/\"\n",
    "folder = np.array(os.listdir(corpus_path + \"rel/\"))\n",
    "folder = corpus_path + \"rel/\" + folder.astype(\"object\") + \"/\"\n",
    "filelist = []\n",
    "for i in range(len(folder)):\n",
    "    filelist.append(glob.glob(folder[i] + \"*.KNP\"))\n",
    "filelist = np.hstack((filelist))\n",
    "m = len(filelist)\n",
    "\n",
    "# 取得したファイルの読み込み\n",
    "kwdlc_text = []\n",
    "for i in range(m):\n",
    "    with open(filelist[i], encoding=\"utf-8\") as f:\n",
    "        kwdlc_text.append(f.read())\n",
    "d = len(kwdlc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# コーパスの文字列から形態素解析結果と格解析結果をデータフレームに保存\n",
    "# 品詞情報を定義\n",
    "hinshi = \" 特殊 | 動詞 | 形容詞 | 判定詞 | 助動詞 | 名詞 | 指示詞 | 副詞 | 助詞 | 接続詞 | 連体詞 | 感動詞 | 接頭辞 | 接尾辞 | 未定義語 \"\n",
    "\n",
    "# 文書ごとに解析結果を保存する\n",
    "info_list = []\n",
    "for i in range(d):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    \n",
    "    # テキスト情報を分割\n",
    "    line_text = pd.Series(kwdlc_text[i].split(\"\\n\"))\n",
    "    index_word = np.where(line_text.str.contains(hinshi))[0].astype(\"int\")\n",
    "    index_id = np.where(line_text.str.contains(\"# S-ID\"))[0].astype(\"int\")\n",
    "    index_phrase = np.where(line_text.str.contains(\"^\\* [0-9]\"))[0].astype(\"int\")\n",
    "    index_tag = np.where(line_text.str.contains(\"^\\+ [0-9]\"))[0].astype(\"int\")\n",
    "    index_rel = np.where(line_text.str.contains(\"<rel type\"))[0].astype(\"int\")\n",
    "\n",
    "\n",
    "    # 形態素情報を抽出\n",
    "    word_split = line_text.iloc[index_word].str.split(\" \")\n",
    "    n = word_split.shape[0]\n",
    "    word_info = pd.DataFrame([word_split.iloc[j] for j in range(n)])\n",
    "    word_info.columns = [\"word\", \"reading\", \"genkei\", \"word_class\", \"class_detail1\", \"class_detail2\", \"class_detail3\"]\n",
    "\n",
    "\n",
    "    # センテンスidを定義\n",
    "    # 文字列を分割\n",
    "    id_split = line_text.iloc[index_id].str.split(\" |:\")\n",
    "    m = id_split.shape[0]\n",
    "    get_id = np.array([id_split.iloc[j][2] for j in range(m)])\n",
    "\n",
    "    # idを配列として保存\n",
    "    allocation = np.repeat(0, m)\n",
    "    for j in range(m):\n",
    "        if j < m - 1:\n",
    "            allocation[j] = np.sum((index_word > index_id[j]) & (index_word < index_id[j+1]))\n",
    "        else:\n",
    "            allocation[j] = np.sum(index_word > index_id[j])\n",
    "    sentence_id = np.repeat(get_id, allocation)\n",
    "    doc_id = np.repeat(i, n)\n",
    "    id_info = pd.DataFrame({\"doc_id\": doc_id, \"sentence_id\": sentence_id})\n",
    "\n",
    "\n",
    "    # 文節とその係り受け関係を取得\n",
    "    # 文字列を分割\n",
    "    phrase_split = line_text.iloc[index_phrase].str.replace(\"-|\\* \", \"\").str.split(\" \")\n",
    "    m = phrase_split.shape[0]\n",
    "\n",
    "    # 文節idおよび係り受けidを定義\n",
    "    phrase = np.repeat(0, m)\n",
    "    dependency = np.repeat(0, m)\n",
    "    types = np.repeat(\"\", m).astype(\"object\")\n",
    "    for j in range(m):\n",
    "        get_phrase = phrase_split.iloc[j]\n",
    "        for q in range(len(get_phrase)):\n",
    "            if q==0:\n",
    "                phrase[j] = int(get_phrase[q])\n",
    "            else:\n",
    "                dependency[j] = int(get_phrase[q][:re.search(\"[0-9]*\", get_phrase[q]).end()])\n",
    "                types[j] = get_phrase[q][re.search(\"[^0-9]\", get_phrase[q]).start():]\n",
    "\n",
    "    # idを配列として保存\n",
    "    allocation = np.repeat(0, m)\n",
    "    for j in range(m):\n",
    "        if j < m - 1:\n",
    "            allocation[j] = np.sum((index_word > index_phrase[j]) & (index_word < index_phrase[j+1]))\n",
    "        else:\n",
    "            allocation[j] = np.sum(index_word > index_phrase[j])\n",
    "    phrase_id = np.repeat(phrase, allocation)\n",
    "    phrase_dependency = np.repeat(dependency, allocation)\n",
    "    dependency_type = np.repeat(types, allocation)\n",
    "    phrase_info = pd.DataFrame({\"phrase_id\": phrase_id, \"phrase_dependency\": phrase_dependency, \"dependency_type1\": dependency_type})\n",
    "\n",
    "\n",
    "    # タグとその係り受け関係を取得\n",
    "    # 文字列を分割\n",
    "    tag_split = line_text.iloc[index_tag].str.replace(\"-|\\+ \", \"\").str.split(\" \")\n",
    "    m = tag_split.shape[0]\n",
    "\n",
    "    # 文節idおよび係り受けidを定義\n",
    "    tag = np.repeat(0, m)\n",
    "    dependency = np.repeat(0, m)\n",
    "    types = np.repeat(\"\", m).astype(\"object\")\n",
    "    for j in range(m):\n",
    "        get_tag = tag_split.iloc[j]\n",
    "        for q in range(2):\n",
    "            if q==0:\n",
    "                tag[j] = int(get_tag[q])\n",
    "            else:\n",
    "                dependency[j] = int(get_tag[q][:re.search(\"[0-9]*\", get_tag[q]).end()])\n",
    "                types[j] = get_tag[q][re.search(\"[^0-9]\", get_tag[q]).start():]\n",
    "\n",
    "    # idを配列として保存\n",
    "    allocation = np.repeat(0, m)\n",
    "    for j in range(m):\n",
    "        if j < m - 1:\n",
    "            allocation[j] = np.sum((index_word > index_tag[j]) & (index_word < index_tag[j+1]))\n",
    "        else:\n",
    "            allocation[j] = np.sum(index_word > index_tag[j])\n",
    "    tag_id = np.repeat(tag, allocation)\n",
    "    tag_dependency = np.repeat(dependency, allocation)\n",
    "    dependency_type = np.repeat(types, allocation)\n",
    "    tag_allcation = np.repeat(index_tag, allocation)\n",
    "    tag_info = pd.DataFrame({\"tag_id\": tag_id, \"tag_dependency\": tag_dependency, \"dependency_type2\": dependency_type})\n",
    "\n",
    "\n",
    "    # 格解析結果と対応関係を取得\n",
    "    # 解析結果を分割\n",
    "    rel_split = line_text.iloc[index_rel].str.split(\"<|/><|/>\")\n",
    "    m = rel_split.shape[0]\n",
    "\n",
    "    # 解析結果の格納用配列\n",
    "    rel = np.repeat(\"\", m).astype(\"object\")\n",
    "    target = np.repeat(\"\", m).astype(\"object\")\n",
    "    sid = np.repeat(\"\", m).astype(\"object\")\n",
    "    tag = np.repeat(\"\", m).astype(\"object\")\n",
    "    no = np.repeat(0, m)\n",
    "\n",
    "    # 格解析結果を配列に保存\n",
    "    for j in range(m):\n",
    "        # 格解析結果に関係のある文字列を取得\n",
    "        get_rel = rel_split.iloc[j]\n",
    "        get_rel = np.array(get_rel)[np.array(get_rel)!=\"\"]\n",
    "        no[j] = int(re.search(\" [0-9]+ \", get_rel[0]).group().replace(\" \", \"\"))\n",
    "        r = get_rel.shape[0]\n",
    "\n",
    "        # 解析結果単位ごとの格納用配列\n",
    "        rel_string = np.repeat(\"\", r-1).astype(\"object\")\n",
    "        target_string = np.repeat(\"\", r-1).astype(\"object\")\n",
    "        sid_string = np.repeat(\"\", r-1).astype(\"object\")\n",
    "        tag_string = np.repeat(\"\", r-1).astype(\"object\")\n",
    "\n",
    "        # 解析単位ごとに格解析結果を抽出\n",
    "        for q in range(1, r):\n",
    "            string = get_rel[q]\n",
    "\n",
    "            rel_match = re.search(\"rel type=\\\"[^\\\"]+\\\"\", string)\n",
    "            if rel_match is not None:\n",
    "                rel_match = rel_match.group()\n",
    "                rel_string[q-1] = re.search(\"\\\".+\\\"\", rel_match).group().replace(\"\\\"\", \"\")\n",
    "            else:\n",
    "                rel_string[q-1] = \"\"\n",
    "\n",
    "            target_match = re.search(\"target=\\\"[^\\\"]+\\\"\", string)\n",
    "            if target_match is not None:\n",
    "                target_match = target_match.group()\n",
    "                target_string[q-1] = re.search(\"\\\".+\\\"\", target_match).group().replace(\"\\\"\", \"\")\n",
    "            else:\n",
    "                target_string[q-1] = \"\"\n",
    "\n",
    "            sid_match = re.search(\"sid=\\\"[^\\\"]+\\\"\", string)\n",
    "            if sid_match is not None:\n",
    "                sid_match = sid_match.group()\n",
    "                sid_string[q-1] = re.search(\"\\\".+\\\"\", sid_match).group().replace(\"\\\"\", \"\")\n",
    "            else:\n",
    "                sid_string[q-1] = \"\"\n",
    "\n",
    "            tag_match = re.search(\"tag=\\\"[^\\\"]+\\\"\", string)\n",
    "            if tag_match is not None:\n",
    "                tag_match = tag_match.group()\n",
    "                tag_string[q-1] = re.search(\"\\\".+\\\"\", tag_match).group().replace(\"\\\"\", \"\")\n",
    "            else:\n",
    "                tag_string[q-1] = \"\"\n",
    "\n",
    "        # 文字列を結合して保存\n",
    "        rel[j] = pd.Series(rel_string).str.cat(sep=\"; \")\n",
    "        target[j] = pd.Series(target_string).str.cat(sep=\"; \")\n",
    "        sid[j] = pd.Series(sid_string).str.cat(sep=\"; \")\n",
    "        tag[j] = pd.Series(tag_string).str.cat(sep=\"; \")\n",
    "\n",
    "    # 格関係の情報をデータフレームに格納\n",
    "    temp1 = pd.DataFrame({\"allocation\": tag_allcation})\n",
    "    temp2 = pd.DataFrame({\"allocation\": index_rel, \"rel\": rel, \"target\": target, \"sid\": sid, \"tag\": tag})\n",
    "    rel_info = pd.merge(temp1, temp2, on=\"allocation\", how=\"left\").iloc[:, 1:]\n",
    "\n",
    "    # データフレームを結合してリストに格納\n",
    "    info_list.append(pd.concat((id_info, word_info, phrase_info, tag_info, rel_info), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストをデータフレームに変換\n",
    "info = pd.concat((info_list), axis=0)\n",
    "info[\"serial_no\"] = np.arange(info.shape[0])\n",
    "info = info[[\"serial_no\", \"doc_id\", \"sentence_id\", \"word\", \"reading\", \"genkei\", \"word_class\", \n",
    "             \"class_detail1\", \"class_detail2\", \"class_detail3\", \"phrase_id\", \"phrase_dependency\", \"dependency_type1\",\n",
    "             \"tag_id\", \"tag_dependency\", \"dependency_type2\", \"rel\", \"target\", \"sid\", \"tag\"]]\n",
    "info.index = np.arange(info.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# イコールはエクセルで問題が生じるので変換しておく\n",
    "info[\"rel\"] = info[\"rel\"].str.replace(\"=\", \"≡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを出力\n",
    "info.to_csv(path + \"tdb/corpus/kwdlc_info.csv\", index=None)\n",
    "info.to_excel(path + \"tdb/corpus/kwdlc_info.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
