{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import itertools\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import jaconv\n",
    "import mojimoji\n",
    "from numpy.random import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\statistics\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "path = \"C:/statistics/data/scenario_extract/tdb/corpus/\"\n",
    "kyoto_text = pd.read_csv(path + \"kyoto_info_original.csv\")\n",
    "naist_text = pd.read_csv(path + \"naist_info.csv\", low_memory=False)\n",
    "naist_text[\"readings\"].iloc[np.where(pd.isna(naist_text[\"readings\"])==True)[0]] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 京都コーパスから単語を抽出\n",
    "kyoto_symbol = np.array(kyoto_text[\"word\"])\n",
    "kyoto_readings = np.array(kyoto_text[\"reading\"])\n",
    "N1 = kyoto_readings.shape[0]\n",
    "\n",
    "# NAISTコーパスから単語を抽出\n",
    "naist_genkei = np.array(naist_text[\"genkei\"])\n",
    "naist_readings = np.array(naist_text[\"readings\"])\n",
    "N2 = kyoto_readings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 京都コーパスの文書idを定義\n",
    "sentence_id1 = np.array(kyoto_text[\"d_id\"])\n",
    "n1 = np.unique(sentence_id1).shape[0]\n",
    "sentence_list1 = [i for i in range(n1)]\n",
    "for i in range(n1):\n",
    "    sentence_list1[i] = np.where(sentence_id1==i)[0].astype(\"int\")\n",
    "    \n",
    "# NAISTコーパスの文書idを定義\n",
    "sentence_id2 = np.array(naist_text[\"sentence_id\"])\n",
    "n2 = np.unique(sentence_id2).shape[0]\n",
    "sentence_list2 = [i for i in range(n2)]\n",
    "for i in range(n2):\n",
    "    sentence_list2[i] = np.where(sentence_id2==i)[0].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAISTコーパスを京都コーパスに結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章の一致を取得\n",
    "# 文章データを作成\n",
    "kyoto_sentence = np.array([pd.Series(kyoto_readings[sentence_list1[i]]).str.cat() for i in range(n1)])\n",
    "naist_sentence = np.array([pd.Series(naist_readings[sentence_list2[i]]).str.cat() for i in range(n2)])\n",
    "kyoto_string = pd.DataFrame({\"kyoto_id\": np.arange(n1), \"sentence\": kyoto_sentence})\n",
    "naist_string = pd.DataFrame({\"naist_id\": np.arange(n2), \"sentence\": naist_sentence})\n",
    "kyoto_string[\"sentence1\"] = kyoto_string[\"sentence\"].str[:15] + kyoto_string[\"sentence\"].str[-15:]\n",
    "kyoto_string[\"sentence2\"] = kyoto_string[\"sentence\"].str[:20]\n",
    "kyoto_string[\"sentence3\"] = kyoto_string[\"sentence\"].str[-20:]\n",
    "naist_string[\"sentence1\"] = naist_string[\"sentence\"].str[:15] + naist_string[\"sentence\"].str[-15:]\n",
    "naist_string[\"sentence2\"] = naist_string[\"sentence\"].str[:20]\n",
    "naist_string[\"sentence3\"] = naist_string[\"sentence\"].str[-20:]\n",
    "\n",
    "# 文字列の一致を取得\n",
    "match_sentence = pd.merge(kyoto_string[[\"kyoto_id\", \"sentence\"]], naist_string[[\"naist_id\", \"sentence\"]], \n",
    "                          on=\"sentence\", how=\"inner\")[[\"kyoto_id\", \"naist_id\", \"sentence\"]]\n",
    "match_sentence1 = pd.merge(kyoto_string[[\"kyoto_id\", \"sentence\", \"sentence1\"]], naist_string[[\"naist_id\", \"sentence1\"]],\n",
    "                           on=\"sentence1\", how=\"inner\")[[\"kyoto_id\", \"naist_id\", \"sentence\", \"sentence1\"]]\n",
    "match_sentence2 = pd.merge(kyoto_string[[\"kyoto_id\", \"sentence\", \"sentence2\"]], naist_string[[\"naist_id\", \"sentence2\"]],\n",
    "                           on=\"sentence2\", how=\"inner\")[[\"kyoto_id\", \"naist_id\", \"sentence\", \"sentence2\"]]\n",
    "match_sentence3 = pd.merge(kyoto_string[[\"kyoto_id\", \"sentence\", \"sentence3\"]], naist_string[[\"naist_id\", \"sentence3\"]],\n",
    "                           on=\"sentence3\", how=\"inner\")[[\"kyoto_id\", \"naist_id\", \"sentence\", \"sentence3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパス間の対応関係を取得\n",
    "match_joint = pd.concat((match_sentence[[\"kyoto_id\", \"naist_id\"]], \n",
    "                         match_sentence1[[\"kyoto_id\", \"naist_id\"]],\n",
    "                         match_sentence2[[\"kyoto_id\", \"naist_id\"]],\n",
    "                         match_sentence3[[\"kyoto_id\", \"naist_id\"]]), axis=0)\n",
    "unique_match = match_joint.iloc[np.where(match_joint[\"kyoto_id\"].duplicated()==False)[0]]\n",
    "m = unique_match.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# 京都コーパスにNAISTコーパスの原形を加える\n",
    "# 形態素のカラムを抽出\n",
    "naist_morpheme = naist_text[[\"genkei\", \"readings\"]]\n",
    "kyoto_morpheme = kyoto_text[[\"serial_no\", \"word\", \"reading\"]]\n",
    "kyoto_morpheme.columns = [\"serial_no\", \"word\", \"readings\"]\n",
    "naist_unique = naist_morpheme.iloc[np.where(naist_morpheme.duplicated()==False)[0]]\n",
    "freq = naist_unique[\"readings\"].value_counts()\n",
    "naist_morpheme = pd.merge(naist_morpheme, pd.DataFrame({\"readings\": freq.index[freq > 1], \"flag\": 1}), on=\"readings\", how=\"left\")\n",
    "naist_morpheme[\"flag\"].iloc[np.where(pd.isna(naist_morpheme[\"flag\"]))[0]] = 0\n",
    "naist_morpheme[\"flag\"] = np.array(naist_morpheme[\"flag\"], dtype=\"int\")\n",
    "\n",
    "# データの設定\n",
    "threshold = 5\n",
    "kyoto_genkei_list = []\n",
    "\n",
    "#センテンスごとに原形をjoinする\n",
    "for i in range(m):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    \n",
    "    # データを定義\n",
    "    id1 = unique_match[\"kyoto_id\"].iloc[i]\n",
    "    id2 = unique_match[\"naist_id\"].iloc[i]\n",
    "    index1 = sentence_list1[id1]\n",
    "    index2 = sentence_list2[id2]\n",
    "    sentence1 = kyoto_sentence[id1]\n",
    "    sentence2 = naist_sentence[id2]\n",
    "\n",
    "    # 文字数がしきい値以上異なると次のセンテンスへ\n",
    "    if np.abs(len(sentence1) - len(sentence2)) >= threshold:\n",
    "        continue\n",
    "        \n",
    "    # NAISTコーパスの原形を京都コーパスに結合する\n",
    "    kyoto_word = kyoto_morpheme.iloc[index1]\n",
    "    naist_word = naist_morpheme.iloc[index2]\n",
    "    naist_unique = naist_word.iloc[np.where(naist_word.duplicated()==False)[0]]\n",
    "    freq = naist_unique[\"readings\"].value_counts()\n",
    "    index_target = np.where((np.in1d(naist_unique[\"readings\"], freq.index[freq==1])) | (naist_unique[\"flag\"]==0))[0].astype(\"int\")\n",
    "    naist_unique = naist_unique.iloc[index_target]\n",
    "    kyoto_genkei_list.append(pd.merge(kyoto_word, naist_unique, on=\"readings\", how=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 京都コーパスに原形を結合\n",
    "kyoto_genkei = pd.concat((kyoto_genkei_list), axis=0)\n",
    "kyoto_genkei.index = np.arange(kyoto_genkei.shape[0])\n",
    "naist_unique = naist_morpheme.iloc[np.where(naist_morpheme[\"flag\"]==0)[0]]\n",
    "naist_unique = naist_unique[[\"genkei\", \"readings\"]].iloc[np.where(naist_unique.duplicated()==False)[0]]\n",
    "kyoto_new1 = pd.merge(kyoto_morpheme, kyoto_genkei[[\"serial_no\", \"genkei\"]], on=\"serial_no\", how=\"left\")\n",
    "kyoto_new2 = pd.merge(kyoto_morpheme, naist_unique, on=\"readings\", how=\"left\")\n",
    "index_genkei1 = np.where(pd.isna(kyoto_new2[\"genkei\"])==False)[0].astype(\"int\")\n",
    "index_genkei2 = np.where(kyoto_text[\"genkei\"]!=\"*\")[0].astype(\"int\")\n",
    "\n",
    "genkei = np.array(kyoto_new1[\"genkei\"])\n",
    "genkei[index_genkei1] = np.array(kyoto_new2[\"genkei\"].iloc[index_genkei1])\n",
    "genkei[index_genkei2] = np.array(kyoto_text[\"genkei\"].iloc[index_genkei2])\n",
    "kyoto_text[\"new_genkei\"] = genkei\n",
    "kyoto_text[\"flag\"] = np.array(pd.isna(genkei)==True, dtype=\"int\")\n",
    "del kyoto_genkei_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素を置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素情報を抽出\n",
    "kyoto_morpheme = kyoto_text[[\"serial_no\", \"flag\", \"new_genkei\", \"reading\", \"word_class\", \n",
    "                             \"class_detail1\", \"class_detail2\", \"class_detail3\"]]\n",
    "kyoto_morpheme = kyoto_morpheme.rename(columns={\"new_genkei\": \"genkei\"})\n",
    "genkei = np.array(kyoto_morpheme[\"genkei\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語および品詞単独での形態素を置き換え\n",
    "# 助詞を置き換え\n",
    "index_positional = np.where((kyoto_morpheme[\"word_class\"]==\"助詞\") & (kyoto_morpheme[\"flag\"]==1))[0].astype(\"int\")\n",
    "genkei[index_positional] = np.array(kyoto_morpheme[\"reading\"].iloc[index_positional])\n",
    "\n",
    "# 人名を置き換え\n",
    "index_name = np.where(kyoto_morpheme[\"class_detail1\"]==\"人名\")[0].astype(\"int\")\n",
    "genkei[index_name] = kyoto_morpheme[\"reading\"].iloc[index_name]\n",
    "\n",
    "# 数値を置き換え\n",
    "index_number = np.where(kyoto_morpheme[\"class_detail1\"]==\"数詞\")[0].astype(\"int\")\n",
    "genkei[index_number] = \"0\"\n",
    "\n",
    "# 地名を置き換え\n",
    "index_locale1 = np.where((kyoto_morpheme[\"class_detail1\"]==\"地名\") & (kyoto_morpheme[\"reading\"]==\"にほん\"))[0].astype(\"int\")\n",
    "index_locale2 = np.where((kyoto_morpheme[\"class_detail1\"]==\"地名\") & (kyoto_morpheme[\"reading\"]==\"にっぽん\"))[0].astype(\"int\")\n",
    "index_locale = np.unique(np.append(index_locale1, index_locale2))\n",
    "genkei[index_locale] = \"日本\"\n",
    "\n",
    "# アルファベットを置き換え\n",
    "index_alphabet = np.where(kyoto_morpheme[\"reading\"].str.contains(\"^[a-zA-ZＡ-ｚ0-9０-９]+$\"))[0].astype(\"int\")\n",
    "alphabet = np.array(kyoto_morpheme[\"reading\"].iloc[index_alphabet])\n",
    "alphabet = np.array([mojimoji.zen_to_han(alphabet[i]) for i in range(alphabet.shape[0])])\n",
    "genkei[index_alphabet] = alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 品詞と品詞クラスの組み合わせでの形態素の置き換え\n",
    "# 助動詞を置き換え\n",
    "index = np.where((kyoto_morpheme[\"word_class\"]==\"助動詞\") & (pd.isna(kyoto_morpheme[\"genkei\"])==False))[0]\n",
    "aux_morpheme = kyoto_morpheme[[\"genkei\", \"reading\", \"class_detail1\", \"class_detail2\"]].iloc[index]\n",
    "aux_morpheme = aux_morpheme.iloc[np.where(aux_morpheme[[\"reading\", \"class_detail1\", \"class_detail2\"]].duplicated()==False)[0]]\n",
    "aux_morpheme = aux_morpheme.rename(columns={\"genkei\": \"new_genkei\"})\n",
    "aux_morpheme.index = np.arange(aux_morpheme.shape[0])\n",
    "joint_morpheme = pd.merge(kyoto_morpheme, aux_morpheme, on=[\"reading\", \"class_detail1\", \"class_detail2\"], how=\"left\")\n",
    "joint_morpheme = joint_morpheme[[\"serial_no\", \"flag\", \"new_genkei\", \"reading\", \"word_class\"]]\n",
    "index_aux = np.where((joint_morpheme[\"flag\"]==1) & (pd.isna(joint_morpheme[\"new_genkei\"])==False) & \n",
    "                     (joint_morpheme[\"word_class\"]==\"助動詞\"))[0].astype(\"int\")\n",
    "genkei[index_aux] = np.array(joint_morpheme[\"new_genkei\"].iloc[index_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形容詞を置き換え\n",
    "index = np.where((kyoto_morpheme[\"word_class\"]==\"形容詞\") & (pd.isna(kyoto_morpheme[\"genkei\"])==False))[0]\n",
    "adverb_morpheme = kyoto_morpheme[[\"genkei\", \"reading\", \"class_detail1\", \"class_detail2\"]].iloc[index]\n",
    "adverb_morpheme = adverb_morpheme.iloc[np.where(adverb_morpheme[[\"reading\", \"class_detail1\", \"class_detail2\"]].duplicated()==False)[0]]\n",
    "adverb_morpheme = adverb_morpheme.rename(columns={\"genkei\": \"new_genkei\"})\n",
    "adverb_morpheme.index = np.arange(adverb_morpheme.shape[0])\n",
    "joint_morpheme = pd.merge(kyoto_morpheme, adverb_morpheme, on=[\"reading\", \"class_detail1\", \"class_detail2\"], how=\"left\")\n",
    "joint_morpheme = joint_morpheme[[\"serial_no\", \"flag\", \"new_genkei\", \"reading\", \"word_class\"]]\n",
    "index_adverb = np.where((joint_morpheme[\"flag\"]==1) & (pd.isna(joint_morpheme[\"new_genkei\"])==False) &\n",
    "                        (joint_morpheme[\"word_class\"]==\"形容詞\"))[0].astype(\"int\")\n",
    "genkei[index_adverb] = np.array(joint_morpheme[\"new_genkei\"].iloc[index_adverb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# その他の品詞を置き換え\n",
    "index1 = np.where((kyoto_morpheme[\"word_class\"]==\"指示詞\") & (pd.isna(kyoto_morpheme[\"genkei\"])==False))[0]\n",
    "index2 = np.where((kyoto_morpheme[\"word_class\"]==\"接続詞\") & (pd.isna(kyoto_morpheme[\"genkei\"])==False))[0]\n",
    "index3 = np.where((kyoto_morpheme[\"word_class\"]==\"連体詞\") & (pd.isna(kyoto_morpheme[\"genkei\"])==False))[0]\n",
    "index = np.unique(np.hstack((index1, index2, index3)))\n",
    "other_morpheme = kyoto_morpheme[[\"genkei\", \"reading\", \"word_class\", \"class_detail1\", \"class_detail2\"]].iloc[index]\n",
    "other_morpheme = other_morpheme.iloc[np.where(other_morpheme[[\"reading\", \"word_class\", \n",
    "                                                              \"class_detail1\", \"class_detail2\"]].duplicated()==False)[0]]\n",
    "other_morpheme = other_morpheme.rename(columns={\"genkei\": \"new_genkei\"})\n",
    "other_morpheme.index = np.arange(other_morpheme.shape[0])\n",
    "joint_morpheme = pd.merge(kyoto_morpheme, other_morpheme, on=[\"reading\", \"word_class\", \"class_detail1\", \"class_detail2\"], how=\"left\")\n",
    "joint_morpheme = joint_morpheme[[\"serial_no\", \"flag\", \"new_genkei\", \"reading\", \"word_class\"]]\n",
    "index_other = np.where((joint_morpheme[\"flag\"]==1) & (pd.isna(joint_morpheme[\"new_genkei\"])==False) &\n",
    "                       ((joint_morpheme[\"word_class\"]==\"指示詞\") | (joint_morpheme[\"word_class\"]==\"接続詞\") |\n",
    "                        (joint_morpheme[\"word_class\"]==\"連体詞\")))[0].astype(\"int\")\n",
    "genkei[index_other] = np.array(joint_morpheme[\"new_genkei\"].iloc[index_other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原形が欠損している部分とその周辺を表示しておくためのflag\n",
    "window = 2\n",
    "flag_nan = np.repeat(0, N1)\n",
    "index_nan = np.where(pd.isna(genkei))[0].astype(\"int\")\n",
    "display_list = []\n",
    "for i in range(index_nan.shape[0]):\n",
    "    display_list.append(np.arange(index_nan[i] - window, index_nan[i] + window))\n",
    "index_nan = np.unique(np.hstack((display_list)))\n",
    "index_nan = index_nan[(index_nan >= 0) & (index_nan <= N1-1)]\n",
    "flag_nan[index_nan] = 1\n",
    "kyoto_text[\"flag\"] = np.array(pd.isna(genkei)==True, dtype=\"int\")\n",
    "kyoto_text[\"display_flag\"] = flag_nan\n",
    "kyoto_text[\"new_genkei\"] = genkei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを定義\n",
    "kyoto_info = kyoto_text[['serial_no', 'doc_id', 'd_id', 'sentence_id', 'flag', 'display_flag', 'word', 'new_genkei', 'reading', \n",
    "                         'word_class', 'class_detail1', 'class_detail2', 'class_detail3', 'phrase_id',\n",
    "                         'phrase_dependency', 'dependency_type1', 'tag_id', 'tag_dependency', 'dependency_type2', \n",
    "                         'rel', 'target', 'sid', 'tag']]\n",
    "kyoto_info = kyoto_info.rename(columns={'new_genkei': 'genkei'})\n",
    "kyoto_info.index = np.arange(kyoto_info.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを出力\n",
    "kyoto_info.to_csv(path + \"/kyoto_info.csv\", index=None)\n",
    "kyoto_info.to_excel(path + \"/kyoto_info.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
