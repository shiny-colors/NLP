{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import itertools\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import jaconv\n",
    "import mojimoji\n",
    "from numpy.random import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "# ファイルの取得\n",
    "path = \"C:/statistics/data/scenario_extract/\"\n",
    "corpus_path = path + \"tdb/corpus/NTC_1.5/dat/\"\n",
    "filelist = glob.glob(corpus_path + \"num/ipa/*.dat\")\n",
    "m = len(filelist)\n",
    "\n",
    "# 取得したファイルの読み込み\n",
    "naist_text = []\n",
    "for i in range(m):\n",
    "    with open(filelist[i], encoding=\"euc-jp\") as f:\n",
    "        naist_text.append(np.array(f.read().split(\"EOS\")))\n",
    "naist_text = np.hstack((naist_text))\n",
    "m = naist_text.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フレーズ情報を処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# フレーズと単語情報の関係を取得\n",
    "# データの格納用配列\n",
    "phrase_no = 0\n",
    "phrase_list = []\n",
    "head_list = []\n",
    "info_list = []\n",
    "    \n",
    "# テキストファイルごとにフレーズと単語情報をリストに格納\n",
    "for i in range(m):\n",
    "    print(i)\n",
    "    \n",
    "    # テキストを分割\n",
    "    line_text = pd.Series(naist_text[i].split(\"\\n\"))\n",
    "    index_phrase = np.where(line_text.str.contains(\"^[0-9]+/[0-9]+$\"))[0].astype(\"int\")\n",
    "    index_word = np.delete(np.arange(line_text.shape[0]), index_phrase)\n",
    "    n1 = index_phrase.shape[0]\n",
    "\n",
    "    # フレーズの対応関係を取得\n",
    "    for j in range(n1):\n",
    "        if j < n1-1:\n",
    "            index1 = index_phrase[j]\n",
    "            index2 = index_phrase[j+1]\n",
    "            allocation = np.array(line_text.iloc[index_word[(index_word > index1) & (index_word < index2)]])\n",
    "        else:\n",
    "            index1 = index_phrase[j]\n",
    "            allocation = np.array(line_text.iloc[index_word[index_word > index1]])\n",
    "        n2 = allocation.shape[0]\n",
    "        \n",
    "        # フレーズごとにリストに情報を格納\n",
    "        phrase_list.append(np.repeat(phrase_no, n2))\n",
    "        head_list.append(np.repeat(line_text[index_phrase[j]], n2))\n",
    "        info_list.append(allocation)\n",
    "        phrase_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストを配列に変換\n",
    "phrase_no = np.hstack((phrase_list))\n",
    "head = np.hstack((head_list))\n",
    "info = np.hstack((info_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素情報を処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 形態素情報を配列に格納\n",
    "# 形態素情報を分割\n",
    "line_info = pd.Series(info).str.split(\"\\t\")\n",
    "N = line_info.shape[0]\n",
    "cl = 4\n",
    "\n",
    "# データの格納用配列\n",
    "types = np.repeat(0, N)\n",
    "readings = np.repeat(\"\", N).astype(\"object\")\n",
    "genkei = np.repeat(\"\", N).astype(\"object\")\n",
    "classes = np.full((N, cl), \"\").astype(\"object\")\n",
    "\n",
    "# 単語ごとに形態素情報を分割する\n",
    "for i in range(N):\n",
    "    if len(line_info.iloc[i]) <= 1:\n",
    "        continue\n",
    "    types[i] = int(line_info.iloc[i][0])\n",
    "    readings[i] = jaconv.kata2hira(line_info.iloc[i][1])\n",
    "    genkei[i] = line_info.iloc[i][2]\n",
    "    class_split = np.array(line_info.iloc[i][3].split(\"-\"))\n",
    "    get_cl = class_split.shape[0]\n",
    "    classes[i, ] = np.append(class_split, np.repeat(\"\", cl - get_cl))\n",
    "\n",
    "# 文章idを定義\n",
    "flag = np.repeat(0, N)\n",
    "flag[np.where(genkei==\"。\")[0]+1] = 1\n",
    "sentence_id = np.cumsum(flag)\n",
    "\n",
    "# データフレームを作成\n",
    "class_info = pd.DataFrame(classes, columns=[\"class\", \"class_detail1\", \"class_detail2\", \"class_detail3\"])\n",
    "phrase_info = pd.DataFrame({\"sentence_id\": sentence_id, \"phrase_id\": phrase_no, \"head\": head})\n",
    "morpheme_info = pd.concat((pd.DataFrame({\"genkei\": genkei, \"readings\": readings}), class_info), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readigsが格納されていないセルを埋める\n",
    "index_space = np.where(morpheme_info[\"readings\"]==\"\")[0].astype(\"int\")\n",
    "index_replace = index_space[np.where(morpheme_info[\"genkei\"].iloc[index_space].str.contains(\"[ａ-ｚＡ-Ｚあ-んア-ン]\"))[0]]\n",
    "target = np.array(morpheme_info[\"genkei\"].iloc[index_replace])\n",
    "morpheme_info[\"readings\"].iloc[index_replace] = [jaconv.kata2hira(target[i]) for i in range(target.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 係り受け情報と格解析情報を処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pandas seriesに変換\n",
    "info_series = pd.Series(info)\n",
    "\n",
    "# 格idを取得\n",
    "index_frame = np.where(info_series.str.contains(\"\\tid=\\\"| id=\\\"\"))[0].astype(\"int\")\n",
    "frame_id = np.repeat(-1, N)\n",
    "n = index_frame.shape[0]\n",
    "for i in range(n):\n",
    "    match_string = re.findall(\"id=\\\".+?\\\"\", info_series.iloc[index_frame[i]])[0]\n",
    "    frame_id[index_frame[i]] = int(re.sub(\"id=|\\\"\", \"\", match_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格と種別を取得\n",
    "# インデックスを取得\n",
    "index_gaoni = np.where(info_series.str.contains(\"ga=\\\"|o=\\\"|ni=\\\"\"))[0].astype(\"int\")\n",
    "n = index_gaoni.shape[0]\n",
    "\n",
    "# オブジェクトの格納用配列\n",
    "ga_id = np.repeat(\"\", N).astype(\"object\")\n",
    "o_id = np.repeat(\"\", N).astype(\"object\")\n",
    "ni_id = np.repeat(\"\", N).astype(\"object\")\n",
    "ga_type = np.repeat(\"\", N).astype(\"object\")\n",
    "o_type = np.repeat(\"\", N).astype(\"object\")\n",
    "ni_type = np.repeat(\"\", N).astype(\"object\")\n",
    "\n",
    "# ガ、オ、ニの格種別を取得\n",
    "for i in range(n):\n",
    "    index = index_gaoni[i]\n",
    "    target = info_series.iloc[index]\n",
    "    ga_string1 = re.findall(\"ga=\\\".+?\\\"\", target)\n",
    "    o_string1 = re.findall(\"o=\\\".+?\\\"\", target)\n",
    "    ni_string1 = re.findall(\"ni=\\\".+?\\\"\", target)\n",
    "    ga_string2 = re.findall(\"ga_type=\\\".+?\\\"\", target)\n",
    "    o_string2 = re.findall(\"o_type=\\\".+?\\\"\", target)\n",
    "    ni_string2 = re.findall(\"ni_type=\\\".+?\\\"\", target)\n",
    "\n",
    "    if len(ga_string1) > 0:\n",
    "        ga_id[index] = re.sub(\"ga=|\\\"\", \"\", ga_string1[0])\n",
    "    if len(o_string1) > 0:\n",
    "        o_id[index] = re.sub(\"o=|\\\"\", \"\", o_string1[0])\n",
    "    if len(ni_string1) > 0:\n",
    "        ni_id[index] = re.sub(\"ni=|\\\"\", \"\", ni_string1[0])\n",
    "    if len(ga_string2) > 0:\n",
    "        ga_type[index] = re.sub(\"ga_type=|\\\"\", \"\", ga_string2[0])\n",
    "    if len(o_string2) > 0:\n",
    "        o_type[index] = re.sub(\"o_type=|\\\"\", \"\", o_string2[0])\n",
    "    if len(ni_string2) > 0:\n",
    "        ni_type[index] = re.sub(\"ni_type=|\\\"\", \"\", ni_string2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# その他の構造を取得\n",
    "# インデックスを取得\n",
    "index_eq = np.where(info_series.str.contains(\"\\teq=\\\"| eq=\\\"\"))[0].astype(\"int\")\n",
    "index_type = np.where(info_series.str.contains(\"\\ttype=\\\"| type=\\\"\"))[0].astype(\"int\")\n",
    "index_alt = np.where(info_series.str.contains(\"\\talt=\\\"| alt=\\\"\"))[0].astype(\"int\")\n",
    "index_noun_type = np.where(info_series.str.contains(\"\\tnoun_type=\\\"| noun_type=\\\"\"))[0].astype(\"int\")\n",
    "index_ana_id = np.where(info_series.str.contains(\"\\tana_id=\\\"| ana_id=\\\"\"))[0].astype(\"int\")\n",
    "index_ant_id = np.where(info_series.str.contains(\"\\tant_id=\\\"| ant_id=\\\"\"))[0].astype(\"int\")\n",
    "index_ana_type = np.where(info_series.str.contains(\"\\tana_type=\\\"| ana_type=\\\"\"))[0].astype(\"int\")\n",
    "index_refexp_type = np.where(info_series.str.contains(\"\\trefexp_type=\\\"| refexp_type=\\\"\"))[0].astype(\"int\")\n",
    "\n",
    "# 構造を配列に格納\n",
    "n = index_eq.shape[0]\n",
    "eq = np.repeat(\"\", N).astype(\"object\")\n",
    "for i in range(n):\n",
    "    index = index_eq[i]\n",
    "    string = re.findall(\"eq=\\\".+?\\\"{1}\", info_series.iloc[index])[0]\n",
    "    eq[index] = re.sub(\"eq=|\\\"\", \"\", string)\n",
    "    \n",
    "n = index_type.shape[0]\n",
    "types = np.repeat(\"\", N).astype(\"object\")\n",
    "for i in range(n):\n",
    "    index = index_type[i]\n",
    "    string = re.findall(\"type=\\\".+?\\\"\", info_series.iloc[index])[0]\n",
    "    types[index] = re.sub(\"type=|\\\"\", \"\", string)\n",
    "\n",
    "n = index_alt.shape[0]\n",
    "alt = np.repeat(\"\", N).astype(\"object\")\n",
    "for i in range(n):\n",
    "    index = index_alt[i]\n",
    "    string = re.findall(\"alt=\\\".+?\\\"\", info_series.iloc[index])[0]\n",
    "    alt[index] = re.sub(\"alt=|\\\"\", \"\", string)\n",
    "    \n",
    "n = index_noun_type.shape[0]\n",
    "noun_type = np.repeat(\"\", N).astype(\"object\")\n",
    "for i in range(n):\n",
    "    index = index_noun_type[i]\n",
    "    string = re.findall(\"noun_type=\\\".+?\\\"\", info_series.iloc[index])[0]\n",
    "    noun_type[index] = re.sub(\"noun_type=|\\\"\", \"\", string)\n",
    "    \n",
    "n = index_ana_id.shape[0]\n",
    "ana_id = np.repeat(\"\", N).astype(\"object\")\n",
    "for i in range(n):\n",
    "    index = index_ana_id[i]\n",
    "    string = re.findall(\"ana_id=\\\".+?\\\"\", info_series.iloc[index])[0]\n",
    "    ana_id[index] = re.sub(\"ana_id=|\\\"\", \"\", string)\n",
    "    \n",
    "n = index_ant_id.shape[0]\n",
    "ant_id = np.repeat(\"\", N).astype(\"object\")\n",
    "for i in range(n):\n",
    "    index = index_ant_id[i]\n",
    "    string = re.findall(\"ant_id=\\\".+?\\\"\", info_series.iloc[index])[0]\n",
    "    ant_id[index] = re.sub(\"ant_id=|\\\"\", \"\", string)\n",
    "    \n",
    "n = index_ana_type.shape[0]\n",
    "ana_type = np.repeat(\"\", N).astype(\"object\")\n",
    "for i in range(n):\n",
    "    index = index_ana_type[i]\n",
    "    string = re.findall(\"ana_type=\\\".+?\\\"\", info_series.iloc[index])[0]\n",
    "    ana_type[index] = re.sub(\"ana_type=|\\\"\", \"\", string)\n",
    "    \n",
    "n = index_refexp_type.shape[0]\n",
    "refexp_type = np.repeat(\"\", N).astype(\"object\")\n",
    "for i in range(n):\n",
    "    index = index_refexp_type[i]\n",
    "    string = re.findall(\"refexp_type=\\\".+?\\\"\", info_series.iloc[index])[0]\n",
    "    refexp_type[index] = re.sub(\"refexp_type=|\\\"\", \"\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データフレームを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームに統合\n",
    "frame_info = pd.DataFrame({\"frame_id\": frame_id, \"ga_id\": ga_id, \"o_id\": o_id, \"ni_id\": ni_id, \"ga_type\": ga_type,\n",
    "                           \"o_type\": o_type, \"ni_type\": ni_type, \"eq\": eq, \"type\": types, \"alt\": alt, \"noun_type\": noun_type,\n",
    "                           \"ant_id\": ant_id, \"ana_id\": ana_id, \"ana_type\": ana_type, \"refexp\": refexp_type})\n",
    "naist_info = pd.concat((phrase_info, morpheme_info, frame_info), axis=1)\n",
    "naist_info = naist_info.iloc[np.where(naist_info[\"genkei\"]!=\"\")[0]]\n",
    "naist_info[\"serial_no\"] = np.arange(naist_info.shape[0])\n",
    "naist_info = naist_info[np.append(\"serial_no\", np.array(naist_info.columns)[:-1]).tolist()]\n",
    "naist_info.index = np.arange(naist_info.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを出力\n",
    "split = 3\n",
    "naist_info.to_csv(path + \"tdb/corpus/naist_info.csv\", index=None)\n",
    "sentence_id = np.unique(naist_info[\"sentence_id\"]).astype(\"int\")\n",
    "split_sentence = np.array_split(sentence_id, split, 0)\n",
    "for j in range(split):\n",
    "    index = np.where(np.in1d(naist_info[\"sentence_id\"], split_sentence[j]))[0]\n",
    "    output_split = naist_info.iloc[index]\n",
    "    output_split.to_excel(path + \"tdb/corpus/naist_info\" + str(j) + \".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
