{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import itertools\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import MeCab\n",
    "from numpy.random import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 250)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文書のデータフレームを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割済みデータから作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "nlp_path = \"D:/Statistics/data/NLP/\"\n",
    "tdb_path = \"D:/Statistics\\data/scenario_extract/tdb/sample/\"\n",
    "symbol_mapping = pd.read_csv(nlp_path + \"dic/symbol_mapping.csv\")\n",
    "tdb = pd.read_csv(tdb_path + \"sample_descriptions_by_sentence.csv\", dtype=\"str\")\n",
    "tdb[\"INTERNAL_ID\"] = tdb[\"INTERNAL_ID\"].astype(\"int\")\n",
    "tdb[\"SENTENCE_ID\"] = tdb[\"SENTENCE_ID\"].astype(\"int\")\n",
    "tdb[\"JGNY_KNJ_2\"] = tdb[\"JGNY_KNJ_2\"].str.normalize(\"NFKC\")\n",
    "tdb[\"JGNY_KNJ_2\"] = tdb[\"JGNY_KNJ_2\"].str.replace(\",\", \"\")\n",
    "tdb[\"JGNY_KNJ_2\"] = tdb[\"JGNY_KNJ_2\"].str.replace(\"および\", \"及び\")\n",
    "for i in range(symbol_mapping.shape[0]):\n",
    "    tdb[\"JGNY_KNJ_2\"] = tdb[\"JGNY_KNJ_2\"].str.replace(symbol_mapping[\"symbol\"].iloc[i], symbol_mapping[\"mapping\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを出力\n",
    "nlp_path = \"D:/Statistics/data/NLP/\"\n",
    "result = pd.DataFrame({\"KGCD\": tdb[\"KGCD\"], \"d_id\": tdb[\"SENTENCE_ID\"], \"pt\": tdb[\"INTERNAL_ID\"] , \"text\": tdb[\"JGNY_KNJ_2\"]})\n",
    "result.to_csv(nlp_path + \"/tdb/tdb_result_by_sentence.csv\", index=None)\n",
    "result[[\"text\"]].to_csv(nlp_path + \"/tdb/tdb_text_by_sentence.txt\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オリジナルデータから作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記号を出力\n",
    "# temp = pd.read_csv(\"D:/Statistics/data/NLP/tdb/tdb_corpus.csv\")\n",
    "# kigo = pd.unique(temp[\"word\"].iloc[np.where(temp[\"class\"]==\"記号\")])\n",
    "# target_kigo = kigo[np.array([len(kigo[i]) for i in range(len(kigo))]) > 1]\n",
    "# target_mapping = np.array([\" \".join(list(target_kigo[i])) for i in range(len(target_kigo))])\n",
    "# output = pd.DataFrame({\"symbol\": target_kigo, \"mapping\": target_mapping})\n",
    "# output.to_csv(\"D:/Statistics/data/NLP/dic/symbol_mapping_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "tdb = pd.read_csv(tdb_path + \"sample_company_business_descriptions.csv\", dtype=str)\n",
    "tdb[\"JGNY_KNJ_2\"] = tdb[\"JGNY_KNJ_2\"].str.normalize(\"NFKC\")\n",
    "tdb[\"JGNY_KNJ_2\"] = tdb[\"JGNY_KNJ_2\"].str.replace(\",\", \"\")\n",
    "tdb[\"JGNY_KNJ_2\"] = tdb[\"JGNY_KNJ_2\"].str.replace(\"および\", \"及び\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 文章を句読点で分割\n",
    "# データの設定\n",
    "kgcd = np.array(tdb[\"KGCD\"])\n",
    "text = np.array(tdb[\"JGNY_KNJ_2\"])\n",
    "D = len(kgcd)\n",
    "\n",
    "# 検索条件を設定\n",
    "brackets_start = np.array([\"\\(\", \"「\"])\n",
    "brackets_end = np.array([\"\\)\", \"」\"])\n",
    "condition1 = \"(?!$)(?:[^(。]*\" + brackets_start[0] + \"[^)]*\" + brackets_end[0] + \")*[^(。]*(?:。|$)\"\n",
    "condition2 = \"(?!$)(?:[^(。]*\" + brackets_start[1] + \"[^)]*\" + brackets_end[1] + \")*[^(。]*(?:。|$)\"\n",
    "condition = \"(\" + condition1 + \"|\" +  condition2 + \")\"\n",
    "\n",
    "# 文を文章に分割\n",
    "sentence_list = []\n",
    "kgcd_list = []\n",
    "no_list1 = []\n",
    "no_list2 = []\n",
    "max_no = 0\n",
    "for i in range(D):\n",
    "    sentence_list.append(np.array(re.findall(condition, text[i])))\n",
    "    kgcd_list.append(np.repeat(kgcd[i], len(sentence_list[i])))\n",
    "    no_list1.append(np.arange(len(sentence_list[i])))\n",
    "    no_list2.append(np.arange(len(sentence_list[i])) + max_no)\n",
    "    max_no = np.max(no_list2[i]) + 1\n",
    "    \n",
    "# リストを変換\n",
    "sentence = np.hstack((sentence_list))\n",
    "kgcd_id = np.hstack((kgcd_list))\n",
    "no1 = np.hstack((no_list1))\n",
    "no2 = np.hstack((no_list2))\n",
    "del sentence_list, kgcd_list, no_list1, no_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 括弧内の文書を除去する\n",
    "# 括弧内を置き換える\n",
    "new_sentence = pd.Series(np.repeat(\"\", len(sentence)).astype(\"object\"))\n",
    "for i in range(len(sentence)):\n",
    "    new_sentence[i] = re.sub(\"\\(.+?\\)\", \"\", sentence[i])\n",
    "    new_sentence[i] = re.sub(\"\\[.+?\\]\", \"\", new_sentence[i])\n",
    "    new_sentence[i] = re.sub(\"\\〔.+?\\〕\", \"\", new_sentence[i])\n",
    "    \n",
    "# 残った括弧を除去\n",
    "index = np.where(new_sentence.str.contains(\"[\\(|\\)|\\[|\\]|〔|〕]\"))[0]\n",
    "target_index = index[new_sentence.iloc[index].str.contains(\"\\(\\)\")==False]\n",
    "new_sentence.iloc[target_index] = new_sentence.iloc[target_index].str.replace(\"[\\(|\\)|\\[|\\]|〔|〕]\", \"\")\n",
    "# new_sentence = new_sentence.iloc[np.where(new_sentence.str.replace(\" \", \"\"))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 記号の連続を置き換える\n",
    "sentence_series = pd.Series(new_sentence)\n",
    "for i in range(symbol_mapping.shape[0]):\n",
    "    sentence_series = sentence_series.str.replace(symbol_mapping[\"symbol\"].iloc[i], symbol_mapping[\"mapping\"].iloc[i])\n",
    "n = tdb.shape[0]\n",
    "\n",
    "# マッピングされているかどうかチェック\n",
    "for i in range(symbol_mapping.shape[0]):\n",
    "    print([i, np.sum(sentence_series.str.contains(symbol_mapping[\"symbol\"].iloc[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認用ファイルを出力\n",
    "check = pd.DataFrame({\"d_id\": np.arange(len(sentence)), \"text\": sentence, \"new_text\": sentence_series,\n",
    "                      \"flag\": np.array(sentence!=sentence_series, dtype=\"int\")})\n",
    "check.to_excel(nlp_path + \"tdb/確認用.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを出力\n",
    "result = pd.DataFrame({\"serial_no\": np.arange(len(kgcd_id)), \"kgcd\": kgcd_id, \"no\": no1, \"d_id\": no2, \"text\": sentence_series})\n",
    "result.to_csv(nlp_path + \"/tdb/tdb_result_by_sentence.csv\", index=None)\n",
    "result[[\"text\"]].to_csv(nlp_path + \"/tdb/tdb_text_by_sentence.txt\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文節区切りに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 形態素解析を実行\n",
    "# mecabの設定\n",
    "N = len(sentence)\n",
    "mecab_columns = [\"word\", \"word_class\", \"class_detail1\", \"class_detail2\", \"class_detail3\",\n",
    "                 \"inflectional1\", \"inflectional2\", \"genkei\", \"readings1\", \"readings2\"]\n",
    "mecab = MeCab.Tagger(\" -d C:/Users/sana/dic/20190510_neologd\")\n",
    "\n",
    "# データの格納用配列\n",
    "parsed_list = [i for i in range(N)]\n",
    "kgcd_list = [i for i in range(N)]\n",
    "no_list1 = [i for i in range(N)]\n",
    "no_list2 = [i for i in range(N)]\n",
    "\n",
    "# 文章ごとにMeCabを実行\n",
    "for i in range(N):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    res = mecab.parse(sentence[i])\n",
    "    parsed_split = pd.Series(res.split(\"\\n\")).str.split('\\t|,').tolist()\n",
    "    parsed_list[i] = pd.DataFrame.from_records(parsed_split[0:len(parsed_split)-2])\n",
    "    parsed_list[i].columns = mecab_columns\n",
    "    n = parsed_list[i].shape[0]\n",
    "\n",
    "    # 単語ごとにidを付与\n",
    "    kgcd_list[i] = np.repeat(kgcd_id[i], n)\n",
    "    no_list1[i] = np.repeat(no1[i], n)\n",
    "    no_list2[i] = np.repeat(no2[i], n)\n",
    "    \n",
    "# データフレームを定義\n",
    "parsed_columns = [\"word\", \"genkei\", \"word_class\", \"class_detail1\", \"class_detail2\", \"class_detail3\",\n",
    "                  \"inflectional1\", \"inflectional2\"]\n",
    "kgcd_long = np.hstack((kgcd_list))\n",
    "no_long1 = np.hstack((no_list1))\n",
    "no_long2 = np.hstack((no_list2))\n",
    "F = len(kgcd_long)\n",
    "tdb_parsed1 = pd.DataFrame({\"serial_no\": np.arange(F),  \"kgcd\": kgcd_long, \"no\": no_long1, \"d_id\": no_long2})\n",
    "tdb_parsed2 = pd.concat((parsed_list), axis=0)[parsed_columns]\n",
    "tdb_parsed2.index = np.arange(tdb_parsed2.shape[0])\n",
    "tdb_parsed = pd.concat((tdb_parsed1, tdb_parsed2), axis=1)\n",
    "del kgcd_list, no_list1, no_list2, tdb_parsed1, tdb_parsed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを出力\n",
    "split = 3\n",
    "path = \"D:/Statistics/data/NLP/\"\n",
    "kgcd = np.unique(tdb_parsed[\"kgcd\"])\n",
    "split_kgcd = np.array_split(kgcd, split, 0)\n",
    "for j in range(split):\n",
    "    index = np.where(np.in1d(tdb_parsed[\"kgcd\"], split_kgcd[j]))[0]\n",
    "    tdb_split = tdb_parsed.iloc[index]\n",
    "    tdb_split.to_excel(path + \"tdb/tdb_parsed\" + str(j) + \".xlsx\")\n",
    "del tdb_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
